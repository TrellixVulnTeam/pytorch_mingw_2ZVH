
#include <Python.h>
#include <exception>

#include "THP.h"
#include "torch/csrc/nn/type_checks.h"

// HIPify isn't being applied to autogenerated files, so defensively
// handle both the CUDA and ROCM cases.
#if defined(USE_CUDA)
#include <c10/cuda/CUDAGuard.h>
using SpecializedDeviceGuard = c10::cuda::CUDAGuard;
#elif defined(USE_ROCM)
#include <ATen/hip/impl/HIPGuardImplMasqueradingAsCUDA.h>
// I'm not sure why the build doesn't like c10::cuda namespace...
using SpecializedDeviceGuard = at::cuda::HIPGuardMasqueradingAsCUDA;
#endif
#include <TH/TH.h>


TH_API void THNN_FloatSpatialConvolutionMM_updateOutput(void*, THFloatTensor*, THFloatTensor*, THFloatTensor*, THFloatTensor*, THFloatTensor*, THFloatTensor*, int, int, int, int, int, int);

PyObject * FloatSpatialConvolutionMM_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 13 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          (THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 4)) || PyTuple_GET_ITEM(args, 4) == Py_None) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THFloatTensor* arg_input = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THFloatTensor* arg_output = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THFloatTensor* arg_weight = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THFloatTensor* arg_bias = (PyTuple_GET_ITEM(args, 4) == Py_None ? NULL : THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 4)));
      THFloatTensor* arg_finput = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THFloatTensor* arg_fgradInput = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_FloatSpatialConvolutionMM_updateOutput(arg_state, arg_input, arg_output, arg_weight, arg_bias, arg_finput, arg_fgradInput, arg_kW, arg_kH, arg_dW, arg_dH, arg_padW, arg_padH);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "FloatSpatialConvolutionMM_updateOutput", 1, "(int state, torch.FloatTensor input, torch.FloatTensor output, torch.FloatTensor weight, [torch.FloatTensor bias or None], torch.FloatTensor finput, torch.FloatTensor fgradInput, int kW, int kH, int dW, int dH, int padW, int padH)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_DoubleSpatialConvolutionMM_updateOutput(void*, THDoubleTensor*, THDoubleTensor*, THDoubleTensor*, THDoubleTensor*, THDoubleTensor*, THDoubleTensor*, int, int, int, int, int, int);

PyObject * DoubleSpatialConvolutionMM_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 13 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          (THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 4)) || PyTuple_GET_ITEM(args, 4) == Py_None) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THDoubleTensor* arg_input = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THDoubleTensor* arg_output = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THDoubleTensor* arg_weight = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THDoubleTensor* arg_bias = (PyTuple_GET_ITEM(args, 4) == Py_None ? NULL : THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 4)));
      THDoubleTensor* arg_finput = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THDoubleTensor* arg_fgradInput = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_DoubleSpatialConvolutionMM_updateOutput(arg_state, arg_input, arg_output, arg_weight, arg_bias, arg_finput, arg_fgradInput, arg_kW, arg_kH, arg_dW, arg_dH, arg_padW, arg_padH);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "DoubleSpatialConvolutionMM_updateOutput", 1, "(int state, torch.DoubleTensor input, torch.DoubleTensor output, torch.DoubleTensor weight, [torch.DoubleTensor bias or None], torch.DoubleTensor finput, torch.DoubleTensor fgradInput, int kW, int kH, int dW, int dH, int padW, int padH)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_FloatVolumetricConvolutionMM_updateOutput(void*, THFloatTensor*, THFloatTensor*, THFloatTensor*, THFloatTensor*, THFloatTensor*, THFloatTensor*, int, int, int, int, int, int, int, int, int);

PyObject * FloatVolumetricConvolutionMM_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 16 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          (THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 4)) || PyTuple_GET_ITEM(args, 4) == Py_None) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 13)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 14)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 15))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THFloatTensor* arg_input = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THFloatTensor* arg_output = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THFloatTensor* arg_weight = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THFloatTensor* arg_bias = (PyTuple_GET_ITEM(args, 4) == Py_None ? NULL : THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 4)));
      THFloatTensor* arg_finput = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THFloatTensor* arg_fgradInput = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int arg_kT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_dT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      int arg_pT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 13));
      int arg_pW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 14));
      int arg_pH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 15));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_FloatVolumetricConvolutionMM_updateOutput(arg_state, arg_input, arg_output, arg_weight, arg_bias, arg_finput, arg_fgradInput, arg_kT, arg_kW, arg_kH, arg_dT, arg_dW, arg_dH, arg_pT, arg_pW, arg_pH);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "FloatVolumetricConvolutionMM_updateOutput", 1, "(int state, torch.FloatTensor input, torch.FloatTensor output, torch.FloatTensor weight, [torch.FloatTensor bias or None], torch.FloatTensor finput, torch.FloatTensor fgradInput, int kT, int kW, int kH, int dT, int dW, int dH, int pT, int pW, int pH)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_DoubleVolumetricConvolutionMM_updateOutput(void*, THDoubleTensor*, THDoubleTensor*, THDoubleTensor*, THDoubleTensor*, THDoubleTensor*, THDoubleTensor*, int, int, int, int, int, int, int, int, int);

PyObject * DoubleVolumetricConvolutionMM_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 16 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          (THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 4)) || PyTuple_GET_ITEM(args, 4) == Py_None) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 13)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 14)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 15))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THDoubleTensor* arg_input = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THDoubleTensor* arg_output = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THDoubleTensor* arg_weight = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THDoubleTensor* arg_bias = (PyTuple_GET_ITEM(args, 4) == Py_None ? NULL : THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 4)));
      THDoubleTensor* arg_finput = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THDoubleTensor* arg_fgradInput = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int arg_kT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_dT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      int arg_pT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 13));
      int arg_pW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 14));
      int arg_pH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 15));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_DoubleVolumetricConvolutionMM_updateOutput(arg_state, arg_input, arg_output, arg_weight, arg_bias, arg_finput, arg_fgradInput, arg_kT, arg_kW, arg_kH, arg_dT, arg_dW, arg_dH, arg_pT, arg_pW, arg_pH);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "DoubleVolumetricConvolutionMM_updateOutput", 1, "(int state, torch.DoubleTensor input, torch.DoubleTensor output, torch.DoubleTensor weight, [torch.DoubleTensor bias or None], torch.DoubleTensor finput, torch.DoubleTensor fgradInput, int kT, int kW, int kH, int dT, int dW, int dH, int pT, int pW, int pH)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_FloatAbsCriterion_updateOutput(void*, THFloatTensor*, THFloatTensor*, THFloatTensor*, int64_t);

PyObject * FloatAbsCriterion_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 5 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THFloatTensor* arg_input = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THFloatTensor* arg_target = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THFloatTensor* arg_output = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      int64_t arg_reduction = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_FloatAbsCriterion_updateOutput(arg_state, arg_input, arg_target, arg_output, arg_reduction);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "FloatAbsCriterion_updateOutput", 1, "(int state, torch.FloatTensor input, torch.FloatTensor target, torch.FloatTensor output, int reduction)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_DoubleAbsCriterion_updateOutput(void*, THDoubleTensor*, THDoubleTensor*, THDoubleTensor*, int64_t);

PyObject * DoubleAbsCriterion_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 5 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THDoubleTensor* arg_input = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THDoubleTensor* arg_target = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THDoubleTensor* arg_output = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      int64_t arg_reduction = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_DoubleAbsCriterion_updateOutput(arg_state, arg_input, arg_target, arg_output, arg_reduction);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "DoubleAbsCriterion_updateOutput", 1, "(int state, torch.DoubleTensor input, torch.DoubleTensor target, torch.DoubleTensor output, int reduction)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_FloatAbsCriterion_updateGradInput(void*, THFloatTensor*, THFloatTensor*, THFloatTensor*, THFloatTensor*, int64_t);

PyObject * FloatAbsCriterion_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 6 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THFloatTensor* arg_input = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THFloatTensor* arg_target = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THFloatTensor* arg_gradOutput = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THFloatTensor* arg_gradInput = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      int64_t arg_reduction = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_FloatAbsCriterion_updateGradInput(arg_state, arg_input, arg_target, arg_gradOutput, arg_gradInput, arg_reduction);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "FloatAbsCriterion_updateGradInput", 1, "(int state, torch.FloatTensor input, torch.FloatTensor target, torch.FloatTensor gradOutput, torch.FloatTensor gradInput, int reduction)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_DoubleAbsCriterion_updateGradInput(void*, THDoubleTensor*, THDoubleTensor*, THDoubleTensor*, THDoubleTensor*, int64_t);

PyObject * DoubleAbsCriterion_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 6 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THDoubleTensor* arg_input = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THDoubleTensor* arg_target = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THDoubleTensor* arg_gradOutput = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THDoubleTensor* arg_gradInput = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      int64_t arg_reduction = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_DoubleAbsCriterion_updateGradInput(arg_state, arg_input, arg_target, arg_gradOutput, arg_gradInput, arg_reduction);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "DoubleAbsCriterion_updateGradInput", 1, "(int state, torch.DoubleTensor input, torch.DoubleTensor target, torch.DoubleTensor gradOutput, torch.DoubleTensor gradInput, int reduction)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_FloatBCECriterion_updateOutput(void*, THFloatTensor*, THFloatTensor*, THFloatTensor*, int64_t, THFloatTensor*);

PyObject * FloatBCECriterion_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 6 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          (THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 5)) || PyTuple_GET_ITEM(args, 5) == Py_None)) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THFloatTensor* arg_input = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THFloatTensor* arg_target = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THFloatTensor* arg_output = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      int64_t arg_reduction = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      THFloatTensor* arg_weights = (PyTuple_GET_ITEM(args, 5) == Py_None ? NULL : THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 5)));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_FloatBCECriterion_updateOutput(arg_state, arg_input, arg_target, arg_output, arg_reduction, arg_weights);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "FloatBCECriterion_updateOutput", 1, "(int state, torch.FloatTensor input, torch.FloatTensor target, torch.FloatTensor output, int reduction, [torch.FloatTensor weights or None])");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_DoubleBCECriterion_updateOutput(void*, THDoubleTensor*, THDoubleTensor*, THDoubleTensor*, int64_t, THDoubleTensor*);

PyObject * DoubleBCECriterion_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 6 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          (THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 5)) || PyTuple_GET_ITEM(args, 5) == Py_None)) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THDoubleTensor* arg_input = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THDoubleTensor* arg_target = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THDoubleTensor* arg_output = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      int64_t arg_reduction = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      THDoubleTensor* arg_weights = (PyTuple_GET_ITEM(args, 5) == Py_None ? NULL : THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 5)));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_DoubleBCECriterion_updateOutput(arg_state, arg_input, arg_target, arg_output, arg_reduction, arg_weights);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "DoubleBCECriterion_updateOutput", 1, "(int state, torch.DoubleTensor input, torch.DoubleTensor target, torch.DoubleTensor output, int reduction, [torch.DoubleTensor weights or None])");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_FloatBCECriterion_updateGradInput(void*, THFloatTensor*, THFloatTensor*, THFloatTensor*, THFloatTensor*, int64_t, THFloatTensor*);

PyObject * FloatBCECriterion_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 7 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          (THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 6)) || PyTuple_GET_ITEM(args, 6) == Py_None)) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THFloatTensor* arg_input = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THFloatTensor* arg_target = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THFloatTensor* arg_gradOutput = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THFloatTensor* arg_gradInput = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      int64_t arg_reduction = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      THFloatTensor* arg_weights = (PyTuple_GET_ITEM(args, 6) == Py_None ? NULL : THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 6)));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_FloatBCECriterion_updateGradInput(arg_state, arg_input, arg_target, arg_gradOutput, arg_gradInput, arg_reduction, arg_weights);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "FloatBCECriterion_updateGradInput", 1, "(int state, torch.FloatTensor input, torch.FloatTensor target, torch.FloatTensor gradOutput, torch.FloatTensor gradInput, int reduction, [torch.FloatTensor weights or None])");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_DoubleBCECriterion_updateGradInput(void*, THDoubleTensor*, THDoubleTensor*, THDoubleTensor*, THDoubleTensor*, int64_t, THDoubleTensor*);

PyObject * DoubleBCECriterion_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 7 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          (THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 6)) || PyTuple_GET_ITEM(args, 6) == Py_None)) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THDoubleTensor* arg_input = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THDoubleTensor* arg_target = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THDoubleTensor* arg_gradOutput = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THDoubleTensor* arg_gradInput = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      int64_t arg_reduction = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      THDoubleTensor* arg_weights = (PyTuple_GET_ITEM(args, 6) == Py_None ? NULL : THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 6)));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_DoubleBCECriterion_updateGradInput(arg_state, arg_input, arg_target, arg_gradOutput, arg_gradInput, arg_reduction, arg_weights);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "DoubleBCECriterion_updateGradInput", 1, "(int state, torch.DoubleTensor input, torch.DoubleTensor target, torch.DoubleTensor gradOutput, torch.DoubleTensor gradInput, int reduction, [torch.DoubleTensor weights or None])");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_FloatClassNLLCriterion_updateOutput(void*, THFloatTensor*, THLongTensor*, THFloatTensor*, int64_t, THFloatTensor*, THFloatTensor*, int64_t);

PyObject * FloatClassNLLCriterion_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 8 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_LongTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          (THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 5)) || PyTuple_GET_ITEM(args, 5) == Py_None) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THFloatTensor* arg_input = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THLongTensor* arg_target = THNN_LongTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THFloatTensor* arg_output = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      int64_t arg_reduction = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      THFloatTensor* arg_weights = (PyTuple_GET_ITEM(args, 5) == Py_None ? NULL : THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 5)));
      THFloatTensor* arg_total_weight = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int64_t arg_ignore_index = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_FloatClassNLLCriterion_updateOutput(arg_state, arg_input, arg_target, arg_output, arg_reduction, arg_weights, arg_total_weight, arg_ignore_index);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "FloatClassNLLCriterion_updateOutput", 1, "(int state, torch.FloatTensor input, torch.LongTensor target, torch.FloatTensor output, int reduction, [torch.FloatTensor weights or None], torch.FloatTensor total_weight, int ignore_index)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_DoubleClassNLLCriterion_updateOutput(void*, THDoubleTensor*, THLongTensor*, THDoubleTensor*, int64_t, THDoubleTensor*, THDoubleTensor*, int64_t);

PyObject * DoubleClassNLLCriterion_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 8 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_LongTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          (THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 5)) || PyTuple_GET_ITEM(args, 5) == Py_None) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THDoubleTensor* arg_input = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THLongTensor* arg_target = THNN_LongTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THDoubleTensor* arg_output = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      int64_t arg_reduction = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      THDoubleTensor* arg_weights = (PyTuple_GET_ITEM(args, 5) == Py_None ? NULL : THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 5)));
      THDoubleTensor* arg_total_weight = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int64_t arg_ignore_index = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_DoubleClassNLLCriterion_updateOutput(arg_state, arg_input, arg_target, arg_output, arg_reduction, arg_weights, arg_total_weight, arg_ignore_index);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "DoubleClassNLLCriterion_updateOutput", 1, "(int state, torch.DoubleTensor input, torch.LongTensor target, torch.DoubleTensor output, int reduction, [torch.DoubleTensor weights or None], torch.DoubleTensor total_weight, int ignore_index)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_FloatClassNLLCriterion_updateGradInput(void*, THFloatTensor*, THLongTensor*, THFloatTensor*, THFloatTensor*, int64_t, THFloatTensor*, THFloatTensor*, int64_t);

PyObject * FloatClassNLLCriterion_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 9 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_LongTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          (THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 6)) || PyTuple_GET_ITEM(args, 6) == Py_None) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THFloatTensor* arg_input = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THLongTensor* arg_target = THNN_LongTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THFloatTensor* arg_gradOutput = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THFloatTensor* arg_gradInput = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      int64_t arg_reduction = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      THFloatTensor* arg_weights = (PyTuple_GET_ITEM(args, 6) == Py_None ? NULL : THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 6)));
      THFloatTensor* arg_total_weight = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 7));
      int64_t arg_ignore_index = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_FloatClassNLLCriterion_updateGradInput(arg_state, arg_input, arg_target, arg_gradOutput, arg_gradInput, arg_reduction, arg_weights, arg_total_weight, arg_ignore_index);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "FloatClassNLLCriterion_updateGradInput", 1, "(int state, torch.FloatTensor input, torch.LongTensor target, torch.FloatTensor gradOutput, torch.FloatTensor gradInput, int reduction, [torch.FloatTensor weights or None], torch.FloatTensor total_weight, int ignore_index)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_DoubleClassNLLCriterion_updateGradInput(void*, THDoubleTensor*, THLongTensor*, THDoubleTensor*, THDoubleTensor*, int64_t, THDoubleTensor*, THDoubleTensor*, int64_t);

PyObject * DoubleClassNLLCriterion_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 9 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_LongTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          (THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 6)) || PyTuple_GET_ITEM(args, 6) == Py_None) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THDoubleTensor* arg_input = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THLongTensor* arg_target = THNN_LongTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THDoubleTensor* arg_gradOutput = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THDoubleTensor* arg_gradInput = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      int64_t arg_reduction = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      THDoubleTensor* arg_weights = (PyTuple_GET_ITEM(args, 6) == Py_None ? NULL : THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 6)));
      THDoubleTensor* arg_total_weight = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 7));
      int64_t arg_ignore_index = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_DoubleClassNLLCriterion_updateGradInput(arg_state, arg_input, arg_target, arg_gradOutput, arg_gradInput, arg_reduction, arg_weights, arg_total_weight, arg_ignore_index);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "DoubleClassNLLCriterion_updateGradInput", 1, "(int state, torch.DoubleTensor input, torch.LongTensor target, torch.DoubleTensor gradOutput, torch.DoubleTensor gradInput, int reduction, [torch.DoubleTensor weights or None], torch.DoubleTensor total_weight, int ignore_index)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_FloatELU_updateOutput(void*, THFloatTensor*, THFloatTensor*, double, double, double, bool);

PyObject * FloatELU_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 7 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 3)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 4)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 5)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 6))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THFloatTensor* arg_input = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THFloatTensor* arg_output = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      double arg_alpha = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 3));
      double arg_scale = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 4));
      double arg_input_scale = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 5));
      bool arg_inplace = (PyTuple_GET_ITEM(args, 6) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_FloatELU_updateOutput(arg_state, arg_input, arg_output, arg_alpha, arg_scale, arg_input_scale, arg_inplace);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "FloatELU_updateOutput", 1, "(int state, torch.FloatTensor input, torch.FloatTensor output, float alpha, float scale, float input_scale, bool inplace)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_DoubleELU_updateOutput(void*, THDoubleTensor*, THDoubleTensor*, double, double, double, bool);

PyObject * DoubleELU_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 7 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 3)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 4)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 5)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 6))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THDoubleTensor* arg_input = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THDoubleTensor* arg_output = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      double arg_alpha = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 3));
      double arg_scale = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 4));
      double arg_input_scale = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 5));
      bool arg_inplace = (PyTuple_GET_ITEM(args, 6) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_DoubleELU_updateOutput(arg_state, arg_input, arg_output, arg_alpha, arg_scale, arg_input_scale, arg_inplace);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "DoubleELU_updateOutput", 1, "(int state, torch.DoubleTensor input, torch.DoubleTensor output, float alpha, float scale, float input_scale, bool inplace)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_FloatELU_updateGradInput(void*, THFloatTensor*, THFloatTensor*, THFloatTensor*, double, double, double);

PyObject * FloatELU_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 7 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 4)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 5)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 6))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THFloatTensor* arg_gradOutput = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THFloatTensor* arg_gradInput = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THFloatTensor* arg_output = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      double arg_alpha = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 4));
      double arg_scale = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 5));
      double arg_input_scale = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 6));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_FloatELU_updateGradInput(arg_state, arg_gradOutput, arg_gradInput, arg_output, arg_alpha, arg_scale, arg_input_scale);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "FloatELU_updateGradInput", 1, "(int state, torch.FloatTensor gradOutput, torch.FloatTensor gradInput, torch.FloatTensor output, float alpha, float scale, float input_scale)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_DoubleELU_updateGradInput(void*, THDoubleTensor*, THDoubleTensor*, THDoubleTensor*, double, double, double);

PyObject * DoubleELU_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 7 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 4)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 5)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 6))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THDoubleTensor* arg_gradOutput = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THDoubleTensor* arg_gradInput = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THDoubleTensor* arg_output = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      double arg_alpha = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 4));
      double arg_scale = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 5));
      double arg_input_scale = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 6));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_DoubleELU_updateGradInput(arg_state, arg_gradOutput, arg_gradInput, arg_output, arg_alpha, arg_scale, arg_input_scale);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "DoubleELU_updateGradInput", 1, "(int state, torch.DoubleTensor gradOutput, torch.DoubleTensor gradInput, torch.DoubleTensor output, float alpha, float scale, float input_scale)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_FloatGatedLinear_updateOutput(void*, THFloatTensor*, THFloatTensor*, int);

PyObject * FloatGatedLinear_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 4 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 3))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THFloatTensor* arg_input = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THFloatTensor* arg_output = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      int arg_dim = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 3));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_FloatGatedLinear_updateOutput(arg_state, arg_input, arg_output, arg_dim);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "FloatGatedLinear_updateOutput", 1, "(int state, torch.FloatTensor input, torch.FloatTensor output, int dim)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_DoubleGatedLinear_updateOutput(void*, THDoubleTensor*, THDoubleTensor*, int);

PyObject * DoubleGatedLinear_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 4 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 3))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THDoubleTensor* arg_input = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THDoubleTensor* arg_output = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      int arg_dim = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 3));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_DoubleGatedLinear_updateOutput(arg_state, arg_input, arg_output, arg_dim);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "DoubleGatedLinear_updateOutput", 1, "(int state, torch.DoubleTensor input, torch.DoubleTensor output, int dim)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_FloatGatedLinear_updateGradInput(void*, THFloatTensor*, THFloatTensor*, THFloatTensor*, int);

PyObject * FloatGatedLinear_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 5 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THFloatTensor* arg_input = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THFloatTensor* arg_gradOutput = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THFloatTensor* arg_gradInput = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      int arg_dim = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_FloatGatedLinear_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_dim);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "FloatGatedLinear_updateGradInput", 1, "(int state, torch.FloatTensor input, torch.FloatTensor gradOutput, torch.FloatTensor gradInput, int dim)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_DoubleGatedLinear_updateGradInput(void*, THDoubleTensor*, THDoubleTensor*, THDoubleTensor*, int);

PyObject * DoubleGatedLinear_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 5 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THDoubleTensor* arg_input = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THDoubleTensor* arg_gradOutput = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THDoubleTensor* arg_gradInput = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      int arg_dim = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_DoubleGatedLinear_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_dim);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "DoubleGatedLinear_updateGradInput", 1, "(int state, torch.DoubleTensor input, torch.DoubleTensor gradOutput, torch.DoubleTensor gradInput, int dim)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_FloatHardTanh_updateOutput(void*, THFloatTensor*, THFloatTensor*, double, double, bool);

PyObject * FloatHardTanh_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 6 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 3)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 4)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 5))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THFloatTensor* arg_input = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THFloatTensor* arg_output = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      double arg_min_val = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 3));
      double arg_max_val = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 4));
      bool arg_inplace = (PyTuple_GET_ITEM(args, 5) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_FloatHardTanh_updateOutput(arg_state, arg_input, arg_output, arg_min_val, arg_max_val, arg_inplace);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "FloatHardTanh_updateOutput", 1, "(int state, torch.FloatTensor input, torch.FloatTensor output, float min_val, float max_val, bool inplace)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_DoubleHardTanh_updateOutput(void*, THDoubleTensor*, THDoubleTensor*, double, double, bool);

PyObject * DoubleHardTanh_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 6 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 3)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 4)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 5))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THDoubleTensor* arg_input = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THDoubleTensor* arg_output = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      double arg_min_val = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 3));
      double arg_max_val = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 4));
      bool arg_inplace = (PyTuple_GET_ITEM(args, 5) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_DoubleHardTanh_updateOutput(arg_state, arg_input, arg_output, arg_min_val, arg_max_val, arg_inplace);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "DoubleHardTanh_updateOutput", 1, "(int state, torch.DoubleTensor input, torch.DoubleTensor output, float min_val, float max_val, bool inplace)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_FloatHardTanh_updateGradInput(void*, THFloatTensor*, THFloatTensor*, THFloatTensor*, double, double, bool);

PyObject * FloatHardTanh_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 7 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 4)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 5)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 6))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THFloatTensor* arg_input = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THFloatTensor* arg_gradOutput = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THFloatTensor* arg_gradInput = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      double arg_min_val = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 4));
      double arg_max_val = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 5));
      bool arg_inplace = (PyTuple_GET_ITEM(args, 6) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_FloatHardTanh_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_min_val, arg_max_val, arg_inplace);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "FloatHardTanh_updateGradInput", 1, "(int state, torch.FloatTensor input, torch.FloatTensor gradOutput, torch.FloatTensor gradInput, float min_val, float max_val, bool inplace)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_DoubleHardTanh_updateGradInput(void*, THDoubleTensor*, THDoubleTensor*, THDoubleTensor*, double, double, bool);

PyObject * DoubleHardTanh_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 7 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 4)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 5)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 6))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THDoubleTensor* arg_input = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THDoubleTensor* arg_gradOutput = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THDoubleTensor* arg_gradInput = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      double arg_min_val = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 4));
      double arg_max_val = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 5));
      bool arg_inplace = (PyTuple_GET_ITEM(args, 6) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_DoubleHardTanh_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_min_val, arg_max_val, arg_inplace);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "DoubleHardTanh_updateGradInput", 1, "(int state, torch.DoubleTensor input, torch.DoubleTensor gradOutput, torch.DoubleTensor gradInput, float min_val, float max_val, bool inplace)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_FloatLeakyReLU_updateOutput(void*, THFloatTensor*, THFloatTensor*, double, bool);

PyObject * FloatLeakyReLU_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 5 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 3)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 4))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THFloatTensor* arg_input = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THFloatTensor* arg_output = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      double arg_negval = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 3));
      bool arg_inplace = (PyTuple_GET_ITEM(args, 4) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_FloatLeakyReLU_updateOutput(arg_state, arg_input, arg_output, arg_negval, arg_inplace);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "FloatLeakyReLU_updateOutput", 1, "(int state, torch.FloatTensor input, torch.FloatTensor output, float negval, bool inplace)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_DoubleLeakyReLU_updateOutput(void*, THDoubleTensor*, THDoubleTensor*, double, bool);

PyObject * DoubleLeakyReLU_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 5 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 3)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 4))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THDoubleTensor* arg_input = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THDoubleTensor* arg_output = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      double arg_negval = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 3));
      bool arg_inplace = (PyTuple_GET_ITEM(args, 4) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_DoubleLeakyReLU_updateOutput(arg_state, arg_input, arg_output, arg_negval, arg_inplace);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "DoubleLeakyReLU_updateOutput", 1, "(int state, torch.DoubleTensor input, torch.DoubleTensor output, float negval, bool inplace)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_FloatLeakyReLU_updateGradInput(void*, THFloatTensor*, THFloatTensor*, THFloatTensor*, double, bool);

PyObject * FloatLeakyReLU_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 6 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 4)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 5))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THFloatTensor* arg_input = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THFloatTensor* arg_gradOutput = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THFloatTensor* arg_gradInput = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      double arg_negval = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 4));
      bool arg_inplace = (PyTuple_GET_ITEM(args, 5) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_FloatLeakyReLU_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_negval, arg_inplace);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "FloatLeakyReLU_updateGradInput", 1, "(int state, torch.FloatTensor input, torch.FloatTensor gradOutput, torch.FloatTensor gradInput, float negval, bool inplace)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_DoubleLeakyReLU_updateGradInput(void*, THDoubleTensor*, THDoubleTensor*, THDoubleTensor*, double, bool);

PyObject * DoubleLeakyReLU_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 6 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 4)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 5))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THDoubleTensor* arg_input = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THDoubleTensor* arg_gradOutput = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THDoubleTensor* arg_gradInput = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      double arg_negval = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 4));
      bool arg_inplace = (PyTuple_GET_ITEM(args, 5) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_DoubleLeakyReLU_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_negval, arg_inplace);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "DoubleLeakyReLU_updateGradInput", 1, "(int state, torch.DoubleTensor input, torch.DoubleTensor gradOutput, torch.DoubleTensor gradInput, float negval, bool inplace)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_FloatLogSigmoid_updateOutput(void*, THFloatTensor*, THFloatTensor*, THFloatTensor*);

PyObject * FloatLogSigmoid_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 4 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 3))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THFloatTensor* arg_input = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THFloatTensor* arg_output = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THFloatTensor* arg_buffer = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_FloatLogSigmoid_updateOutput(arg_state, arg_input, arg_output, arg_buffer);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "FloatLogSigmoid_updateOutput", 1, "(int state, torch.FloatTensor input, torch.FloatTensor output, torch.FloatTensor buffer)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_DoubleLogSigmoid_updateOutput(void*, THDoubleTensor*, THDoubleTensor*, THDoubleTensor*);

PyObject * DoubleLogSigmoid_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 4 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 3))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THDoubleTensor* arg_input = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THDoubleTensor* arg_output = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THDoubleTensor* arg_buffer = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_DoubleLogSigmoid_updateOutput(arg_state, arg_input, arg_output, arg_buffer);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "DoubleLogSigmoid_updateOutput", 1, "(int state, torch.DoubleTensor input, torch.DoubleTensor output, torch.DoubleTensor buffer)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_FloatLogSigmoid_updateGradInput(void*, THFloatTensor*, THFloatTensor*, THFloatTensor*, THFloatTensor*);

PyObject * FloatLogSigmoid_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 5 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 4))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THFloatTensor* arg_input = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THFloatTensor* arg_gradOutput = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THFloatTensor* arg_gradInput = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THFloatTensor* arg_buffer = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_FloatLogSigmoid_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_buffer);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "FloatLogSigmoid_updateGradInput", 1, "(int state, torch.FloatTensor input, torch.FloatTensor gradOutput, torch.FloatTensor gradInput, torch.FloatTensor buffer)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_DoubleLogSigmoid_updateGradInput(void*, THDoubleTensor*, THDoubleTensor*, THDoubleTensor*, THDoubleTensor*);

PyObject * DoubleLogSigmoid_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 5 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 4))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THDoubleTensor* arg_input = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THDoubleTensor* arg_gradOutput = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THDoubleTensor* arg_gradInput = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THDoubleTensor* arg_buffer = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_DoubleLogSigmoid_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_buffer);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "DoubleLogSigmoid_updateGradInput", 1, "(int state, torch.DoubleTensor input, torch.DoubleTensor gradOutput, torch.DoubleTensor gradInput, torch.DoubleTensor buffer)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_FloatSoftMarginCriterion_updateOutput(void*, THFloatTensor*, THFloatTensor*, THFloatTensor*, int64_t);

PyObject * FloatSoftMarginCriterion_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 5 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THFloatTensor* arg_input = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THFloatTensor* arg_target = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THFloatTensor* arg_output = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      int64_t arg_reduction = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_FloatSoftMarginCriterion_updateOutput(arg_state, arg_input, arg_target, arg_output, arg_reduction);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "FloatSoftMarginCriterion_updateOutput", 1, "(int state, torch.FloatTensor input, torch.FloatTensor target, torch.FloatTensor output, int reduction)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_DoubleSoftMarginCriterion_updateOutput(void*, THDoubleTensor*, THDoubleTensor*, THDoubleTensor*, int64_t);

PyObject * DoubleSoftMarginCriterion_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 5 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THDoubleTensor* arg_input = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THDoubleTensor* arg_target = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THDoubleTensor* arg_output = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      int64_t arg_reduction = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_DoubleSoftMarginCriterion_updateOutput(arg_state, arg_input, arg_target, arg_output, arg_reduction);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "DoubleSoftMarginCriterion_updateOutput", 1, "(int state, torch.DoubleTensor input, torch.DoubleTensor target, torch.DoubleTensor output, int reduction)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_FloatSoftMarginCriterion_updateGradInput(void*, THFloatTensor*, THFloatTensor*, THFloatTensor*, THFloatTensor*, int64_t);

PyObject * FloatSoftMarginCriterion_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 6 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THFloatTensor* arg_input = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THFloatTensor* arg_target = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THFloatTensor* arg_gradOutput = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THFloatTensor* arg_gradInput = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      int64_t arg_reduction = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_FloatSoftMarginCriterion_updateGradInput(arg_state, arg_input, arg_target, arg_gradOutput, arg_gradInput, arg_reduction);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "FloatSoftMarginCriterion_updateGradInput", 1, "(int state, torch.FloatTensor input, torch.FloatTensor target, torch.FloatTensor gradOutput, torch.FloatTensor gradInput, int reduction)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_DoubleSoftMarginCriterion_updateGradInput(void*, THDoubleTensor*, THDoubleTensor*, THDoubleTensor*, THDoubleTensor*, int64_t);

PyObject * DoubleSoftMarginCriterion_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 6 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THDoubleTensor* arg_input = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THDoubleTensor* arg_target = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THDoubleTensor* arg_gradOutput = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THDoubleTensor* arg_gradInput = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      int64_t arg_reduction = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_DoubleSoftMarginCriterion_updateGradInput(arg_state, arg_input, arg_target, arg_gradOutput, arg_gradInput, arg_reduction);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "DoubleSoftMarginCriterion_updateGradInput", 1, "(int state, torch.DoubleTensor input, torch.DoubleTensor target, torch.DoubleTensor gradOutput, torch.DoubleTensor gradInput, int reduction)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_FloatMSECriterion_updateOutput(void*, THFloatTensor*, THFloatTensor*, THFloatTensor*, int64_t);

PyObject * FloatMSECriterion_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 5 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THFloatTensor* arg_input = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THFloatTensor* arg_target = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THFloatTensor* arg_output = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      int64_t arg_reduction = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_FloatMSECriterion_updateOutput(arg_state, arg_input, arg_target, arg_output, arg_reduction);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "FloatMSECriterion_updateOutput", 1, "(int state, torch.FloatTensor input, torch.FloatTensor target, torch.FloatTensor output, int reduction)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_DoubleMSECriterion_updateOutput(void*, THDoubleTensor*, THDoubleTensor*, THDoubleTensor*, int64_t);

PyObject * DoubleMSECriterion_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 5 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THDoubleTensor* arg_input = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THDoubleTensor* arg_target = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THDoubleTensor* arg_output = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      int64_t arg_reduction = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_DoubleMSECriterion_updateOutput(arg_state, arg_input, arg_target, arg_output, arg_reduction);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "DoubleMSECriterion_updateOutput", 1, "(int state, torch.DoubleTensor input, torch.DoubleTensor target, torch.DoubleTensor output, int reduction)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_FloatMSECriterion_updateGradInput(void*, THFloatTensor*, THFloatTensor*, THFloatTensor*, THFloatTensor*, int64_t);

PyObject * FloatMSECriterion_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 6 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THFloatTensor* arg_input = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THFloatTensor* arg_target = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THFloatTensor* arg_gradOutput = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THFloatTensor* arg_gradInput = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      int64_t arg_reduction = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_FloatMSECriterion_updateGradInput(arg_state, arg_input, arg_target, arg_gradOutput, arg_gradInput, arg_reduction);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "FloatMSECriterion_updateGradInput", 1, "(int state, torch.FloatTensor input, torch.FloatTensor target, torch.FloatTensor gradOutput, torch.FloatTensor gradInput, int reduction)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_DoubleMSECriterion_updateGradInput(void*, THDoubleTensor*, THDoubleTensor*, THDoubleTensor*, THDoubleTensor*, int64_t);

PyObject * DoubleMSECriterion_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 6 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THDoubleTensor* arg_input = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THDoubleTensor* arg_target = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THDoubleTensor* arg_gradOutput = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THDoubleTensor* arg_gradInput = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      int64_t arg_reduction = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_DoubleMSECriterion_updateGradInput(arg_state, arg_input, arg_target, arg_gradOutput, arg_gradInput, arg_reduction);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "DoubleMSECriterion_updateGradInput", 1, "(int state, torch.DoubleTensor input, torch.DoubleTensor target, torch.DoubleTensor gradOutput, torch.DoubleTensor gradInput, int reduction)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_FloatMultiLabelMarginCriterion_updateOutput(void*, THFloatTensor*, THLongTensor*, THFloatTensor*, THFloatTensor*, int64_t);

PyObject * FloatMultiLabelMarginCriterion_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 6 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_LongTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THFloatTensor* arg_input = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THLongTensor* arg_target = THNN_LongTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THFloatTensor* arg_output = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THFloatTensor* arg_isTarget = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      int64_t arg_reduction = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_FloatMultiLabelMarginCriterion_updateOutput(arg_state, arg_input, arg_target, arg_output, arg_isTarget, arg_reduction);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "FloatMultiLabelMarginCriterion_updateOutput", 1, "(int state, torch.FloatTensor input, torch.LongTensor target, torch.FloatTensor output, torch.FloatTensor isTarget, int reduction)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_DoubleMultiLabelMarginCriterion_updateOutput(void*, THDoubleTensor*, THLongTensor*, THDoubleTensor*, THDoubleTensor*, int64_t);

PyObject * DoubleMultiLabelMarginCriterion_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 6 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_LongTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THDoubleTensor* arg_input = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THLongTensor* arg_target = THNN_LongTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THDoubleTensor* arg_output = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THDoubleTensor* arg_isTarget = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      int64_t arg_reduction = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_DoubleMultiLabelMarginCriterion_updateOutput(arg_state, arg_input, arg_target, arg_output, arg_isTarget, arg_reduction);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "DoubleMultiLabelMarginCriterion_updateOutput", 1, "(int state, torch.DoubleTensor input, torch.LongTensor target, torch.DoubleTensor output, torch.DoubleTensor isTarget, int reduction)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_FloatMultiLabelMarginCriterion_updateGradInput(void*, THFloatTensor*, THLongTensor*, THFloatTensor*, THFloatTensor*, THFloatTensor*, int64_t);

PyObject * FloatMultiLabelMarginCriterion_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 7 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_LongTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THFloatTensor* arg_input = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THLongTensor* arg_target = THNN_LongTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THFloatTensor* arg_gradOutput = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THFloatTensor* arg_gradInput = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THFloatTensor* arg_isTarget = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      int64_t arg_reduction = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_FloatMultiLabelMarginCriterion_updateGradInput(arg_state, arg_input, arg_target, arg_gradOutput, arg_gradInput, arg_isTarget, arg_reduction);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "FloatMultiLabelMarginCriterion_updateGradInput", 1, "(int state, torch.FloatTensor input, torch.LongTensor target, torch.FloatTensor gradOutput, torch.FloatTensor gradInput, torch.FloatTensor isTarget, int reduction)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_DoubleMultiLabelMarginCriterion_updateGradInput(void*, THDoubleTensor*, THLongTensor*, THDoubleTensor*, THDoubleTensor*, THDoubleTensor*, int64_t);

PyObject * DoubleMultiLabelMarginCriterion_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 7 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_LongTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THDoubleTensor* arg_input = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THLongTensor* arg_target = THNN_LongTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THDoubleTensor* arg_gradOutput = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THDoubleTensor* arg_gradInput = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THDoubleTensor* arg_isTarget = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      int64_t arg_reduction = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_DoubleMultiLabelMarginCriterion_updateGradInput(arg_state, arg_input, arg_target, arg_gradOutput, arg_gradInput, arg_isTarget, arg_reduction);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "DoubleMultiLabelMarginCriterion_updateGradInput", 1, "(int state, torch.DoubleTensor input, torch.LongTensor target, torch.DoubleTensor gradOutput, torch.DoubleTensor gradInput, torch.DoubleTensor isTarget, int reduction)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_FloatMultiMarginCriterion_updateOutput(void*, THFloatTensor*, THLongTensor*, THFloatTensor*, int64_t, int, THFloatTensor*, double);

PyObject * FloatMultiMarginCriterion_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 8 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_LongTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          (THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 6)) || PyTuple_GET_ITEM(args, 6) == Py_None) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 7))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THFloatTensor* arg_input = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THLongTensor* arg_target = THNN_LongTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THFloatTensor* arg_output = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      int64_t arg_reduction = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_p = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      THFloatTensor* arg_weights = (PyTuple_GET_ITEM(args, 6) == Py_None ? NULL : THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 6)));
      double arg_margin = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 7));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_FloatMultiMarginCriterion_updateOutput(arg_state, arg_input, arg_target, arg_output, arg_reduction, arg_p, arg_weights, arg_margin);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "FloatMultiMarginCriterion_updateOutput", 1, "(int state, torch.FloatTensor input, torch.LongTensor target, torch.FloatTensor output, int reduction, int p, [torch.FloatTensor weights or None], float margin)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_DoubleMultiMarginCriterion_updateOutput(void*, THDoubleTensor*, THLongTensor*, THDoubleTensor*, int64_t, int, THDoubleTensor*, double);

PyObject * DoubleMultiMarginCriterion_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 8 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_LongTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          (THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 6)) || PyTuple_GET_ITEM(args, 6) == Py_None) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 7))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THDoubleTensor* arg_input = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THLongTensor* arg_target = THNN_LongTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THDoubleTensor* arg_output = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      int64_t arg_reduction = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_p = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      THDoubleTensor* arg_weights = (PyTuple_GET_ITEM(args, 6) == Py_None ? NULL : THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 6)));
      double arg_margin = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 7));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_DoubleMultiMarginCriterion_updateOutput(arg_state, arg_input, arg_target, arg_output, arg_reduction, arg_p, arg_weights, arg_margin);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "DoubleMultiMarginCriterion_updateOutput", 1, "(int state, torch.DoubleTensor input, torch.LongTensor target, torch.DoubleTensor output, int reduction, int p, [torch.DoubleTensor weights or None], float margin)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_FloatMultiMarginCriterion_updateGradInput(void*, THFloatTensor*, THLongTensor*, THFloatTensor*, THFloatTensor*, int64_t, int, THFloatTensor*, double);

PyObject * FloatMultiMarginCriterion_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 9 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_LongTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          (THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 7)) || PyTuple_GET_ITEM(args, 7) == Py_None) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 8))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THFloatTensor* arg_input = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THLongTensor* arg_target = THNN_LongTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THFloatTensor* arg_gradOutput = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THFloatTensor* arg_gradInput = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      int64_t arg_reduction = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_p = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      THFloatTensor* arg_weights = (PyTuple_GET_ITEM(args, 7) == Py_None ? NULL : THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 7)));
      double arg_margin = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 8));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_FloatMultiMarginCriterion_updateGradInput(arg_state, arg_input, arg_target, arg_gradOutput, arg_gradInput, arg_reduction, arg_p, arg_weights, arg_margin);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "FloatMultiMarginCriterion_updateGradInput", 1, "(int state, torch.FloatTensor input, torch.LongTensor target, torch.FloatTensor gradOutput, torch.FloatTensor gradInput, int reduction, int p, [torch.FloatTensor weights or None], float margin)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_DoubleMultiMarginCriterion_updateGradInput(void*, THDoubleTensor*, THLongTensor*, THDoubleTensor*, THDoubleTensor*, int64_t, int, THDoubleTensor*, double);

PyObject * DoubleMultiMarginCriterion_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 9 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_LongTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          (THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 7)) || PyTuple_GET_ITEM(args, 7) == Py_None) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 8))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THDoubleTensor* arg_input = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THLongTensor* arg_target = THNN_LongTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THDoubleTensor* arg_gradOutput = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THDoubleTensor* arg_gradInput = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      int64_t arg_reduction = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_p = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      THDoubleTensor* arg_weights = (PyTuple_GET_ITEM(args, 7) == Py_None ? NULL : THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 7)));
      double arg_margin = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 8));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_DoubleMultiMarginCriterion_updateGradInput(arg_state, arg_input, arg_target, arg_gradOutput, arg_gradInput, arg_reduction, arg_p, arg_weights, arg_margin);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "DoubleMultiMarginCriterion_updateGradInput", 1, "(int state, torch.DoubleTensor input, torch.LongTensor target, torch.DoubleTensor gradOutput, torch.DoubleTensor gradInput, int reduction, int p, [torch.DoubleTensor weights or None], float margin)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_FloatRReLU_updateOutput(void*, THFloatTensor*, THFloatTensor*, THFloatTensor*, double, double, bool, bool, at::Generator*);

PyObject * FloatRReLU_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 9 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 4)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 5)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 6)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 7)) &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(args, 8)) == THPGeneratorClass) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THFloatTensor* arg_input = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THFloatTensor* arg_output = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THFloatTensor* arg_noise = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      double arg_lower = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 4));
      double arg_upper = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 5));
      bool arg_train = (PyTuple_GET_ITEM(args, 6) == Py_True ? true : false);
      bool arg_inplace = (PyTuple_GET_ITEM(args, 7) == Py_True ? true : false);
      at::Generator* arg_generator = ((THPGenerator*)PyTuple_GET_ITEM(args, 8))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_FloatRReLU_updateOutput(arg_state, arg_input, arg_output, arg_noise, arg_lower, arg_upper, arg_train, arg_inplace, arg_generator);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "FloatRReLU_updateOutput", 1, "(int state, torch.FloatTensor input, torch.FloatTensor output, torch.FloatTensor noise, float lower, float upper, bool train, bool inplace, Generator generator)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_DoubleRReLU_updateOutput(void*, THDoubleTensor*, THDoubleTensor*, THDoubleTensor*, double, double, bool, bool, at::Generator*);

PyObject * DoubleRReLU_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 9 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 4)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 5)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 6)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 7)) &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(args, 8)) == THPGeneratorClass) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THDoubleTensor* arg_input = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THDoubleTensor* arg_output = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THDoubleTensor* arg_noise = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      double arg_lower = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 4));
      double arg_upper = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 5));
      bool arg_train = (PyTuple_GET_ITEM(args, 6) == Py_True ? true : false);
      bool arg_inplace = (PyTuple_GET_ITEM(args, 7) == Py_True ? true : false);
      at::Generator* arg_generator = ((THPGenerator*)PyTuple_GET_ITEM(args, 8))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_DoubleRReLU_updateOutput(arg_state, arg_input, arg_output, arg_noise, arg_lower, arg_upper, arg_train, arg_inplace, arg_generator);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "DoubleRReLU_updateOutput", 1, "(int state, torch.DoubleTensor input, torch.DoubleTensor output, torch.DoubleTensor noise, float lower, float upper, bool train, bool inplace, Generator generator)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_FloatRReLU_updateGradInput(void*, THFloatTensor*, THFloatTensor*, THFloatTensor*, THFloatTensor*, double, double, bool, bool);

PyObject * FloatRReLU_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 9 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 5)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 6)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 7)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 8))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THFloatTensor* arg_input = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THFloatTensor* arg_gradOutput = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THFloatTensor* arg_gradInput = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THFloatTensor* arg_noise = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      double arg_lower = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 5));
      double arg_upper = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 6));
      bool arg_train = (PyTuple_GET_ITEM(args, 7) == Py_True ? true : false);
      bool arg_inplace = (PyTuple_GET_ITEM(args, 8) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_FloatRReLU_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_noise, arg_lower, arg_upper, arg_train, arg_inplace);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "FloatRReLU_updateGradInput", 1, "(int state, torch.FloatTensor input, torch.FloatTensor gradOutput, torch.FloatTensor gradInput, torch.FloatTensor noise, float lower, float upper, bool train, bool inplace)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_DoubleRReLU_updateGradInput(void*, THDoubleTensor*, THDoubleTensor*, THDoubleTensor*, THDoubleTensor*, double, double, bool, bool);

PyObject * DoubleRReLU_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 9 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 5)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 6)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 7)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 8))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THDoubleTensor* arg_input = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THDoubleTensor* arg_gradOutput = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THDoubleTensor* arg_gradInput = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THDoubleTensor* arg_noise = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      double arg_lower = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 5));
      double arg_upper = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 6));
      bool arg_train = (PyTuple_GET_ITEM(args, 7) == Py_True ? true : false);
      bool arg_inplace = (PyTuple_GET_ITEM(args, 8) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_DoubleRReLU_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_noise, arg_lower, arg_upper, arg_train, arg_inplace);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "DoubleRReLU_updateGradInput", 1, "(int state, torch.DoubleTensor input, torch.DoubleTensor gradOutput, torch.DoubleTensor gradInput, torch.DoubleTensor noise, float lower, float upper, bool train, bool inplace)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_FloatSigmoid_updateOutput(void*, THFloatTensor*, THFloatTensor*);

PyObject * FloatSigmoid_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 3 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 2))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THFloatTensor* arg_input = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THFloatTensor* arg_output = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_FloatSigmoid_updateOutput(arg_state, arg_input, arg_output);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "FloatSigmoid_updateOutput", 1, "(int state, torch.FloatTensor input, torch.FloatTensor output)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_DoubleSigmoid_updateOutput(void*, THDoubleTensor*, THDoubleTensor*);

PyObject * DoubleSigmoid_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 3 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 2))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THDoubleTensor* arg_input = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THDoubleTensor* arg_output = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_DoubleSigmoid_updateOutput(arg_state, arg_input, arg_output);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "DoubleSigmoid_updateOutput", 1, "(int state, torch.DoubleTensor input, torch.DoubleTensor output)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_FloatSigmoid_updateGradInput(void*, THFloatTensor*, THFloatTensor*, THFloatTensor*);

PyObject * FloatSigmoid_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 4 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 3))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THFloatTensor* arg_gradOutput = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THFloatTensor* arg_gradInput = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THFloatTensor* arg_output = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_FloatSigmoid_updateGradInput(arg_state, arg_gradOutput, arg_gradInput, arg_output);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "FloatSigmoid_updateGradInput", 1, "(int state, torch.FloatTensor gradOutput, torch.FloatTensor gradInput, torch.FloatTensor output)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_DoubleSigmoid_updateGradInput(void*, THDoubleTensor*, THDoubleTensor*, THDoubleTensor*);

PyObject * DoubleSigmoid_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 4 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 3))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THDoubleTensor* arg_gradOutput = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THDoubleTensor* arg_gradInput = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THDoubleTensor* arg_output = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_DoubleSigmoid_updateGradInput(arg_state, arg_gradOutput, arg_gradInput, arg_output);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "DoubleSigmoid_updateGradInput", 1, "(int state, torch.DoubleTensor gradOutput, torch.DoubleTensor gradInput, torch.DoubleTensor output)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_FloatSmoothL1Criterion_updateOutput(void*, THFloatTensor*, THFloatTensor*, THFloatTensor*, int64_t);

PyObject * FloatSmoothL1Criterion_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 5 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THFloatTensor* arg_input = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THFloatTensor* arg_target = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THFloatTensor* arg_output = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      int64_t arg_reduction = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_FloatSmoothL1Criterion_updateOutput(arg_state, arg_input, arg_target, arg_output, arg_reduction);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "FloatSmoothL1Criterion_updateOutput", 1, "(int state, torch.FloatTensor input, torch.FloatTensor target, torch.FloatTensor output, int reduction)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_DoubleSmoothL1Criterion_updateOutput(void*, THDoubleTensor*, THDoubleTensor*, THDoubleTensor*, int64_t);

PyObject * DoubleSmoothL1Criterion_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 5 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THDoubleTensor* arg_input = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THDoubleTensor* arg_target = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THDoubleTensor* arg_output = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      int64_t arg_reduction = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_DoubleSmoothL1Criterion_updateOutput(arg_state, arg_input, arg_target, arg_output, arg_reduction);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "DoubleSmoothL1Criterion_updateOutput", 1, "(int state, torch.DoubleTensor input, torch.DoubleTensor target, torch.DoubleTensor output, int reduction)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_FloatSmoothL1Criterion_updateGradInput(void*, THFloatTensor*, THFloatTensor*, THFloatTensor*, THFloatTensor*, int64_t);

PyObject * FloatSmoothL1Criterion_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 6 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THFloatTensor* arg_input = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THFloatTensor* arg_target = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THFloatTensor* arg_gradOutput = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THFloatTensor* arg_gradInput = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      int64_t arg_reduction = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_FloatSmoothL1Criterion_updateGradInput(arg_state, arg_input, arg_target, arg_gradOutput, arg_gradInput, arg_reduction);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "FloatSmoothL1Criterion_updateGradInput", 1, "(int state, torch.FloatTensor input, torch.FloatTensor target, torch.FloatTensor gradOutput, torch.FloatTensor gradInput, int reduction)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_DoubleSmoothL1Criterion_updateGradInput(void*, THDoubleTensor*, THDoubleTensor*, THDoubleTensor*, THDoubleTensor*, int64_t);

PyObject * DoubleSmoothL1Criterion_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 6 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THDoubleTensor* arg_input = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THDoubleTensor* arg_target = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THDoubleTensor* arg_gradOutput = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THDoubleTensor* arg_gradInput = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      int64_t arg_reduction = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_DoubleSmoothL1Criterion_updateGradInput(arg_state, arg_input, arg_target, arg_gradOutput, arg_gradInput, arg_reduction);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "DoubleSmoothL1Criterion_updateGradInput", 1, "(int state, torch.DoubleTensor input, torch.DoubleTensor target, torch.DoubleTensor gradOutput, torch.DoubleTensor gradInput, int reduction)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_FloatSoftPlus_updateOutput(void*, THFloatTensor*, THFloatTensor*, double, double);

PyObject * FloatSoftPlus_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 5 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 3)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 4))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THFloatTensor* arg_input = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THFloatTensor* arg_output = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      double arg_beta = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 3));
      double arg_threshold = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 4));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_FloatSoftPlus_updateOutput(arg_state, arg_input, arg_output, arg_beta, arg_threshold);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "FloatSoftPlus_updateOutput", 1, "(int state, torch.FloatTensor input, torch.FloatTensor output, float beta, float threshold)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_DoubleSoftPlus_updateOutput(void*, THDoubleTensor*, THDoubleTensor*, double, double);

PyObject * DoubleSoftPlus_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 5 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 3)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 4))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THDoubleTensor* arg_input = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THDoubleTensor* arg_output = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      double arg_beta = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 3));
      double arg_threshold = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 4));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_DoubleSoftPlus_updateOutput(arg_state, arg_input, arg_output, arg_beta, arg_threshold);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "DoubleSoftPlus_updateOutput", 1, "(int state, torch.DoubleTensor input, torch.DoubleTensor output, float beta, float threshold)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_FloatSoftPlus_updateGradInput(void*, THFloatTensor*, THFloatTensor*, THFloatTensor*, THFloatTensor*, double, double);

PyObject * FloatSoftPlus_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 7 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 5)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 6))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THFloatTensor* arg_input = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THFloatTensor* arg_gradOutput = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THFloatTensor* arg_gradInput = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THFloatTensor* arg_output = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      double arg_beta = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 5));
      double arg_threshold = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 6));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_FloatSoftPlus_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_output, arg_beta, arg_threshold);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "FloatSoftPlus_updateGradInput", 1, "(int state, torch.FloatTensor input, torch.FloatTensor gradOutput, torch.FloatTensor gradInput, torch.FloatTensor output, float beta, float threshold)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_DoubleSoftPlus_updateGradInput(void*, THDoubleTensor*, THDoubleTensor*, THDoubleTensor*, THDoubleTensor*, double, double);

PyObject * DoubleSoftPlus_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 7 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 5)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 6))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THDoubleTensor* arg_input = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THDoubleTensor* arg_gradOutput = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THDoubleTensor* arg_gradInput = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THDoubleTensor* arg_output = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      double arg_beta = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 5));
      double arg_threshold = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 6));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_DoubleSoftPlus_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_output, arg_beta, arg_threshold);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "DoubleSoftPlus_updateGradInput", 1, "(int state, torch.DoubleTensor input, torch.DoubleTensor gradOutput, torch.DoubleTensor gradInput, torch.DoubleTensor output, float beta, float threshold)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_FloatSoftShrink_updateOutput(void*, THFloatTensor*, THFloatTensor*, double);

PyObject * FloatSoftShrink_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 4 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 3))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THFloatTensor* arg_input = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THFloatTensor* arg_output = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      double arg_lambda = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 3));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_FloatSoftShrink_updateOutput(arg_state, arg_input, arg_output, arg_lambda);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "FloatSoftShrink_updateOutput", 1, "(int state, torch.FloatTensor input, torch.FloatTensor output, float lambda)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_DoubleSoftShrink_updateOutput(void*, THDoubleTensor*, THDoubleTensor*, double);

PyObject * DoubleSoftShrink_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 4 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 3))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THDoubleTensor* arg_input = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THDoubleTensor* arg_output = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      double arg_lambda = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 3));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_DoubleSoftShrink_updateOutput(arg_state, arg_input, arg_output, arg_lambda);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "DoubleSoftShrink_updateOutput", 1, "(int state, torch.DoubleTensor input, torch.DoubleTensor output, float lambda)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_FloatSoftShrink_updateGradInput(void*, THFloatTensor*, THFloatTensor*, THFloatTensor*, double);

PyObject * FloatSoftShrink_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 5 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 4))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THFloatTensor* arg_input = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THFloatTensor* arg_gradOutput = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THFloatTensor* arg_gradInput = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      double arg_lambda = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 4));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_FloatSoftShrink_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_lambda);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "FloatSoftShrink_updateGradInput", 1, "(int state, torch.FloatTensor input, torch.FloatTensor gradOutput, torch.FloatTensor gradInput, float lambda)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_DoubleSoftShrink_updateGradInput(void*, THDoubleTensor*, THDoubleTensor*, THDoubleTensor*, double);

PyObject * DoubleSoftShrink_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 5 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 4))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THDoubleTensor* arg_input = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THDoubleTensor* arg_gradOutput = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THDoubleTensor* arg_gradInput = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      double arg_lambda = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 4));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_DoubleSoftShrink_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_lambda);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "DoubleSoftShrink_updateGradInput", 1, "(int state, torch.DoubleTensor input, torch.DoubleTensor gradOutput, torch.DoubleTensor gradInput, float lambda)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_FloatIndexLinear_updateOutput(void*, THLongTensor*, int64_t, THFloatTensor*, THLongTensor*, THLongTensor*, THFloatTensor*, THFloatTensor*, THFloatTensor*, THFloatTensor*, int);

PyObject * FloatIndexLinear_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 11 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_LongTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 2)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_LongTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_LongTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 7)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 8)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THLongTensor* arg_keys = THNN_LongTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      int64_t arg_keysOffset = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 2));
      THFloatTensor* arg_values = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THLongTensor* arg_sizes = THNN_LongTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THLongTensor* arg_cumSumSizes = THNN_LongTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THFloatTensor* arg_output = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      THFloatTensor* arg_weight = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 7));
      THFloatTensor* arg_bias = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 8));
      THFloatTensor* arg_normalizedValues = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 9));
      int arg_train = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_FloatIndexLinear_updateOutput(arg_state, arg_keys, arg_keysOffset, arg_values, arg_sizes, arg_cumSumSizes, arg_output, arg_weight, arg_bias, arg_normalizedValues, arg_train);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "FloatIndexLinear_updateOutput", 1, "(int state, torch.LongTensor keys, int keysOffset, torch.FloatTensor values, torch.LongTensor sizes, torch.LongTensor cumSumSizes, torch.FloatTensor output, torch.FloatTensor weight, torch.FloatTensor bias, torch.FloatTensor normalizedValues, int train)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_DoubleIndexLinear_updateOutput(void*, THLongTensor*, int64_t, THDoubleTensor*, THLongTensor*, THLongTensor*, THDoubleTensor*, THDoubleTensor*, THDoubleTensor*, THDoubleTensor*, int);

PyObject * DoubleIndexLinear_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 11 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_LongTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 2)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_LongTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_LongTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 7)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 8)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THLongTensor* arg_keys = THNN_LongTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      int64_t arg_keysOffset = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 2));
      THDoubleTensor* arg_values = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THLongTensor* arg_sizes = THNN_LongTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THLongTensor* arg_cumSumSizes = THNN_LongTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THDoubleTensor* arg_output = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      THDoubleTensor* arg_weight = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 7));
      THDoubleTensor* arg_bias = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 8));
      THDoubleTensor* arg_normalizedValues = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 9));
      int arg_train = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_DoubleIndexLinear_updateOutput(arg_state, arg_keys, arg_keysOffset, arg_values, arg_sizes, arg_cumSumSizes, arg_output, arg_weight, arg_bias, arg_normalizedValues, arg_train);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "DoubleIndexLinear_updateOutput", 1, "(int state, torch.LongTensor keys, int keysOffset, torch.DoubleTensor values, torch.LongTensor sizes, torch.LongTensor cumSumSizes, torch.DoubleTensor output, torch.DoubleTensor weight, torch.DoubleTensor bias, torch.DoubleTensor normalizedValues, int train)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_FloatIndexLinear_accGradParameters(void*, THLongTensor*, int64_t, THFloatTensor*, THLongTensor*, THLongTensor*, THFloatTensor*, THFloatTensor*, THFloatTensor*, THFloatTensor*, THFloatTensor*, THFloatTensor*, double, double);

PyObject * FloatIndexLinear_accGradParameters(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 14 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_LongTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 2)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_LongTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_LongTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 7)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 8)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 9)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 10)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 11)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 12)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 13))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THLongTensor* arg_keys = THNN_LongTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      int64_t arg_keysOffset = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 2));
      THFloatTensor* arg_values = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THLongTensor* arg_sizes = THNN_LongTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THLongTensor* arg_cumSumSizes = THNN_LongTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THFloatTensor* arg_gradOutput = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      THFloatTensor* arg_gradWeight = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 7));
      THFloatTensor* arg_gradBias = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 8));
      THFloatTensor* arg_weight = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 9));
      THFloatTensor* arg_bias = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 10));
      THFloatTensor* arg_valuesBuffer = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 11));
      double arg_weightDecay = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 12));
      double arg_scale = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 13));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_FloatIndexLinear_accGradParameters(arg_state, arg_keys, arg_keysOffset, arg_values, arg_sizes, arg_cumSumSizes, arg_gradOutput, arg_gradWeight, arg_gradBias, arg_weight, arg_bias, arg_valuesBuffer, arg_weightDecay, arg_scale);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "FloatIndexLinear_accGradParameters", 1, "(int state, torch.LongTensor keys, int keysOffset, torch.FloatTensor values, torch.LongTensor sizes, torch.LongTensor cumSumSizes, torch.FloatTensor gradOutput, torch.FloatTensor gradWeight, torch.FloatTensor gradBias, torch.FloatTensor weight, torch.FloatTensor bias, torch.FloatTensor valuesBuffer, float weightDecay, float scale)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_DoubleIndexLinear_accGradParameters(void*, THLongTensor*, int64_t, THDoubleTensor*, THLongTensor*, THLongTensor*, THDoubleTensor*, THDoubleTensor*, THDoubleTensor*, THDoubleTensor*, THDoubleTensor*, THDoubleTensor*, double, double);

PyObject * DoubleIndexLinear_accGradParameters(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 14 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_LongTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 2)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_LongTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_LongTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 7)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 8)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 9)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 10)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 11)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 12)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 13))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THLongTensor* arg_keys = THNN_LongTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      int64_t arg_keysOffset = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 2));
      THDoubleTensor* arg_values = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THLongTensor* arg_sizes = THNN_LongTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THLongTensor* arg_cumSumSizes = THNN_LongTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THDoubleTensor* arg_gradOutput = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      THDoubleTensor* arg_gradWeight = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 7));
      THDoubleTensor* arg_gradBias = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 8));
      THDoubleTensor* arg_weight = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 9));
      THDoubleTensor* arg_bias = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 10));
      THDoubleTensor* arg_valuesBuffer = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 11));
      double arg_weightDecay = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 12));
      double arg_scale = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 13));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_DoubleIndexLinear_accGradParameters(arg_state, arg_keys, arg_keysOffset, arg_values, arg_sizes, arg_cumSumSizes, arg_gradOutput, arg_gradWeight, arg_gradBias, arg_weight, arg_bias, arg_valuesBuffer, arg_weightDecay, arg_scale);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "DoubleIndexLinear_accGradParameters", 1, "(int state, torch.LongTensor keys, int keysOffset, torch.DoubleTensor values, torch.LongTensor sizes, torch.LongTensor cumSumSizes, torch.DoubleTensor gradOutput, torch.DoubleTensor gradWeight, torch.DoubleTensor gradBias, torch.DoubleTensor weight, torch.DoubleTensor bias, torch.DoubleTensor valuesBuffer, float weightDecay, float scale)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_FloatIndexLinear_accUpdateGradParameters(void*, THLongTensor*, int64_t, THFloatTensor*, THLongTensor*, THLongTensor*, THFloatTensor*, THFloatTensor*, THFloatTensor*, double, double);

PyObject * FloatIndexLinear_accUpdateGradParameters(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 11 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_LongTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 2)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_LongTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_LongTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 7)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 8)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 9)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 10))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THLongTensor* arg_keys = THNN_LongTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      int64_t arg_keysOffset = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 2));
      THFloatTensor* arg_values = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THLongTensor* arg_sizes = THNN_LongTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THLongTensor* arg_cumSumSizes = THNN_LongTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THFloatTensor* arg_gradOutput = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      THFloatTensor* arg_weight = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 7));
      THFloatTensor* arg_bias = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 8));
      double arg_weightDecay = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 9));
      double arg_scale = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 10));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_FloatIndexLinear_accUpdateGradParameters(arg_state, arg_keys, arg_keysOffset, arg_values, arg_sizes, arg_cumSumSizes, arg_gradOutput, arg_weight, arg_bias, arg_weightDecay, arg_scale);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "FloatIndexLinear_accUpdateGradParameters", 1, "(int state, torch.LongTensor keys, int keysOffset, torch.FloatTensor values, torch.LongTensor sizes, torch.LongTensor cumSumSizes, torch.FloatTensor gradOutput, torch.FloatTensor weight, torch.FloatTensor bias, float weightDecay, float scale)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_DoubleIndexLinear_accUpdateGradParameters(void*, THLongTensor*, int64_t, THDoubleTensor*, THLongTensor*, THLongTensor*, THDoubleTensor*, THDoubleTensor*, THDoubleTensor*, double, double);

PyObject * DoubleIndexLinear_accUpdateGradParameters(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 11 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_LongTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 2)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_LongTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_LongTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 7)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 8)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 9)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 10))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THLongTensor* arg_keys = THNN_LongTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      int64_t arg_keysOffset = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 2));
      THDoubleTensor* arg_values = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THLongTensor* arg_sizes = THNN_LongTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THLongTensor* arg_cumSumSizes = THNN_LongTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THDoubleTensor* arg_gradOutput = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      THDoubleTensor* arg_weight = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 7));
      THDoubleTensor* arg_bias = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 8));
      double arg_weightDecay = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 9));
      double arg_scale = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 10));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_DoubleIndexLinear_accUpdateGradParameters(arg_state, arg_keys, arg_keysOffset, arg_values, arg_sizes, arg_cumSumSizes, arg_gradOutput, arg_weight, arg_bias, arg_weightDecay, arg_scale);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "DoubleIndexLinear_accUpdateGradParameters", 1, "(int state, torch.LongTensor keys, int keysOffset, torch.DoubleTensor values, torch.LongTensor sizes, torch.LongTensor cumSumSizes, torch.DoubleTensor gradOutput, torch.DoubleTensor weight, torch.DoubleTensor bias, float weightDecay, float scale)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_FloatIndexLinear_updateParameters(void*, THFloatTensor*, THFloatTensor*, THFloatTensor*, THFloatTensor*, THLongTensor*, THLongTensor*, int64_t, double, double);

PyObject * FloatIndexLinear_updateParameters(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 10 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_LongTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_LongTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 8)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 9))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THFloatTensor* arg_gradWeight = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THFloatTensor* arg_gradBias = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THFloatTensor* arg_weight = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THFloatTensor* arg_bias = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THLongTensor* arg_runningKeys = THNN_LongTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THLongTensor* arg_cumSumSizes = THNN_LongTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int64_t arg_keysOffset = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      double arg_weightDecay = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 8));
      double arg_learningRate = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 9));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_FloatIndexLinear_updateParameters(arg_state, arg_gradWeight, arg_gradBias, arg_weight, arg_bias, arg_runningKeys, arg_cumSumSizes, arg_keysOffset, arg_weightDecay, arg_learningRate);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "FloatIndexLinear_updateParameters", 1, "(int state, torch.FloatTensor gradWeight, torch.FloatTensor gradBias, torch.FloatTensor weight, torch.FloatTensor bias, torch.LongTensor runningKeys, torch.LongTensor cumSumSizes, int keysOffset, float weightDecay, float learningRate)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_DoubleIndexLinear_updateParameters(void*, THDoubleTensor*, THDoubleTensor*, THDoubleTensor*, THDoubleTensor*, THLongTensor*, THLongTensor*, int64_t, double, double);

PyObject * DoubleIndexLinear_updateParameters(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 10 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_LongTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_LongTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 8)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 9))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THDoubleTensor* arg_gradWeight = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THDoubleTensor* arg_gradBias = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THDoubleTensor* arg_weight = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THDoubleTensor* arg_bias = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THLongTensor* arg_runningKeys = THNN_LongTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THLongTensor* arg_cumSumSizes = THNN_LongTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int64_t arg_keysOffset = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      double arg_weightDecay = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 8));
      double arg_learningRate = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 9));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_DoubleIndexLinear_updateParameters(arg_state, arg_gradWeight, arg_gradBias, arg_weight, arg_bias, arg_runningKeys, arg_cumSumSizes, arg_keysOffset, arg_weightDecay, arg_learningRate);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "DoubleIndexLinear_updateParameters", 1, "(int state, torch.DoubleTensor gradWeight, torch.DoubleTensor gradBias, torch.DoubleTensor weight, torch.DoubleTensor bias, torch.LongTensor runningKeys, torch.LongTensor cumSumSizes, int keysOffset, float weightDecay, float learningRate)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_FloatTemporalRowConvolution_updateOutput(void*, THFloatTensor*, THFloatTensor*, THFloatTensor*, THFloatTensor*, THFloatTensor*, THFloatTensor*, int, int, int, bool);

PyObject * FloatTemporalRowConvolution_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 11 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 10))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THFloatTensor* arg_input = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THFloatTensor* arg_output = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THFloatTensor* arg_weight = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THFloatTensor* arg_bias = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THFloatTensor* arg_finput = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THFloatTensor* arg_fgradInput = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      bool arg_featFirst = (PyTuple_GET_ITEM(args, 10) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_FloatTemporalRowConvolution_updateOutput(arg_state, arg_input, arg_output, arg_weight, arg_bias, arg_finput, arg_fgradInput, arg_kW, arg_dW, arg_padW, arg_featFirst);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "FloatTemporalRowConvolution_updateOutput", 1, "(int state, torch.FloatTensor input, torch.FloatTensor output, torch.FloatTensor weight, torch.FloatTensor bias, torch.FloatTensor finput, torch.FloatTensor fgradInput, int kW, int dW, int padW, bool featFirst)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_DoubleTemporalRowConvolution_updateOutput(void*, THDoubleTensor*, THDoubleTensor*, THDoubleTensor*, THDoubleTensor*, THDoubleTensor*, THDoubleTensor*, int, int, int, bool);

PyObject * DoubleTemporalRowConvolution_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 11 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 10))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THDoubleTensor* arg_input = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THDoubleTensor* arg_output = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THDoubleTensor* arg_weight = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THDoubleTensor* arg_bias = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THDoubleTensor* arg_finput = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THDoubleTensor* arg_fgradInput = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      bool arg_featFirst = (PyTuple_GET_ITEM(args, 10) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_DoubleTemporalRowConvolution_updateOutput(arg_state, arg_input, arg_output, arg_weight, arg_bias, arg_finput, arg_fgradInput, arg_kW, arg_dW, arg_padW, arg_featFirst);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "DoubleTemporalRowConvolution_updateOutput", 1, "(int state, torch.DoubleTensor input, torch.DoubleTensor output, torch.DoubleTensor weight, torch.DoubleTensor bias, torch.DoubleTensor finput, torch.DoubleTensor fgradInput, int kW, int dW, int padW, bool featFirst)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_FloatTemporalRowConvolution_updateGradInput(void*, THFloatTensor*, THFloatTensor*, THFloatTensor*, THFloatTensor*, THFloatTensor*, THFloatTensor*, int, int, int, bool);

PyObject * FloatTemporalRowConvolution_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 11 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 10))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THFloatTensor* arg_input = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THFloatTensor* arg_gradOutput = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THFloatTensor* arg_gradInput = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THFloatTensor* arg_weight = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THFloatTensor* arg_finput = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THFloatTensor* arg_fgradInput = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      bool arg_featFirst = (PyTuple_GET_ITEM(args, 10) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_FloatTemporalRowConvolution_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_weight, arg_finput, arg_fgradInput, arg_kW, arg_dW, arg_padW, arg_featFirst);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "FloatTemporalRowConvolution_updateGradInput", 1, "(int state, torch.FloatTensor input, torch.FloatTensor gradOutput, torch.FloatTensor gradInput, torch.FloatTensor weight, torch.FloatTensor finput, torch.FloatTensor fgradInput, int kW, int dW, int padW, bool featFirst)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_DoubleTemporalRowConvolution_updateGradInput(void*, THDoubleTensor*, THDoubleTensor*, THDoubleTensor*, THDoubleTensor*, THDoubleTensor*, THDoubleTensor*, int, int, int, bool);

PyObject * DoubleTemporalRowConvolution_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 11 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 10))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THDoubleTensor* arg_input = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THDoubleTensor* arg_gradOutput = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THDoubleTensor* arg_gradInput = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THDoubleTensor* arg_weight = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THDoubleTensor* arg_finput = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THDoubleTensor* arg_fgradInput = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      bool arg_featFirst = (PyTuple_GET_ITEM(args, 10) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_DoubleTemporalRowConvolution_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_weight, arg_finput, arg_fgradInput, arg_kW, arg_dW, arg_padW, arg_featFirst);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "DoubleTemporalRowConvolution_updateGradInput", 1, "(int state, torch.DoubleTensor input, torch.DoubleTensor gradOutput, torch.DoubleTensor gradInput, torch.DoubleTensor weight, torch.DoubleTensor finput, torch.DoubleTensor fgradInput, int kW, int dW, int padW, bool featFirst)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_FloatTemporalRowConvolution_accGradParameters(void*, THFloatTensor*, THFloatTensor*, THFloatTensor*, THFloatTensor*, THFloatTensor*, THFloatTensor*, int, int, int, bool, double);

PyObject * FloatTemporalRowConvolution_accGradParameters(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 12 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 10)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 11))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THFloatTensor* arg_input = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THFloatTensor* arg_gradOutput = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THFloatTensor* arg_gradWeight = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THFloatTensor* arg_gradBias = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THFloatTensor* arg_finput = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THFloatTensor* arg_fgradInput = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      bool arg_featFirst = (PyTuple_GET_ITEM(args, 10) == Py_True ? true : false);
      double arg_scale = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 11));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_FloatTemporalRowConvolution_accGradParameters(arg_state, arg_input, arg_gradOutput, arg_gradWeight, arg_gradBias, arg_finput, arg_fgradInput, arg_kW, arg_dW, arg_padW, arg_featFirst, arg_scale);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "FloatTemporalRowConvolution_accGradParameters", 1, "(int state, torch.FloatTensor input, torch.FloatTensor gradOutput, torch.FloatTensor gradWeight, torch.FloatTensor gradBias, torch.FloatTensor finput, torch.FloatTensor fgradInput, int kW, int dW, int padW, bool featFirst, float scale)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_DoubleTemporalRowConvolution_accGradParameters(void*, THDoubleTensor*, THDoubleTensor*, THDoubleTensor*, THDoubleTensor*, THDoubleTensor*, THDoubleTensor*, int, int, int, bool, double);

PyObject * DoubleTemporalRowConvolution_accGradParameters(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 12 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 10)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 11))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THDoubleTensor* arg_input = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THDoubleTensor* arg_gradOutput = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THDoubleTensor* arg_gradWeight = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THDoubleTensor* arg_gradBias = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THDoubleTensor* arg_finput = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THDoubleTensor* arg_fgradInput = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      bool arg_featFirst = (PyTuple_GET_ITEM(args, 10) == Py_True ? true : false);
      double arg_scale = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 11));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_DoubleTemporalRowConvolution_accGradParameters(arg_state, arg_input, arg_gradOutput, arg_gradWeight, arg_gradBias, arg_finput, arg_fgradInput, arg_kW, arg_dW, arg_padW, arg_featFirst, arg_scale);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "DoubleTemporalRowConvolution_accGradParameters", 1, "(int state, torch.DoubleTensor input, torch.DoubleTensor gradOutput, torch.DoubleTensor gradWeight, torch.DoubleTensor gradBias, torch.DoubleTensor finput, torch.DoubleTensor fgradInput, int kW, int dW, int padW, bool featFirst, float scale)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_FloatSpatialConvolutionMM_updateGradInput(void*, THFloatTensor*, THFloatTensor*, THFloatTensor*, THFloatTensor*, THFloatTensor*, THFloatTensor*, int, int, int, int, int, int);

PyObject * FloatSpatialConvolutionMM_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 13 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THFloatTensor* arg_input = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THFloatTensor* arg_gradOutput = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THFloatTensor* arg_gradInput = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THFloatTensor* arg_weight = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THFloatTensor* arg_finput = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THFloatTensor* arg_fgradInput = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_FloatSpatialConvolutionMM_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_weight, arg_finput, arg_fgradInput, arg_kW, arg_kH, arg_dW, arg_dH, arg_padW, arg_padH);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "FloatSpatialConvolutionMM_updateGradInput", 1, "(int state, torch.FloatTensor input, torch.FloatTensor gradOutput, torch.FloatTensor gradInput, torch.FloatTensor weight, torch.FloatTensor finput, torch.FloatTensor fgradInput, int kW, int kH, int dW, int dH, int padW, int padH)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_DoubleSpatialConvolutionMM_updateGradInput(void*, THDoubleTensor*, THDoubleTensor*, THDoubleTensor*, THDoubleTensor*, THDoubleTensor*, THDoubleTensor*, int, int, int, int, int, int);

PyObject * DoubleSpatialConvolutionMM_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 13 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THDoubleTensor* arg_input = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THDoubleTensor* arg_gradOutput = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THDoubleTensor* arg_gradInput = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THDoubleTensor* arg_weight = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THDoubleTensor* arg_finput = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THDoubleTensor* arg_fgradInput = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_DoubleSpatialConvolutionMM_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_weight, arg_finput, arg_fgradInput, arg_kW, arg_kH, arg_dW, arg_dH, arg_padW, arg_padH);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "DoubleSpatialConvolutionMM_updateGradInput", 1, "(int state, torch.DoubleTensor input, torch.DoubleTensor gradOutput, torch.DoubleTensor gradInput, torch.DoubleTensor weight, torch.DoubleTensor finput, torch.DoubleTensor fgradInput, int kW, int kH, int dW, int dH, int padW, int padH)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_FloatSpatialConvolutionMM_accGradParameters(void*, THFloatTensor*, THFloatTensor*, THFloatTensor*, THFloatTensor*, THFloatTensor*, THFloatTensor*, int, int, int, int, int, int, double);

PyObject * FloatSpatialConvolutionMM_accGradParameters(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 14 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          (THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 4)) || PyTuple_GET_ITEM(args, 4) == Py_None) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 13))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THFloatTensor* arg_input = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THFloatTensor* arg_gradOutput = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THFloatTensor* arg_gradWeight = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THFloatTensor* arg_gradBias = (PyTuple_GET_ITEM(args, 4) == Py_None ? NULL : THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 4)));
      THFloatTensor* arg_finput = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THFloatTensor* arg_fgradInput = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      double arg_scale = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 13));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_FloatSpatialConvolutionMM_accGradParameters(arg_state, arg_input, arg_gradOutput, arg_gradWeight, arg_gradBias, arg_finput, arg_fgradInput, arg_kW, arg_kH, arg_dW, arg_dH, arg_padW, arg_padH, arg_scale);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "FloatSpatialConvolutionMM_accGradParameters", 1, "(int state, torch.FloatTensor input, torch.FloatTensor gradOutput, torch.FloatTensor gradWeight, [torch.FloatTensor gradBias or None], torch.FloatTensor finput, torch.FloatTensor fgradInput, int kW, int kH, int dW, int dH, int padW, int padH, float scale)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_DoubleSpatialConvolutionMM_accGradParameters(void*, THDoubleTensor*, THDoubleTensor*, THDoubleTensor*, THDoubleTensor*, THDoubleTensor*, THDoubleTensor*, int, int, int, int, int, int, double);

PyObject * DoubleSpatialConvolutionMM_accGradParameters(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 14 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          (THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 4)) || PyTuple_GET_ITEM(args, 4) == Py_None) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 13))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THDoubleTensor* arg_input = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THDoubleTensor* arg_gradOutput = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THDoubleTensor* arg_gradWeight = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THDoubleTensor* arg_gradBias = (PyTuple_GET_ITEM(args, 4) == Py_None ? NULL : THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 4)));
      THDoubleTensor* arg_finput = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THDoubleTensor* arg_fgradInput = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      double arg_scale = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 13));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_DoubleSpatialConvolutionMM_accGradParameters(arg_state, arg_input, arg_gradOutput, arg_gradWeight, arg_gradBias, arg_finput, arg_fgradInput, arg_kW, arg_kH, arg_dW, arg_dH, arg_padW, arg_padH, arg_scale);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "DoubleSpatialConvolutionMM_accGradParameters", 1, "(int state, torch.DoubleTensor input, torch.DoubleTensor gradOutput, torch.DoubleTensor gradWeight, [torch.DoubleTensor gradBias or None], torch.DoubleTensor finput, torch.DoubleTensor fgradInput, int kW, int kH, int dW, int dH, int padW, int padH, float scale)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_Floatunfolded_acc(THFloatTensor*, THFloatTensor*, int, int, int, int, int, int, int, int, int, int, int);

PyObject * Floatunfolded_acc(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 13 &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 0)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 2)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12))) {
      
      
      THFloatTensor* arg_finput = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 0));
      THFloatTensor* arg_input = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 2));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 3));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_nInputPlane = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_inputWidth = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_inputHeight = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_osizeW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_outputHeight = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_Floatunfolded_acc(arg_finput, arg_input, arg_kW, arg_kH, arg_dW, arg_dH, arg_padW, arg_padH, arg_nInputPlane, arg_inputWidth, arg_inputHeight, arg_osizeW, arg_outputHeight);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "Floatunfolded_acc", 1, "(torch.FloatTensor finput, torch.FloatTensor input, int kW, int kH, int dW, int dH, int padW, int padH, int nInputPlane, int inputWidth, int inputHeight, int osizeW, int outputHeight)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_Doubleunfolded_acc(THDoubleTensor*, THDoubleTensor*, int, int, int, int, int, int, int, int, int, int, int);

PyObject * Doubleunfolded_acc(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 13 &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 0)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 2)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12))) {
      
      
      THDoubleTensor* arg_finput = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 0));
      THDoubleTensor* arg_input = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 2));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 3));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_nInputPlane = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_inputWidth = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_inputHeight = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_osizeW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_outputHeight = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_Doubleunfolded_acc(arg_finput, arg_input, arg_kW, arg_kH, arg_dW, arg_dH, arg_padW, arg_padH, arg_nInputPlane, arg_inputWidth, arg_inputHeight, arg_osizeW, arg_outputHeight);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "Doubleunfolded_acc", 1, "(torch.DoubleTensor finput, torch.DoubleTensor input, int kW, int kH, int dW, int dH, int padW, int padH, int nInputPlane, int inputWidth, int inputHeight, int osizeW, int outputHeight)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_Floatunfolded_copy(THFloatTensor*, THFloatTensor*, int, int, int, int, int, int, int, int, int, int, int);

PyObject * Floatunfolded_copy(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 13 &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 0)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 2)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12))) {
      
      
      THFloatTensor* arg_finput = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 0));
      THFloatTensor* arg_input = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 2));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 3));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_nInputPlane = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_inputWidth = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_inputHeight = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_outputWidth = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_outputHeight = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_Floatunfolded_copy(arg_finput, arg_input, arg_kW, arg_kH, arg_dW, arg_dH, arg_padW, arg_padH, arg_nInputPlane, arg_inputWidth, arg_inputHeight, arg_outputWidth, arg_outputHeight);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "Floatunfolded_copy", 1, "(torch.FloatTensor finput, torch.FloatTensor input, int kW, int kH, int dW, int dH, int padW, int padH, int nInputPlane, int inputWidth, int inputHeight, int outputWidth, int outputHeight)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_Doubleunfolded_copy(THDoubleTensor*, THDoubleTensor*, int, int, int, int, int, int, int, int, int, int, int);

PyObject * Doubleunfolded_copy(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 13 &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 0)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 2)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12))) {
      
      
      THDoubleTensor* arg_finput = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 0));
      THDoubleTensor* arg_input = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 2));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 3));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      int arg_padW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_padH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_nInputPlane = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_inputWidth = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_inputHeight = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_outputWidth = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_outputHeight = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_Doubleunfolded_copy(arg_finput, arg_input, arg_kW, arg_kH, arg_dW, arg_dH, arg_padW, arg_padH, arg_nInputPlane, arg_inputWidth, arg_inputHeight, arg_outputWidth, arg_outputHeight);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "Doubleunfolded_copy", 1, "(torch.DoubleTensor finput, torch.DoubleTensor input, int kW, int kH, int dW, int dH, int padW, int padH, int nInputPlane, int inputWidth, int inputHeight, int outputWidth, int outputHeight)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_FloatFeatureLPPooling_updateOutput(void*, THFloatTensor*, THFloatTensor*, double, int, int, bool);

PyObject * FloatFeatureLPPooling_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 7 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 6))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THFloatTensor* arg_input = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THFloatTensor* arg_output = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      double arg_power = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 3));
      int arg_width = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_stride = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      bool arg_batchMode = (PyTuple_GET_ITEM(args, 6) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_FloatFeatureLPPooling_updateOutput(arg_state, arg_input, arg_output, arg_power, arg_width, arg_stride, arg_batchMode);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "FloatFeatureLPPooling_updateOutput", 1, "(int state, torch.FloatTensor input, torch.FloatTensor output, float power, int width, int stride, bool batchMode)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_DoubleFeatureLPPooling_updateOutput(void*, THDoubleTensor*, THDoubleTensor*, double, int, int, bool);

PyObject * DoubleFeatureLPPooling_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 7 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 6))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THDoubleTensor* arg_input = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THDoubleTensor* arg_output = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      double arg_power = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 3));
      int arg_width = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      int arg_stride = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      bool arg_batchMode = (PyTuple_GET_ITEM(args, 6) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_DoubleFeatureLPPooling_updateOutput(arg_state, arg_input, arg_output, arg_power, arg_width, arg_stride, arg_batchMode);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "DoubleFeatureLPPooling_updateOutput", 1, "(int state, torch.DoubleTensor input, torch.DoubleTensor output, float power, int width, int stride, bool batchMode)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_FloatFeatureLPPooling_updateGradInput(void*, THFloatTensor*, THFloatTensor*, THFloatTensor*, THFloatTensor*, double, int, int, bool);

PyObject * FloatFeatureLPPooling_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 9 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 8))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THFloatTensor* arg_gradOutput = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THFloatTensor* arg_input = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THFloatTensor* arg_output = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THFloatTensor* arg_gradInput = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      double arg_power = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 5));
      int arg_width = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_stride = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      bool arg_batchMode = (PyTuple_GET_ITEM(args, 8) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_FloatFeatureLPPooling_updateGradInput(arg_state, arg_gradOutput, arg_input, arg_output, arg_gradInput, arg_power, arg_width, arg_stride, arg_batchMode);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "FloatFeatureLPPooling_updateGradInput", 1, "(int state, torch.FloatTensor gradOutput, torch.FloatTensor input, torch.FloatTensor output, torch.FloatTensor gradInput, float power, int width, int stride, bool batchMode)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_DoubleFeatureLPPooling_updateGradInput(void*, THDoubleTensor*, THDoubleTensor*, THDoubleTensor*, THDoubleTensor*, double, int, int, bool);

PyObject * DoubleFeatureLPPooling_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 9 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 5)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          PyBool_Check(PyTuple_GET_ITEM(args, 8))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THDoubleTensor* arg_gradOutput = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THDoubleTensor* arg_input = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THDoubleTensor* arg_output = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THDoubleTensor* arg_gradInput = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      double arg_power = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 5));
      int arg_width = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 6));
      int arg_stride = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      bool arg_batchMode = (PyTuple_GET_ITEM(args, 8) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_DoubleFeatureLPPooling_updateGradInput(arg_state, arg_gradOutput, arg_input, arg_output, arg_gradInput, arg_power, arg_width, arg_stride, arg_batchMode);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "DoubleFeatureLPPooling_updateGradInput", 1, "(int state, torch.DoubleTensor gradOutput, torch.DoubleTensor input, torch.DoubleTensor output, torch.DoubleTensor gradInput, float power, int width, int stride, bool batchMode)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_FloatTanh_updateOutput(void*, THFloatTensor*, THFloatTensor*);

PyObject * FloatTanh_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 3 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 2))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THFloatTensor* arg_input = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THFloatTensor* arg_output = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_FloatTanh_updateOutput(arg_state, arg_input, arg_output);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "FloatTanh_updateOutput", 1, "(int state, torch.FloatTensor input, torch.FloatTensor output)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_DoubleTanh_updateOutput(void*, THDoubleTensor*, THDoubleTensor*);

PyObject * DoubleTanh_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 3 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 2))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THDoubleTensor* arg_input = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THDoubleTensor* arg_output = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_DoubleTanh_updateOutput(arg_state, arg_input, arg_output);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "DoubleTanh_updateOutput", 1, "(int state, torch.DoubleTensor input, torch.DoubleTensor output)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_FloatTanh_updateGradInput(void*, THFloatTensor*, THFloatTensor*, THFloatTensor*);

PyObject * FloatTanh_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 4 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 3))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THFloatTensor* arg_gradOutput = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THFloatTensor* arg_gradInput = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THFloatTensor* arg_output = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_FloatTanh_updateGradInput(arg_state, arg_gradOutput, arg_gradInput, arg_output);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "FloatTanh_updateGradInput", 1, "(int state, torch.FloatTensor gradOutput, torch.FloatTensor gradInput, torch.FloatTensor output)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_DoubleTanh_updateGradInput(void*, THDoubleTensor*, THDoubleTensor*, THDoubleTensor*);

PyObject * DoubleTanh_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 4 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 3))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THDoubleTensor* arg_gradOutput = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THDoubleTensor* arg_gradInput = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THDoubleTensor* arg_output = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_DoubleTanh_updateGradInput(arg_state, arg_gradOutput, arg_gradInput, arg_output);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "DoubleTanh_updateGradInput", 1, "(int state, torch.DoubleTensor gradOutput, torch.DoubleTensor gradInput, torch.DoubleTensor output)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_FloatVolumetricConvolutionMM_updateGradInput(void*, THFloatTensor*, THFloatTensor*, THFloatTensor*, THFloatTensor*, THFloatTensor*, THFloatTensor*, int, int, int, int, int, int, int, int, int);

PyObject * FloatVolumetricConvolutionMM_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 16 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 13)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 14)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 15))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THFloatTensor* arg_input = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THFloatTensor* arg_gradOutput = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THFloatTensor* arg_gradInput = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THFloatTensor* arg_weight = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THFloatTensor* arg_finput = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THFloatTensor* arg_fgradInput = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int arg_kT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_dT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      int arg_pT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 13));
      int arg_pW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 14));
      int arg_pH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 15));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_FloatVolumetricConvolutionMM_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_weight, arg_finput, arg_fgradInput, arg_kT, arg_kW, arg_kH, arg_dT, arg_dW, arg_dH, arg_pT, arg_pW, arg_pH);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "FloatVolumetricConvolutionMM_updateGradInput", 1, "(int state, torch.FloatTensor input, torch.FloatTensor gradOutput, torch.FloatTensor gradInput, torch.FloatTensor weight, torch.FloatTensor finput, torch.FloatTensor fgradInput, int kT, int kW, int kH, int dT, int dW, int dH, int pT, int pW, int pH)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_DoubleVolumetricConvolutionMM_updateGradInput(void*, THDoubleTensor*, THDoubleTensor*, THDoubleTensor*, THDoubleTensor*, THDoubleTensor*, THDoubleTensor*, int, int, int, int, int, int, int, int, int);

PyObject * DoubleVolumetricConvolutionMM_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 16 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 13)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 14)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 15))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THDoubleTensor* arg_input = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THDoubleTensor* arg_gradOutput = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THDoubleTensor* arg_gradInput = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THDoubleTensor* arg_weight = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      THDoubleTensor* arg_finput = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THDoubleTensor* arg_fgradInput = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int arg_kT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_dT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      int arg_pT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 13));
      int arg_pW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 14));
      int arg_pH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 15));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_DoubleVolumetricConvolutionMM_updateGradInput(arg_state, arg_input, arg_gradOutput, arg_gradInput, arg_weight, arg_finput, arg_fgradInput, arg_kT, arg_kW, arg_kH, arg_dT, arg_dW, arg_dH, arg_pT, arg_pW, arg_pH);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "DoubleVolumetricConvolutionMM_updateGradInput", 1, "(int state, torch.DoubleTensor input, torch.DoubleTensor gradOutput, torch.DoubleTensor gradInput, torch.DoubleTensor weight, torch.DoubleTensor finput, torch.DoubleTensor fgradInput, int kT, int kW, int kH, int dT, int dW, int dH, int pT, int pW, int pH)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_FloatVolumetricConvolutionMM_accGradParameters(void*, THFloatTensor*, THFloatTensor*, THFloatTensor*, THFloatTensor*, THFloatTensor*, THFloatTensor*, int, int, int, int, int, int, int, int, int, double);

PyObject * FloatVolumetricConvolutionMM_accGradParameters(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 17 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          (THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 4)) || PyTuple_GET_ITEM(args, 4) == Py_None) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 13)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 14)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 15)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 16))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THFloatTensor* arg_input = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THFloatTensor* arg_gradOutput = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THFloatTensor* arg_gradWeight = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THFloatTensor* arg_gradBias = (PyTuple_GET_ITEM(args, 4) == Py_None ? NULL : THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 4)));
      THFloatTensor* arg_finput = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THFloatTensor* arg_fgradInput = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int arg_kT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_dT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      int arg_pT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 13));
      int arg_pW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 14));
      int arg_pH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 15));
      double arg_scale = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 16));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_FloatVolumetricConvolutionMM_accGradParameters(arg_state, arg_input, arg_gradOutput, arg_gradWeight, arg_gradBias, arg_finput, arg_fgradInput, arg_kT, arg_kW, arg_kH, arg_dT, arg_dW, arg_dH, arg_pT, arg_pW, arg_pH, arg_scale);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "FloatVolumetricConvolutionMM_accGradParameters", 1, "(int state, torch.FloatTensor input, torch.FloatTensor gradOutput, torch.FloatTensor gradWeight, [torch.FloatTensor gradBias or None], torch.FloatTensor finput, torch.FloatTensor fgradInput, int kT, int kW, int kH, int dT, int dW, int dH, int pT, int pW, int pH, float scale)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_DoubleVolumetricConvolutionMM_accGradParameters(void*, THDoubleTensor*, THDoubleTensor*, THDoubleTensor*, THDoubleTensor*, THDoubleTensor*, THDoubleTensor*, int, int, int, int, int, int, int, int, int, double);

PyObject * DoubleVolumetricConvolutionMM_accGradParameters(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 17 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          (THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 4)) || PyTuple_GET_ITEM(args, 4) == Py_None) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 5)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 9)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 10)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 11)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 12)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 13)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 14)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 15)) &&
          THPDoubleUtils_checkReal(PyTuple_GET_ITEM(args, 16))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THDoubleTensor* arg_input = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THDoubleTensor* arg_gradOutput = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THDoubleTensor* arg_gradWeight = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THDoubleTensor* arg_gradBias = (PyTuple_GET_ITEM(args, 4) == Py_None ? NULL : THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 4)));
      THDoubleTensor* arg_finput = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 5));
      THDoubleTensor* arg_fgradInput = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int arg_kT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      int arg_kW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      int arg_kH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 9));
      int arg_dT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 10));
      int arg_dW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 11));
      int arg_dH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 12));
      int arg_pT = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 13));
      int arg_pW = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 14));
      int arg_pH = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 15));
      double arg_scale = THPDoubleUtils_unpackReal(PyTuple_GET_ITEM(args, 16));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_DoubleVolumetricConvolutionMM_accGradParameters(arg_state, arg_input, arg_gradOutput, arg_gradWeight, arg_gradBias, arg_finput, arg_fgradInput, arg_kT, arg_kW, arg_kH, arg_dT, arg_dW, arg_dH, arg_pT, arg_pW, arg_pH, arg_scale);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "DoubleVolumetricConvolutionMM_accGradParameters", 1, "(int state, torch.DoubleTensor input, torch.DoubleTensor gradOutput, torch.DoubleTensor gradWeight, [torch.DoubleTensor gradBias or None], torch.DoubleTensor finput, torch.DoubleTensor fgradInput, int kT, int kW, int kH, int dT, int dW, int dH, int pT, int pW, int pH, float scale)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_FloatSpatialClassNLLCriterion_updateOutput(void*, THFloatTensor*, THLongTensor*, THFloatTensor*, int64_t, THFloatTensor*, THFloatTensor*, int64_t);

PyObject * FloatSpatialClassNLLCriterion_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 8 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_LongTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          (THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 5)) || PyTuple_GET_ITEM(args, 5) == Py_None) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THFloatTensor* arg_input = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THLongTensor* arg_target = THNN_LongTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THFloatTensor* arg_output = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      int64_t arg_reduction = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      THFloatTensor* arg_weights = (PyTuple_GET_ITEM(args, 5) == Py_None ? NULL : THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 5)));
      THFloatTensor* arg_total_weight = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int64_t arg_ignore_index = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_FloatSpatialClassNLLCriterion_updateOutput(arg_state, arg_input, arg_target, arg_output, arg_reduction, arg_weights, arg_total_weight, arg_ignore_index);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "FloatSpatialClassNLLCriterion_updateOutput", 1, "(int state, torch.FloatTensor input, torch.LongTensor target, torch.FloatTensor output, int reduction, [torch.FloatTensor weights or None], torch.FloatTensor total_weight, int ignore_index)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_DoubleSpatialClassNLLCriterion_updateOutput(void*, THDoubleTensor*, THLongTensor*, THDoubleTensor*, int64_t, THDoubleTensor*, THDoubleTensor*, int64_t);

PyObject * DoubleSpatialClassNLLCriterion_updateOutput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 8 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_LongTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 4)) &&
          (THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 5)) || PyTuple_GET_ITEM(args, 5) == Py_None) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 6)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 7))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THDoubleTensor* arg_input = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THLongTensor* arg_target = THNN_LongTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THDoubleTensor* arg_output = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      int64_t arg_reduction = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 4));
      THDoubleTensor* arg_weights = (PyTuple_GET_ITEM(args, 5) == Py_None ? NULL : THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 5)));
      THDoubleTensor* arg_total_weight = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 6));
      int64_t arg_ignore_index = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 7));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_DoubleSpatialClassNLLCriterion_updateOutput(arg_state, arg_input, arg_target, arg_output, arg_reduction, arg_weights, arg_total_weight, arg_ignore_index);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "DoubleSpatialClassNLLCriterion_updateOutput", 1, "(int state, torch.DoubleTensor input, torch.LongTensor target, torch.DoubleTensor output, int reduction, [torch.DoubleTensor weights or None], torch.DoubleTensor total_weight, int ignore_index)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_FloatSpatialClassNLLCriterion_updateGradInput(void*, THFloatTensor*, THLongTensor*, THFloatTensor*, THFloatTensor*, int64_t, THFloatTensor*, THFloatTensor*, int64_t);

PyObject * FloatSpatialClassNLLCriterion_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 9 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_LongTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          (THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 6)) || PyTuple_GET_ITEM(args, 6) == Py_None) &&
          THNN_FloatTensor_Check(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THFloatTensor* arg_input = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THLongTensor* arg_target = THNN_LongTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THFloatTensor* arg_gradOutput = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THFloatTensor* arg_gradInput = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      int64_t arg_reduction = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      THFloatTensor* arg_weights = (PyTuple_GET_ITEM(args, 6) == Py_None ? NULL : THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 6)));
      THFloatTensor* arg_total_weight = THNN_FloatTensor_Unpack(PyTuple_GET_ITEM(args, 7));
      int64_t arg_ignore_index = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_FloatSpatialClassNLLCriterion_updateGradInput(arg_state, arg_input, arg_target, arg_gradOutput, arg_gradInput, arg_reduction, arg_weights, arg_total_weight, arg_ignore_index);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "FloatSpatialClassNLLCriterion_updateGradInput", 1, "(int state, torch.FloatTensor input, torch.LongTensor target, torch.FloatTensor gradOutput, torch.FloatTensor gradInput, int reduction, [torch.FloatTensor weights or None], torch.FloatTensor total_weight, int ignore_index)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    


TH_API void THNN_DoubleSpatialClassNLLCriterion_updateGradInput(void*, THDoubleTensor*, THLongTensor*, THDoubleTensor*, THDoubleTensor*, int64_t, THDoubleTensor*, THDoubleTensor*, int64_t);

PyObject * DoubleSpatialClassNLLCriterion_updateGradInput(PyObject *_unused, PyObject *args)
{
  HANDLE_TH_ERRORS
  int __argcount = args ? PyTuple_Size(args) : 0;
    
    if (__argcount == 9 &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 0)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 1)) &&
          THNN_LongTensor_Check(PyTuple_GET_ITEM(args, 2)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 3)) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 4)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 5)) &&
          (THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 6)) || PyTuple_GET_ITEM(args, 6) == Py_None) &&
          THNN_DoubleTensor_Check(PyTuple_GET_ITEM(args, 7)) &&
          THPUtils_checkLong(PyTuple_GET_ITEM(args, 8))) {
      
      
      void* arg_state = (void*)THPUtils_unpackLong(PyTuple_GET_ITEM(args, 0));
      THDoubleTensor* arg_input = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 1));
      THLongTensor* arg_target = THNN_LongTensor_Unpack(PyTuple_GET_ITEM(args, 2));
      THDoubleTensor* arg_gradOutput = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 3));
      THDoubleTensor* arg_gradInput = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 4));
      int64_t arg_reduction = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 5));
      THDoubleTensor* arg_weights = (PyTuple_GET_ITEM(args, 6) == Py_None ? NULL : THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 6)));
      THDoubleTensor* arg_total_weight = THNN_DoubleTensor_Unpack(PyTuple_GET_ITEM(args, 7));
      int64_t arg_ignore_index = THPUtils_unpackLong(PyTuple_GET_ITEM(args, 8));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THNN_DoubleSpatialClassNLLCriterion_updateGradInput(arg_state, arg_input, arg_target, arg_gradOutput, arg_gradInput, arg_reduction, arg_weights, arg_total_weight, arg_ignore_index);
        Py_BLOCK_THREADS;
        Py_RETURN_NONE;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
  } else {
    THPUtils_invalidArguments(args, NULL, "DoubleSpatialClassNLLCriterion_updateGradInput", 1, "(int state, torch.DoubleTensor input, torch.LongTensor target, torch.DoubleTensor gradOutput, torch.DoubleTensor gradInput, int reduction, [torch.DoubleTensor weights or None], torch.DoubleTensor total_weight, int ignore_index)");
    return NULL;
  }
  END_HANDLE_TH_ERRORS
}
    



static PyMethodDef module_methods[] = {
  {"FloatSpatialConvolutionMM_updateOutput", (PyCFunction)FloatSpatialConvolutionMM_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"DoubleSpatialConvolutionMM_updateOutput", (PyCFunction)DoubleSpatialConvolutionMM_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"FloatVolumetricConvolutionMM_updateOutput", (PyCFunction)FloatVolumetricConvolutionMM_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"DoubleVolumetricConvolutionMM_updateOutput", (PyCFunction)DoubleVolumetricConvolutionMM_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"FloatAbsCriterion_updateOutput", (PyCFunction)FloatAbsCriterion_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"DoubleAbsCriterion_updateOutput", (PyCFunction)DoubleAbsCriterion_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"FloatAbsCriterion_updateGradInput", (PyCFunction)FloatAbsCriterion_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"DoubleAbsCriterion_updateGradInput", (PyCFunction)DoubleAbsCriterion_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"FloatBCECriterion_updateOutput", (PyCFunction)FloatBCECriterion_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"DoubleBCECriterion_updateOutput", (PyCFunction)DoubleBCECriterion_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"FloatBCECriterion_updateGradInput", (PyCFunction)FloatBCECriterion_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"DoubleBCECriterion_updateGradInput", (PyCFunction)DoubleBCECriterion_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"FloatClassNLLCriterion_updateOutput", (PyCFunction)FloatClassNLLCriterion_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"DoubleClassNLLCriterion_updateOutput", (PyCFunction)DoubleClassNLLCriterion_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"FloatClassNLLCriterion_updateGradInput", (PyCFunction)FloatClassNLLCriterion_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"DoubleClassNLLCriterion_updateGradInput", (PyCFunction)DoubleClassNLLCriterion_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"FloatELU_updateOutput", (PyCFunction)FloatELU_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"DoubleELU_updateOutput", (PyCFunction)DoubleELU_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"FloatELU_updateGradInput", (PyCFunction)FloatELU_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"DoubleELU_updateGradInput", (PyCFunction)DoubleELU_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"FloatGatedLinear_updateOutput", (PyCFunction)FloatGatedLinear_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"DoubleGatedLinear_updateOutput", (PyCFunction)DoubleGatedLinear_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"FloatGatedLinear_updateGradInput", (PyCFunction)FloatGatedLinear_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"DoubleGatedLinear_updateGradInput", (PyCFunction)DoubleGatedLinear_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"FloatHardTanh_updateOutput", (PyCFunction)FloatHardTanh_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"DoubleHardTanh_updateOutput", (PyCFunction)DoubleHardTanh_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"FloatHardTanh_updateGradInput", (PyCFunction)FloatHardTanh_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"DoubleHardTanh_updateGradInput", (PyCFunction)DoubleHardTanh_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"FloatLeakyReLU_updateOutput", (PyCFunction)FloatLeakyReLU_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"DoubleLeakyReLU_updateOutput", (PyCFunction)DoubleLeakyReLU_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"FloatLeakyReLU_updateGradInput", (PyCFunction)FloatLeakyReLU_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"DoubleLeakyReLU_updateGradInput", (PyCFunction)DoubleLeakyReLU_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"FloatLogSigmoid_updateOutput", (PyCFunction)FloatLogSigmoid_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"DoubleLogSigmoid_updateOutput", (PyCFunction)DoubleLogSigmoid_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"FloatLogSigmoid_updateGradInput", (PyCFunction)FloatLogSigmoid_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"DoubleLogSigmoid_updateGradInput", (PyCFunction)DoubleLogSigmoid_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"FloatSoftMarginCriterion_updateOutput", (PyCFunction)FloatSoftMarginCriterion_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"DoubleSoftMarginCriterion_updateOutput", (PyCFunction)DoubleSoftMarginCriterion_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"FloatSoftMarginCriterion_updateGradInput", (PyCFunction)FloatSoftMarginCriterion_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"DoubleSoftMarginCriterion_updateGradInput", (PyCFunction)DoubleSoftMarginCriterion_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"FloatMSECriterion_updateOutput", (PyCFunction)FloatMSECriterion_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"DoubleMSECriterion_updateOutput", (PyCFunction)DoubleMSECriterion_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"FloatMSECriterion_updateGradInput", (PyCFunction)FloatMSECriterion_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"DoubleMSECriterion_updateGradInput", (PyCFunction)DoubleMSECriterion_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"FloatMultiLabelMarginCriterion_updateOutput", (PyCFunction)FloatMultiLabelMarginCriterion_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"DoubleMultiLabelMarginCriterion_updateOutput", (PyCFunction)DoubleMultiLabelMarginCriterion_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"FloatMultiLabelMarginCriterion_updateGradInput", (PyCFunction)FloatMultiLabelMarginCriterion_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"DoubleMultiLabelMarginCriterion_updateGradInput", (PyCFunction)DoubleMultiLabelMarginCriterion_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"FloatMultiMarginCriterion_updateOutput", (PyCFunction)FloatMultiMarginCriterion_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"DoubleMultiMarginCriterion_updateOutput", (PyCFunction)DoubleMultiMarginCriterion_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"FloatMultiMarginCriterion_updateGradInput", (PyCFunction)FloatMultiMarginCriterion_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"DoubleMultiMarginCriterion_updateGradInput", (PyCFunction)DoubleMultiMarginCriterion_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"FloatRReLU_updateOutput", (PyCFunction)FloatRReLU_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"DoubleRReLU_updateOutput", (PyCFunction)DoubleRReLU_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"FloatRReLU_updateGradInput", (PyCFunction)FloatRReLU_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"DoubleRReLU_updateGradInput", (PyCFunction)DoubleRReLU_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"FloatSigmoid_updateOutput", (PyCFunction)FloatSigmoid_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"DoubleSigmoid_updateOutput", (PyCFunction)DoubleSigmoid_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"FloatSigmoid_updateGradInput", (PyCFunction)FloatSigmoid_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"DoubleSigmoid_updateGradInput", (PyCFunction)DoubleSigmoid_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"FloatSmoothL1Criterion_updateOutput", (PyCFunction)FloatSmoothL1Criterion_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"DoubleSmoothL1Criterion_updateOutput", (PyCFunction)DoubleSmoothL1Criterion_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"FloatSmoothL1Criterion_updateGradInput", (PyCFunction)FloatSmoothL1Criterion_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"DoubleSmoothL1Criterion_updateGradInput", (PyCFunction)DoubleSmoothL1Criterion_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"FloatSoftPlus_updateOutput", (PyCFunction)FloatSoftPlus_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"DoubleSoftPlus_updateOutput", (PyCFunction)DoubleSoftPlus_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"FloatSoftPlus_updateGradInput", (PyCFunction)FloatSoftPlus_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"DoubleSoftPlus_updateGradInput", (PyCFunction)DoubleSoftPlus_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"FloatSoftShrink_updateOutput", (PyCFunction)FloatSoftShrink_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"DoubleSoftShrink_updateOutput", (PyCFunction)DoubleSoftShrink_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"FloatSoftShrink_updateGradInput", (PyCFunction)FloatSoftShrink_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"DoubleSoftShrink_updateGradInput", (PyCFunction)DoubleSoftShrink_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"FloatIndexLinear_updateOutput", (PyCFunction)FloatIndexLinear_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"DoubleIndexLinear_updateOutput", (PyCFunction)DoubleIndexLinear_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"FloatIndexLinear_accGradParameters", (PyCFunction)FloatIndexLinear_accGradParameters, METH_STATIC | METH_VARARGS, NULL},
  {"DoubleIndexLinear_accGradParameters", (PyCFunction)DoubleIndexLinear_accGradParameters, METH_STATIC | METH_VARARGS, NULL},
  {"FloatIndexLinear_accUpdateGradParameters", (PyCFunction)FloatIndexLinear_accUpdateGradParameters, METH_STATIC | METH_VARARGS, NULL},
  {"DoubleIndexLinear_accUpdateGradParameters", (PyCFunction)DoubleIndexLinear_accUpdateGradParameters, METH_STATIC | METH_VARARGS, NULL},
  {"FloatIndexLinear_updateParameters", (PyCFunction)FloatIndexLinear_updateParameters, METH_STATIC | METH_VARARGS, NULL},
  {"DoubleIndexLinear_updateParameters", (PyCFunction)DoubleIndexLinear_updateParameters, METH_STATIC | METH_VARARGS, NULL},
  {"FloatTemporalRowConvolution_updateOutput", (PyCFunction)FloatTemporalRowConvolution_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"DoubleTemporalRowConvolution_updateOutput", (PyCFunction)DoubleTemporalRowConvolution_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"FloatTemporalRowConvolution_updateGradInput", (PyCFunction)FloatTemporalRowConvolution_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"DoubleTemporalRowConvolution_updateGradInput", (PyCFunction)DoubleTemporalRowConvolution_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"FloatTemporalRowConvolution_accGradParameters", (PyCFunction)FloatTemporalRowConvolution_accGradParameters, METH_STATIC | METH_VARARGS, NULL},
  {"DoubleTemporalRowConvolution_accGradParameters", (PyCFunction)DoubleTemporalRowConvolution_accGradParameters, METH_STATIC | METH_VARARGS, NULL},
  {"FloatSpatialConvolutionMM_updateGradInput", (PyCFunction)FloatSpatialConvolutionMM_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"DoubleSpatialConvolutionMM_updateGradInput", (PyCFunction)DoubleSpatialConvolutionMM_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"FloatSpatialConvolutionMM_accGradParameters", (PyCFunction)FloatSpatialConvolutionMM_accGradParameters, METH_STATIC | METH_VARARGS, NULL},
  {"DoubleSpatialConvolutionMM_accGradParameters", (PyCFunction)DoubleSpatialConvolutionMM_accGradParameters, METH_STATIC | METH_VARARGS, NULL},
  {"Floatunfolded_acc", (PyCFunction)Floatunfolded_acc, METH_STATIC | METH_VARARGS, NULL},
  {"Doubleunfolded_acc", (PyCFunction)Doubleunfolded_acc, METH_STATIC | METH_VARARGS, NULL},
  {"Floatunfolded_copy", (PyCFunction)Floatunfolded_copy, METH_STATIC | METH_VARARGS, NULL},
  {"Doubleunfolded_copy", (PyCFunction)Doubleunfolded_copy, METH_STATIC | METH_VARARGS, NULL},
  {"FloatFeatureLPPooling_updateOutput", (PyCFunction)FloatFeatureLPPooling_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"DoubleFeatureLPPooling_updateOutput", (PyCFunction)DoubleFeatureLPPooling_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"FloatFeatureLPPooling_updateGradInput", (PyCFunction)FloatFeatureLPPooling_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"DoubleFeatureLPPooling_updateGradInput", (PyCFunction)DoubleFeatureLPPooling_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"FloatTanh_updateOutput", (PyCFunction)FloatTanh_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"DoubleTanh_updateOutput", (PyCFunction)DoubleTanh_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"FloatTanh_updateGradInput", (PyCFunction)FloatTanh_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"DoubleTanh_updateGradInput", (PyCFunction)DoubleTanh_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"FloatVolumetricConvolutionMM_updateGradInput", (PyCFunction)FloatVolumetricConvolutionMM_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"DoubleVolumetricConvolutionMM_updateGradInput", (PyCFunction)DoubleVolumetricConvolutionMM_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"FloatVolumetricConvolutionMM_accGradParameters", (PyCFunction)FloatVolumetricConvolutionMM_accGradParameters, METH_STATIC | METH_VARARGS, NULL},
  {"DoubleVolumetricConvolutionMM_accGradParameters", (PyCFunction)DoubleVolumetricConvolutionMM_accGradParameters, METH_STATIC | METH_VARARGS, NULL},
  {"FloatSpatialClassNLLCriterion_updateOutput", (PyCFunction)FloatSpatialClassNLLCriterion_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"DoubleSpatialClassNLLCriterion_updateOutput", (PyCFunction)DoubleSpatialClassNLLCriterion_updateOutput, METH_STATIC | METH_VARARGS, NULL},
  {"FloatSpatialClassNLLCriterion_updateGradInput", (PyCFunction)FloatSpatialClassNLLCriterion_updateGradInput, METH_STATIC | METH_VARARGS, NULL},
  {"DoubleSpatialClassNLLCriterion_updateGradInput", (PyCFunction)DoubleSpatialClassNLLCriterion_updateGradInput, METH_STATIC | METH_VARARGS, NULL},

  {NULL, NULL, 0, NULL}
};
namespace torch { namespace nn {

static PyTypeObject thnn_type;

void init__THNN(PyObject* c_module) {
  ((PyObject*)&thnn_type)->ob_refcnt = 1;
  thnn_type.tp_flags = Py_TPFLAGS_DEFAULT;
  thnn_type.tp_methods = module_methods;
  thnn_type.tp_name = "torch._C._THNN";
  if (PyType_Ready(&thnn_type) < 0) {
    throw python_error();
  }

  PyObject* type_obj = (PyObject*)&thnn_type;
  Py_INCREF(type_obj);
  if (PyModule_AddObject(c_module, "_THNN", type_obj) < 0) {
    throw python_error();
  }
}

}}  // namespace torch::nn
