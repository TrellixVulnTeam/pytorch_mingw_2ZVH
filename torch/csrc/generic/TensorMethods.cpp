
#ifdef THC_GENERIC_FILE
#define THCP_AUTO_GPU 1
#else
#define THCP_AUTO_GPU 0
#endif
#if defined(TH_REAL_IS_FLOAT) || defined(TH_REAL_IS_DOUBLE) || \
    defined(THC_REAL_IS_FLOAT) || defined(THC_REAL_IS_DOUBLE) || \
    defined(THC_REAL_IS_HALF)
#define RealStr "float"
#else
#define RealStr "int"
#endif

#ifdef THC_REAL_IS_HALF
#define AS_REAL(x) THC_float2half(x)
#else
#define AS_REAL(x) x
#endif

#ifdef THD_GENERIC_FILE
#define IS_DISTRIBUTED true
#else
#define IS_DISTRIBUTED false
#endif

#ifndef THC_GENERIC_FILE
#define IS_CUDA false
#define CUDA_FLOAT false
#else

#define IS_CUDA true

#if defined(THC_REAL_IS_BYTE)
#define CUDA_BYTE 1
#else
#define CUDA_BYTE 0
#endif

#if defined(THC_REAL_IS_CHAR)
#define CUDA_CHAR 1
#else
#define CUDA_CHAR 0
#endif

#if defined(THC_REAL_IS_SHORT)
#define CUDA_SHORT 1
#else
#define CUDA_SHORT 0
#endif

#if defined(THC_REAL_IS_INT)
#define CUDA_INT 1
#else
#define CUDA_INT 0
#endif

#if defined(THC_REAL_IS_LONG)
#define CUDA_LONG 1
#else
#define CUDA_LONG 0
#endif

#if defined(THC_REAL_IS_FLOAT)
#define CUDA_FLOAT 1
#else
#define CUDA_FLOAT 0
#endif

#if defined(THC_REAL_IS_DOUBLE)
#define CUDA_DOUBLE 1
#else
#define CUDA_DOUBLE 0
#endif

#if defined(THC_REAL_IS_HALF)
#define CUDA_HALF 1
#else
#define CUDA_HALF 0
#endif

#endif // ifndef THC_GENERIC_FILE

#if IS_CUDA
#define THIndexTensor THCudaLongTensor
#define THIndexTensor_(NAME) TH_CONCAT_2(THCudaLongTensor_,NAME)
#define THPIndexTensor THCPLongTensor
#define THPIndexTensor_(NAME) TH_CONCAT_2(THCPLongTensor_,NAME)
#define THPIndexTensorClass THCPLongTensorClass
#elif IS_DISTRIBUTED
#define THIndexTensor THDLongTensor
#define THIndexTensor_(NAME) TH_CONCAT_2(THDLongTensor_,NAME)
#define THPIndexTensor THDPLongTensor
#define THPIndexTensor_(NAME) TH_CONCAT_2(THDPLongTensor_,NAME)
#define THPIndexTensorClass THDPLongTensorClass
#else
#define THIndexTensor THLongTensor
#define THIndexTensor_(NAME) TH_CONCAT_2(THLongTensor_,NAME)
#define THPIndexTensor THPLongTensor
#define THPIndexTensor_(NAME) TH_CONCAT_2(THPLongTensor_,NAME)
#define THPIndexTensorClass THPLongTensorClass
#endif

#if IS_CUDA
#define THIntegerTensor THCudaIntTensor
#define THIntegerTensor_(NAME) TH_CONCAT_2(THCudaIntTensor_,NAME)
#define THPIntegerTensor THCPIntTensor
#define THPIntegerTensorClass THCPIntTensorClass
#elif IS_DISTRIBUTED
#define THIntegerTensor THDIntTensor
#define THIntegerTensor_(NAME) TH_CONCAT_2(THDIntTensor_,NAME)
#define THPIntegerTensor THDPIntTensor
#define THPIntegerTensorClass THDPIntTensorClass
#else
#define THIntegerTensor THIntTensor
#define THIntegerTensor_(NAME) TH_CONCAT_2(THIntTensor_,NAME)
#define THPIntegerTensor THPIntTensor
#define THPIntegerTensorClass THPIntTensorClass
#endif

#if IS_CUDA
#define THBoolTensor THCudaByteTensor
#define THPBoolTensor THCPByteTensor
#define THPBoolTensorClass THCPByteTensorClass
#elif IS_DISTRIBUTED
#define THBoolTensor THDByteTensor
#define THPBoolTensor THDPByteTensor
#define THPBoolTensorClass THDPByteTensorClass
#else
#define THBoolTensor THByteTensor
#define THPBoolTensor THPByteTensor
#define THPBoolTensorClass THPByteTensorClass
#endif

#if IS_CUDA
#define THPModuleStr "torch.cuda."
#elif IS_DISTRIBUTED
#define THPModuleStr "torch.distributed."
#else
#define THPModuleStr "torch."
#endif

// The C API uses THLongStorage for size and stride, but the Python API uses
// torch.Size or tuple
typedef THLongStorage THSize;
typedef THLongStorage THStride;

// TODO: check that there are no args
static PyObject * THPTensor_(elementSize)(THPTensor *self, PyObject *args)
{
  return PyLong_FromLong(THStorage_(elementSize)(LIBRARY_STATE_NOARGS));
}

// TODO: check that there are no args
static PyObject * THPTensor_(storage)(THPTensor *self, PyObject *args)
{
  // TODO: memory leak on error
  THStorage *result = THTensor_(storage)(LIBRARY_STATE self->cdata);
  if (result == NULL)
    Py_RETURN_NONE;
  THStorage_(retain)(LIBRARY_STATE result);
  THStoragePtr _tmp(result);
  PyObject *ret = THPStorage_(New)(result);
  _tmp.release();
  return ret;
}

PyObject * THPTensor_(storageOffset)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    
    if (kwargs) {
      
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 0) {
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        int64_t __result = THTensor_(storageOffset)(LIBRARY_STATE arg_self);
        Py_BLOCK_THREADS;
        return PyInt_FromLong(__result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "storage_offset", 1, "no arguments");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    

PyObject * THPTensor_(nDimension)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    
    if (kwargs) {
      
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 0) {
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        int64_t __result = THTensor_(nDimension)(LIBRARY_STATE arg_self);
        Py_BLOCK_THREADS;
        return PyInt_FromLong(__result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "ndimension", 1, "no arguments");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    


PyObject * THPTensor_(setIndex)(THPTensor *self, PyObject *args)
{
  THPUtils_assert(PyTuple_GET_SIZE(args) == 2, "set_index takes exactly two "
      "arguments (%d given)", (int)PyTuple_GET_SIZE(args));
  if (THPTensor_(setValue)<true>(self, PyTuple_GET_ITEM(args, 0), PyTuple_GET_ITEM(args, 1)) != 0)
    return NULL;
  Py_RETURN_NONE;
}




PyObject * THPTensor_(resize_)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    
    if (kwargs) {
      
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    THLongStoragePtr __size;
    

    
    if (__dictcount == 0 &&
          __argcount >= 1 &&
          THPUtils_tryUnpackLongVarArgs(args, 0, __size)) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THSize* arg_size = __size.get();
      
      PyThreadState *_save = NULL;
      try {
        THPUtils_assert(arg_self->storage->flag & TH_STORAGE_RESIZABLE, "calling resize_ on a tensor that has non-resizable storage. Clone it first " "or create a new tensor instead.");
        Py_UNBLOCK_THREADS;
        THTensor_(resize)(LIBRARY_STATE arg_self, arg_size, NULL);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "resize_", 2, "(int ... size)", "(torch.Size size)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    

#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_stateless_(zeros)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    
    if (kwargs) {
      
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    THLongStoragePtr __size;
PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (__dictcount == 1 &&
          ___out != NULL &&
          __argcount >= 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          THPUtils_tryUnpackLongVarArgs(args, 0, __size)) {
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THSize* arg_size = __size.get();
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(zeros)(LIBRARY_STATE arg_result, arg_size);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__dictcount == 0 &&
          ___out == NULL &&
          __argcount >= 1 &&
          THPUtils_tryUnpackLongVarArgs(args, 0, __size)) {
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THSize* arg_size = __size.get();
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(zeros)(LIBRARY_STATE arg_result, arg_size);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.zeros", 2, "(int ... size, #" THPTensorStr " out)", "(torch.Size size, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_stateless_(zeros_like)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_input = NULL;
    if (kwargs) {
      __kw_input = PyDict_GetItemString(kwargs, "input");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_input) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_input)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_input = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_input))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(zerosLike)(LIBRARY_STATE arg_result, arg_input);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_input) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_input)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_input = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_input))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(zerosLike)(LIBRARY_STATE arg_result, arg_input);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.zeros_like", 1, "(" THPTensorStr " input, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_stateless_(ones)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    
    if (kwargs) {
      
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    THLongStoragePtr __size;
PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (__dictcount == 1 &&
          ___out != NULL &&
          __argcount >= 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          THPUtils_tryUnpackLongVarArgs(args, 0, __size)) {
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THSize* arg_size = __size.get();
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(ones)(LIBRARY_STATE arg_result, arg_size);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__dictcount == 0 &&
          ___out == NULL &&
          __argcount >= 1 &&
          THPUtils_tryUnpackLongVarArgs(args, 0, __size)) {
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THSize* arg_size = __size.get();
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(ones)(LIBRARY_STATE arg_result, arg_size);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.ones", 2, "(int ... size, #" THPTensorStr " out)", "(torch.Size size, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_stateless_(ones_like)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_input = NULL;
    if (kwargs) {
      __kw_input = PyDict_GetItemString(kwargs, "input");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_input) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_input)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_input = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_input))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(onesLike)(LIBRARY_STATE arg_result, arg_input);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_input) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_input)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_input = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_input))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(onesLike)(LIBRARY_STATE arg_result, arg_input);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.ones_like", 1, "(" THPTensorStr " input, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


PyObject * THPTensor_(numel)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    
    if (kwargs) {
      
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 0) {
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        int64_t __result = THTensor_(nElement)(LIBRARY_STATE arg_self);
        Py_BLOCK_THREADS;
        return PyInt_FromLong(__result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "numel", 1, "no arguments");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    
PyObject * THPTensor_stateless_(numel)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 1 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        int64_t __result = THTensor_(nElement)(LIBRARY_STATE arg_self);
        Py_BLOCK_THREADS;
        return PyInt_FromLong(__result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.numel", 1, "(" THPTensorStr " source)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    

PyObject * THPTensor_(set_)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_sourceStorage = NULL;
    PyObject *__kw_storage_offset = NULL;
    PyObject *__kw_size = NULL;
    PyObject *__kw_stride = NULL;
    PyObject *__kw_source = NULL;
    PyObject *__kw_storage = NULL;
    if (kwargs) {
      __kw_sourceStorage = PyDict_GetItemString(kwargs, "sourceStorage");
      __kw_storage_offset = PyDict_GetItemString(kwargs, "storage_offset");
      __kw_size = PyDict_GetItemString(kwargs, "size");
      __kw_stride = PyDict_GetItemString(kwargs, "stride");
      __kw_source = PyDict_GetItemString(kwargs, "source");
      __kw_storage = PyDict_GetItemString(kwargs, "storage");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    THLongStoragePtr __size;
THLongStoragePtr __stride;
    

    
    if (__argcount == 4 &&
          (__tuplecount > 0 || __kw_sourceStorage) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_sourceStorage)) == THPStorageClass &&
          (__tuplecount > 1 || __kw_storage_offset) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_storage_offset)) &&
          (__tuplecount > 2 || __kw_size) && THPUtils_tryUnpackLongs((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_size), __size) &&
          (__tuplecount > 3 || __kw_stride) && THPUtils_tryUnpackLongs((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_stride), __stride)) {
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THStorage* arg_sourceStorage = ((THPStorage*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_sourceStorage))->cdata;
      int64_t arg_storage_offset = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_storage_offset));
      THSize* arg_size = __size.get();
      THStride* arg_stride = __stride.get();
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(setStorage)(LIBRARY_STATE arg_self, arg_sourceStorage, arg_storage_offset, arg_size, arg_stride);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)(self);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 3 &&
          (__tuplecount > 0 || __kw_sourceStorage) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_sourceStorage)) == THPStorageClass &&
          (__tuplecount > 1 || __kw_storage_offset) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_storage_offset)) &&
          (__tuplecount > 2 || __kw_size) && THPUtils_tryUnpackLongs((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_size), __size)) {
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THStorage* arg_sourceStorage = ((THPStorage*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_sourceStorage))->cdata;
      int64_t arg_storage_offset = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_storage_offset));
      THSize* arg_size = __size.get();
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(setStorage)(LIBRARY_STATE arg_self, arg_sourceStorage, arg_storage_offset, arg_size, NULL);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)(self);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 1 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_source = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(set)(LIBRARY_STATE arg_self, arg_source);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)(self);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 1 &&
          (__tuplecount > 0 || __kw_storage) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_storage)) == THPStorageClass) {
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THStorage* arg_storage = ((THPStorage*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_storage))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        THLongStoragePtr __storage_size(THLongStorage_newWithSize1(THStorage_(size)(LIBRARY_STATE arg_storage)));
        Py_UNBLOCK_THREADS;
        THTensor_(setStorage)(LIBRARY_STATE arg_self, arg_storage, 0, __storage_size.get(), NULL);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)(self);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 0) {
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(setStorage)(LIBRARY_STATE arg_self, NULL, 0, NULL, NULL);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)(self);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "set_", 5, "no arguments", "(" THPTensorStr " source)", "(" THPStorageStr " storage)", "(" THPStorageStr " sourceStorage, int storage_offset, torch.Size size)", "(" THPStorageStr " sourceStorage, int storage_offset, torch.Size size, tuple stride)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    

static PyObject * THPTensor_(select)(THPTensor *self, PyObject *args)
{
  HANDLE_TH_ERRORS
  int64_t dim, idx;
  if (!PyArg_ParseTuple(args, "LL", &dim, &idx))
    return NULL;

  int ndim = THTensor_(nDimension)(LIBRARY_STATE self->cdata);

  THPUtils_assert(dim >= -(ndim) && dim < (ndim),
    "dimension out of range (expected to be in range of [%d, %d], but got %d)",
    -(ndim), (ndim)-1, dim);
  if (dim<0) dim += ndim;

  if(ndim > 1) {
    THTensorPtr selected(THTensor_(newWithTensor)(LIBRARY_STATE self->cdata));
    THTensor_(select)(LIBRARY_STATE selected.get(), NULL, (int) dim, idx);
    return THPTensor_(New)(selected.release());
  }
  else {
    THArgCheck(ndim == 1, 1, "empty Tensor");
    return THPUtils_(newReal)(THTensor_(get1d)(LIBRARY_STATE self->cdata, idx));
  }
  END_HANDLE_TH_ERRORS
}

PyObject * THPTensor_(size)(PyObject *self, PyObject *args, PyObject *kwargs)
{
  HANDLE_TH_ERRORS
  THTensor* tensor = ((THPTensor*)self)->cdata;
  if (PyTuple_Size(args) == 0 && (!kwargs || PyDict_Size(kwargs) == 0)) {
    return THPSize_New(tensor->nDimension, tensor->size);
  }

  int tuplecount = args ? (int) PyTuple_Size(args) : 0;
  int dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;

  PyObject* pydim = NULL;
  if (tuplecount == 1 && dictcount == 0) {
    pydim = PyTuple_GET_ITEM(args, 0);
  } else if (dictcount == 1 && tuplecount == 0) {
    pydim = PyDict_GetItemString(kwargs, "dim");
  }

  if (pydim && THPUtils_checkLong(pydim)) {
    int dim = (int)THPUtils_unpackLong(pydim);
    if (dim < 0)
      dim += tensor->nDimension;
    return PyInt_FromLong(THTensor_(size)(LIBRARY_STATE tensor, dim));
  }

  THPUtils_invalidArguments(args, kwargs, "size", 2, "(int dim)", "no arguments");
  return NULL;
  END_HANDLE_TH_ERRORS
}

PyObject * THPTensor_(stride)(PyObject *self, PyObject *args, PyObject *kwargs)
{
  HANDLE_TH_ERRORS
  THTensor* tensor = ((THPTensor*)self)->cdata;
  if (PyTuple_Size(args) == 0 && (!kwargs || PyDict_Size(kwargs) == 0)) {
    PyObject* stride = PyTuple_New(tensor->nDimension);
    for (int i = 0; i != tensor->nDimension; ++i) {
      PyTuple_SET_ITEM(stride, i, PyLong_FromLong(tensor->stride[i]));
    }
    return stride;
  }

  int tuplecount = args ? (int) PyTuple_Size(args) : 0;
  int dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;

  PyObject* pydim = NULL;
  if (tuplecount == 1 && dictcount == 0) {
    pydim = PyTuple_GET_ITEM(args, 0);
  } else if (dictcount == 1 && tuplecount == 0) {
    pydim = PyDict_GetItemString(kwargs, "dim");
  }

  if (pydim && THPUtils_checkLong(pydim)) {
    int dim = (int)THPUtils_unpackLong(pydim);
    if (dim < 0)
      dim += tensor->nDimension;
    return PyInt_FromLong(THTensor_(stride)(LIBRARY_STATE tensor, dim));
  }

  THPUtils_invalidArguments(args, kwargs, "stride", 2, "(int dim)", "no arguments");
  return NULL;
  END_HANDLE_TH_ERRORS
}

#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_(fill_)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_value = NULL;
    if (kwargs) {
      __kw_value = PyDict_GetItemString(kwargs, "value");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 1 &&
          (__tuplecount > 0 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(fill)(LIBRARY_STATE arg_self, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "fill_", 1, "(" RealStr " value)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


PyObject * THPTensor_(isSameSizeAs)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_other = NULL;
    if (kwargs) {
      __kw_other = PyDict_GetItemString(kwargs, "other");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 1 &&
          (__tuplecount > 0 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other)) == THPTensorClass) {
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        bool __result = THTensor_(isSameSizeAs)(LIBRARY_STATE arg_self, arg_other);
        Py_BLOCK_THREADS;
        return PyBool_FromLong(__result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "is_same_size", 1, "(" THPTensorStr " other)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    

PyObject * THPTensor_(isContiguous)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    
    if (kwargs) {
      
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 0) {
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        bool __result = THTensor_(isContiguous)(LIBRARY_STATE arg_self);
        Py_BLOCK_THREADS;
        return PyBool_FromLong(__result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "is_contiguous", 1, "no arguments");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    

PyObject * THPTensor_(isSetTo)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_tensor = NULL;
    if (kwargs) {
      __kw_tensor = PyDict_GetItemString(kwargs, "tensor");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 1 &&
          (__tuplecount > 0 || __kw_tensor) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor)) == THPTensorClass) {
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_tensor = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        bool __result = THTensor_(isSetTo)(LIBRARY_STATE arg_self, arg_tensor);
        Py_BLOCK_THREADS;
        return PyBool_FromLong(__result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "is_set_to", 1, "(" THPTensorStr " tensor)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    

#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_(maskedFill_)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_mask = NULL;
    PyObject *__kw_value = NULL;
    if (kwargs) {
      __kw_mask = PyDict_GetItemString(kwargs, "mask");
      __kw_value = PyDict_GetItemString(kwargs, "value");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 2 &&
          (__tuplecount > 0 || __kw_mask) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mask)) == THPBoolTensorClass &&
          (__tuplecount > 1 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THBoolTensor* arg_mask = ((THPBoolTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mask))->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value));
      
      #if !IS_CUDA
      THByteTensor *arg_mask_save = arg_mask;
      THByteTensorPtr arg_mask_guard(nullptr);
      #else
      THCudaByteTensor *arg_mask_save = arg_mask;
      THPPointer<THCudaByteTensor> arg_mask_guard(nullptr);
      #endif
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_mask->size, arg_mask->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_mask_guard =
          #if !IS_CUDA
          THByteTensor_new(LIBRARY_STATE_NOARGS);
          #else
          THCudaByteTensor_new(LIBRARY_STATE_NOARGS);
          #endif
          
          expand_inplace1(LIBRARY_STATE arg_mask_guard.get(), arg_mask, arg_self,
              "mask", "self", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_mask = arg_mask_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(maskedFill)(LIBRARY_STATE arg_self, arg_mask, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_mask = arg_mask_save;
      
      
    
    }

    THPUtils_invalidArguments(args, kwargs, "masked_fill_", 1, "(" THPModuleStr "ByteTensor mask, " RealStr " value)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_(maskedCopy_)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_mask = NULL;
    PyObject *__kw_source = NULL;
    if (kwargs) {
      __kw_mask = PyDict_GetItemString(kwargs, "mask");
      __kw_source = PyDict_GetItemString(kwargs, "source");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 2 &&
          (__tuplecount > 0 || __kw_mask) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mask)) == THPBoolTensorClass &&
          (__tuplecount > 1 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THBoolTensor* arg_mask = ((THPBoolTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mask))->cdata;
      THTensor* arg_source = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_source))->cdata;
      
      #if !IS_CUDA
      THByteTensor *arg_mask_save = arg_mask;
      THByteTensorPtr arg_mask_guard(nullptr);
      #else
      THCudaByteTensor *arg_mask_save = arg_mask;
      THPPointer<THCudaByteTensor> arg_mask_guard(nullptr);
      #endif
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_mask->size, arg_mask->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_mask_guard =
          #if !IS_CUDA
          THByteTensor_new(LIBRARY_STATE_NOARGS);
          #else
          THCudaByteTensor_new(LIBRARY_STATE_NOARGS);
          #endif
          
          expand_inplace1(LIBRARY_STATE arg_mask_guard.get(), arg_mask, arg_self,
              "mask", "self", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_mask = arg_mask_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(maskedCopy)(LIBRARY_STATE arg_self, arg_mask, arg_source);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_mask = arg_mask_save;
      
      
    
    }

    THPUtils_invalidArguments(args, kwargs, "masked_scatter_", 1, "(" THPModuleStr "ByteTensor mask, " THPTensorStr " source)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_(maskedSelect)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_mask = NULL;
    if (kwargs) {
      __kw_mask = PyDict_GetItemString(kwargs, "mask");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_mask) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mask)) == THPBoolTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THBoolTensor* arg_mask = ((THPBoolTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mask))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      #if !IS_CUDA
      THByteTensor *arg_mask_save = arg_mask;
      THByteTensorPtr arg_mask_guard(nullptr);
      #else
      THCudaByteTensor *arg_mask_save = arg_mask;
      THPPointer<THCudaByteTensor> arg_mask_guard(nullptr);
      #endif
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_mask->size, arg_mask->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_mask_guard =
          #if !IS_CUDA
          THByteTensor_new(LIBRARY_STATE_NOARGS);
          #else
          THCudaByteTensor_new(LIBRARY_STATE_NOARGS);
          #endif
          
          expand_outplace2(LIBRARY_STATE arg_self_guard.get(), arg_mask_guard.get(),
              arg_self, arg_mask,
              "self", "mask", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_mask = arg_mask_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(maskedSelect)(LIBRARY_STATE arg_result, arg_self, arg_mask);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_mask = arg_mask_save;
      
      
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_mask) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mask)) == THPBoolTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THBoolTensor* arg_mask = ((THPBoolTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mask))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      #if !IS_CUDA
      THByteTensor *arg_mask_save = arg_mask;
      THByteTensorPtr arg_mask_guard(nullptr);
      #else
      THCudaByteTensor *arg_mask_save = arg_mask;
      THPPointer<THCudaByteTensor> arg_mask_guard(nullptr);
      #endif
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_mask->size, arg_mask->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_mask_guard =
          #if !IS_CUDA
          THByteTensor_new(LIBRARY_STATE_NOARGS);
          #else
          THCudaByteTensor_new(LIBRARY_STATE_NOARGS);
          #endif
          
          expand_outplace2(LIBRARY_STATE arg_self_guard.get(), arg_mask_guard.get(),
              arg_self, arg_mask,
              "self", "mask", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_mask = arg_mask_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(maskedSelect)(LIBRARY_STATE arg_result, arg_self, arg_mask);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_mask = arg_mask_save;
      
      
    
    }

    THPUtils_invalidArguments(args, kwargs, "masked_select", 1, "(" THPModuleStr "ByteTensor mask, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_stateless_(maskedSelect)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    PyObject *__kw_mask = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
      __kw_mask = PyDict_GetItemString(kwargs, "mask");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_mask) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mask)) == THPBoolTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THBoolTensor* arg_mask = ((THPBoolTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mask))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      #if !IS_CUDA
      THByteTensor *arg_mask_save = arg_mask;
      THByteTensorPtr arg_mask_guard(nullptr);
      #else
      THCudaByteTensor *arg_mask_save = arg_mask;
      THPPointer<THCudaByteTensor> arg_mask_guard(nullptr);
      #endif
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_mask->size, arg_mask->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_mask_guard =
          #if !IS_CUDA
          THByteTensor_new(LIBRARY_STATE_NOARGS);
          #else
          THCudaByteTensor_new(LIBRARY_STATE_NOARGS);
          #endif
          
          expand_outplace2(LIBRARY_STATE arg_self_guard.get(), arg_mask_guard.get(),
              arg_self, arg_mask,
              "self", "mask", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_mask = arg_mask_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(maskedSelect)(LIBRARY_STATE arg_result, arg_self, arg_mask);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_mask = arg_mask_save;
      
      
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_mask) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mask)) == THPBoolTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THBoolTensor* arg_mask = ((THPBoolTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mask))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      #if !IS_CUDA
      THByteTensor *arg_mask_save = arg_mask;
      THByteTensorPtr arg_mask_guard(nullptr);
      #else
      THCudaByteTensor *arg_mask_save = arg_mask;
      THPPointer<THCudaByteTensor> arg_mask_guard(nullptr);
      #endif
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_mask->size, arg_mask->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_mask_guard =
          #if !IS_CUDA
          THByteTensor_new(LIBRARY_STATE_NOARGS);
          #else
          THCudaByteTensor_new(LIBRARY_STATE_NOARGS);
          #endif
          
          expand_outplace2(LIBRARY_STATE arg_self_guard.get(), arg_mask_guard.get(),
              arg_self, arg_mask,
              "self", "mask", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_mask = arg_mask_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(maskedSelect)(LIBRARY_STATE arg_result, arg_self, arg_mask);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_mask = arg_mask_save;
      
      
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.masked_select", 1, "(" THPTensorStr " source, " THPModuleStr "ByteTensor mask, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


PyObject * THPTensor_(transpose)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_dim0 = NULL;
    PyObject *__kw_dim1 = NULL;
    if (kwargs) {
      __kw_dim0 = PyDict_GetItemString(kwargs, "dim0");
      __kw_dim1 = PyDict_GetItemString(kwargs, "dim1");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 2 &&
          (__tuplecount > 0 || __kw_dim0) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim0)) &&
          (__tuplecount > 1 || __kw_dim1) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim1))) {
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_dim0 = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim0));
      int64_t arg_dim1 = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim1));
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim0);
      THPUtils_assert(arg_dim0 >= -(arg_self->nDimension) && arg_dim0 < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim0);
      if (arg_dim0 < 0) arg_dim0 += (arg_self->nDimension);
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim1);
      THPUtils_assert(arg_dim1 >= -(arg_self->nDimension) && arg_dim1 < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim1);
      if (arg_dim1 < 0) arg_dim1 += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor* __result = THTensor_(newTranspose)(LIBRARY_STATE arg_self, arg_dim0, arg_dim1);
        Py_BLOCK_THREADS;
        return THPTensor_(New)(__result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "transpose", 1, "(int dim0, int dim1)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    
PyObject * THPTensor_stateless_(transpose)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    PyObject *__kw_dim0 = NULL;
    PyObject *__kw_dim1 = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
      __kw_dim0 = PyDict_GetItemString(kwargs, "dim0");
      __kw_dim1 = PyDict_GetItemString(kwargs, "dim1");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 3 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_dim0) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim0)) &&
          (__tuplecount > 2 || __kw_dim1) && THPUtils_checkLong((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_dim1))) {
      
      
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int64_t arg_dim0 = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim0));
      int64_t arg_dim1 = THPUtils_unpackLong((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_dim1));
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim0);
      THPUtils_assert(arg_dim0 >= -(arg_self->nDimension) && arg_dim0 < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim0);
      if (arg_dim0 < 0) arg_dim0 += (arg_self->nDimension);
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim1);
      THPUtils_assert(arg_dim1 >= -(arg_self->nDimension) && arg_dim1 < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim1);
      if (arg_dim1 < 0) arg_dim1 += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor* __result = THTensor_(newTranspose)(LIBRARY_STATE arg_self, arg_dim0, arg_dim1);
        Py_BLOCK_THREADS;
        return THPTensor_(New)(__result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.transpose", 1, "(" THPTensorStr " source, int dim0, int dim1)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    

PyObject * THPTensor_(transpose_)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_dim0 = NULL;
    PyObject *__kw_dim1 = NULL;
    if (kwargs) {
      __kw_dim0 = PyDict_GetItemString(kwargs, "dim0");
      __kw_dim1 = PyDict_GetItemString(kwargs, "dim1");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 2 &&
          (__tuplecount > 0 || __kw_dim0) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim0)) &&
          (__tuplecount > 1 || __kw_dim1) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim1))) {
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_dim0 = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim0));
      int64_t arg_dim1 = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim1));
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim0);
      THPUtils_assert(arg_dim0 >= -(arg_self->nDimension) && arg_dim0 < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim0);
      if (arg_dim0 < 0) arg_dim0 += (arg_self->nDimension);
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim1);
      THPUtils_assert(arg_dim1 >= -(arg_self->nDimension) && arg_dim1 < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim1);
      if (arg_dim1 < 0) arg_dim1 += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(transpose)(LIBRARY_STATE arg_self, arg_self, arg_dim0, arg_dim1);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "transpose_", 1, "(int dim0, int dim1)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    

#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_(t)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    
    if (kwargs) {
      
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 0) {
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        int64_t ndim = arg_self->nDimension;
        THPUtils_assert(ndim == 2, "t() expects a 2D tensor, but self is %ldD", ndim);
        
        Py_UNBLOCK_THREADS;
        THTensor* __result = THTensor_(newTranspose)(LIBRARY_STATE arg_self, 0, 1);
        Py_BLOCK_THREADS;
        return THPTensor_(New)(__result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "t", 1, "no arguments");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_stateless_(t)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 1 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        int64_t ndim = arg_self->nDimension;
        THPUtils_assert(ndim == 2, "t() expects a 2D tensor, but self is %ldD", ndim);
        
        Py_UNBLOCK_THREADS;
        THTensor* __result = THTensor_(newTranspose)(LIBRARY_STATE arg_self, 0, 1);
        Py_BLOCK_THREADS;
        return THPTensor_(New)(__result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.t", 1, "(" THPTensorStr " source)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_(t_)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    
    if (kwargs) {
      
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 0) {
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        int64_t ndim = arg_self->nDimension;
        THPUtils_assert(ndim == 2, "t_() expects a 2D tensor, but self is %ldD", ndim);
        
        Py_UNBLOCK_THREADS;
        THTensor_(transpose)(LIBRARY_STATE arg_self, arg_self, 0, 1);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "t_", 1, "no arguments");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


PyObject * THPTensor_(squeeze)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_dim = NULL;
    if (kwargs) {
      __kw_dim = PyDict_GetItemString(kwargs, "dim");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_dim) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim));
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(squeeze1d)(LIBRARY_STATE arg_result, arg_self, arg_dim);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 1 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(squeeze)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_dim) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim));
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(squeeze1d)(LIBRARY_STATE arg_result, arg_self, arg_dim);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(squeeze)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "squeeze", 2, "(#" THPTensorStr " out)", "(int dim, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    
PyObject * THPTensor_stateless_(squeeze)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    PyObject *__kw_dim = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
      __kw_dim = PyDict_GetItemString(kwargs, "dim");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_dim) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim));
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(squeeze1d)(LIBRARY_STATE arg_result, arg_self, arg_dim);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(squeeze)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_dim) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim));
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(squeeze1d)(LIBRARY_STATE arg_result, arg_self, arg_dim);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(squeeze)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.squeeze", 2, "(" THPTensorStr " source, #" THPTensorStr " out)", "(" THPTensorStr " source, int dim, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    

PyObject * THPTensor_(squeeze_)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_dim = NULL;
    if (kwargs) {
      __kw_dim = PyDict_GetItemString(kwargs, "dim");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 1 &&
          (__tuplecount > 0 || __kw_dim) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim));
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(squeeze1d)(LIBRARY_STATE arg_self, arg_self, arg_dim);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(squeeze)(LIBRARY_STATE arg_self, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "squeeze_", 2, "(int dim)", "no arguments");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    

PyObject * THPTensor_(unsqueeze)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_dim = NULL;
    if (kwargs) {
      __kw_dim = PyDict_GetItemString(kwargs, "dim");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_dim) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim))) {
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim));
      
      THPUtils_assert(arg_self->nDimension+1 > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension+1) && arg_dim < (arg_self->nDimension+1),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension+1), (arg_self->nDimension+1)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension+1);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(unsqueeze1d)(LIBRARY_STATE arg_result, arg_self, arg_dim);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_dim) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim))) {
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim));
      
      THPUtils_assert(arg_self->nDimension+1 > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension+1) && arg_dim < (arg_self->nDimension+1),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension+1), (arg_self->nDimension+1)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension+1);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(unsqueeze1d)(LIBRARY_STATE arg_result, arg_self, arg_dim);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "unsqueeze", 1, "(int dim, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    
PyObject * THPTensor_stateless_(unsqueeze)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    PyObject *__kw_dim = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
      __kw_dim = PyDict_GetItemString(kwargs, "dim");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_dim) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim))) {
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim));
      
      THPUtils_assert(arg_self->nDimension+1 > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension+1) && arg_dim < (arg_self->nDimension+1),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension+1), (arg_self->nDimension+1)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension+1);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(unsqueeze1d)(LIBRARY_STATE arg_result, arg_self, arg_dim);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_dim) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim))) {
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim));
      
      THPUtils_assert(arg_self->nDimension+1 > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension+1) && arg_dim < (arg_self->nDimension+1),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension+1), (arg_self->nDimension+1)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension+1);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(unsqueeze1d)(LIBRARY_STATE arg_result, arg_self, arg_dim);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.unsqueeze", 1, "(" THPTensorStr " source, int dim, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    

PyObject * THPTensor_(unsqueeze_)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_dim = NULL;
    if (kwargs) {
      __kw_dim = PyDict_GetItemString(kwargs, "dim");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 1 &&
          (__tuplecount > 0 || __kw_dim) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim))) {
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim));
      
      THPUtils_assert(arg_self->nDimension+1 > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension+1) && arg_dim < (arg_self->nDimension+1),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension+1), (arg_self->nDimension+1)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension+1);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(unsqueeze1d)(LIBRARY_STATE arg_self, arg_self, arg_dim);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "unsqueeze_", 1, "(int dim)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    

#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_(nonzero)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    
    if (kwargs) {
      
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 1 &&
          (PyObject*)Py_TYPE(___out) == THPIndexTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THIndexTensor* arg_result = ((THPIndexTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(nonzero)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      #if IS_CUDA
      THCPLongTensorPtr _result_guard((THCPLongTensor*) THCPLongTensor_NewEmpty());
      if (!_result_guard.get()) return NULL;
      THCPLongTensor* result = _result_guard.get();
      
      #else
      THPLongTensorPtr _result_guard((THPLongTensor*) THPLongTensor_NewEmpty());
      if (!_result_guard.get()) return NULL;
      THPLongTensor* result = _result_guard.get();
      
      #endif
      
      
      THIndexTensor* arg_result = ((THPIndexTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(nonzero)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "nonzero", 1, "(#" THPModuleStr "LongTensor out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_stateless_(nonzero)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPIndexTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THIndexTensor* arg_result = ((THPIndexTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(nonzero)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      #if IS_CUDA
      THCPLongTensorPtr _result_guard((THCPLongTensor*) THCPLongTensor_NewEmpty());
      if (!_result_guard.get()) return NULL;
      THCPLongTensor* result = _result_guard.get();
      
      #else
      THPLongTensorPtr _result_guard((THPLongTensor*) THPLongTensor_NewEmpty());
      if (!_result_guard.get()) return NULL;
      THPLongTensor* result = _result_guard.get();
      
      #endif
      
      
      THIndexTensor* arg_result = ((THPIndexTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(nonzero)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.nonzero", 1, "(" THPTensorStr " source, #" THPModuleStr "LongTensor out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_(contiguous)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    
    if (kwargs) {
      
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor* __result = THTensor_(newContiguous)(LIBRARY_STATE arg_self);
        Py_BLOCK_THREADS;
        return THPTensor_(New)(__result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "contiguous", 1, "no arguments");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_(clone)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    
    if (kwargs) {
      
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor* __result = THTensor_(newClone)(LIBRARY_STATE arg_self);
        Py_BLOCK_THREADS;
        return THPTensor_(New)(__result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "clone", 1, "no arguments");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_(view)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    
    if (kwargs) {
      
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    THLongStoragePtr __size;
    

    
    if (__dictcount == 0 &&
          __argcount >= 1 &&
          THPUtils_tryUnpackLongVarArgs(args, 0, __size)) {
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THSize* arg_size = __size.get();
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor* __result = THTensor_(newView)(LIBRARY_STATE arg_self, arg_size);
        Py_BLOCK_THREADS;
        return THPTensor_(New)(__result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "view", 2, "(int ... size)", "(torch.Size size)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_(expand)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    
    if (kwargs) {
      
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    THLongStoragePtr __size;
    

    
    if (__dictcount == 0 &&
          __argcount >= 1 &&
          THPUtils_tryUnpackLongVarArgs(args, 0, __size)) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THSize* arg_size = __size.get();
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor* __result = THTensor_(newExpand)(LIBRARY_STATE arg_self, arg_size);
        Py_BLOCK_THREADS;
        return THPTensor_(New)(__result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "expand", 2, "(int ... size)", "(torch.Size size)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_(resizeAs_)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_the_template = NULL;
    if (kwargs) {
      __kw_the_template = PyDict_GetItemString(kwargs, "the_template");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 1 &&
          (__tuplecount > 0 || __kw_the_template) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_the_template)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_the_template = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_the_template))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(resizeAs)(LIBRARY_STATE arg_self, arg_the_template);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "resize_as_", 1, "(" THPTensorStr " the_template)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_(indexSelect)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_dim = NULL;
    PyObject *__kw_index = NULL;
    if (kwargs) {
      __kw_dim = PyDict_GetItemString(kwargs, "dim");
      __kw_index = PyDict_GetItemString(kwargs, "index");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_dim) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim)) &&
          (__tuplecount > 1 || __kw_index) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_index)) == THPIndexTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim));
      THIndexTensor* arg_index = ((THPIndexTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_index))->cdata;
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(indexSelect)(LIBRARY_STATE arg_result, arg_self, arg_dim, arg_index);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_dim) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim)) &&
          (__tuplecount > 1 || __kw_index) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_index)) == THPIndexTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim));
      THIndexTensor* arg_index = ((THPIndexTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_index))->cdata;
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(indexSelect)(LIBRARY_STATE arg_result, arg_self, arg_dim, arg_index);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "index_select", 1, "(int dim, " THPModuleStr "LongTensor index, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_stateless_(indexSelect)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    PyObject *__kw_dim = NULL;
    PyObject *__kw_index = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
      __kw_dim = PyDict_GetItemString(kwargs, "dim");
      __kw_index = PyDict_GetItemString(kwargs, "index");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 4 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_dim) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim)) &&
          (__tuplecount > 2 || __kw_index) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_index)) == THPIndexTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim));
      THIndexTensor* arg_index = ((THPIndexTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_index))->cdata;
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(indexSelect)(LIBRARY_STATE arg_result, arg_self, arg_dim, arg_index);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 3 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_dim) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim)) &&
          (__tuplecount > 2 || __kw_index) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_index)) == THPIndexTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim));
      THIndexTensor* arg_index = ((THPIndexTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_index))->cdata;
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(indexSelect)(LIBRARY_STATE arg_result, arg_self, arg_dim, arg_index);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.index_select", 1, "(" THPTensorStr " source, int dim, " THPModuleStr "LongTensor index, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_(indexCopy_)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_dim = NULL;
    PyObject *__kw_index = NULL;
    PyObject *__kw_source = NULL;
    if (kwargs) {
      __kw_dim = PyDict_GetItemString(kwargs, "dim");
      __kw_index = PyDict_GetItemString(kwargs, "index");
      __kw_source = PyDict_GetItemString(kwargs, "source");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 3 &&
          (__tuplecount > 0 || __kw_dim) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim)) &&
          (__tuplecount > 1 || __kw_index) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_index)) == THPIndexTensorClass &&
          (__tuplecount > 2 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim));
      THIndexTensor* arg_index = ((THPIndexTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_index))->cdata;
      THTensor* arg_source = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_source))->cdata;
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(indexCopy)(LIBRARY_STATE arg_self, arg_dim, arg_index, arg_source);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)(self);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "index_copy_", 1, "(int dim, " THPModuleStr "LongTensor index, " THPTensorStr " source)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_(take)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_index = NULL;
    if (kwargs) {
      __kw_index = PyDict_GetItemString(kwargs, "index");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_index) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_index)) == THPIndexTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THIndexTensor* arg_index = ((THPIndexTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_index))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(take)(LIBRARY_STATE arg_result, arg_self, arg_index);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_index) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_index)) == THPIndexTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THIndexTensor* arg_index = ((THPIndexTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_index))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(take)(LIBRARY_STATE arg_result, arg_self, arg_index);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "take", 1, "(" THPModuleStr "LongTensor index, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_stateless_(take)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    PyObject *__kw_index = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
      __kw_index = PyDict_GetItemString(kwargs, "index");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_index) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_index)) == THPIndexTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THIndexTensor* arg_index = ((THPIndexTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_index))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(take)(LIBRARY_STATE arg_result, arg_self, arg_index);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_index) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_index)) == THPIndexTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THIndexTensor* arg_index = ((THPIndexTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_index))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(take)(LIBRARY_STATE arg_result, arg_self, arg_index);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.take", 1, "(" THPTensorStr " source, " THPModuleStr "LongTensor index, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_(put_)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_index = NULL;
    PyObject *__kw_source = NULL;
    PyObject *__kw_accumulate = NULL;
    if (kwargs) {
      __kw_index = PyDict_GetItemString(kwargs, "index");
      __kw_source = PyDict_GetItemString(kwargs, "source");
      __kw_accumulate = PyDict_GetItemString(kwargs, "accumulate");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 3 &&
          (__tuplecount > 0 || __kw_index) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_index)) == THPIndexTensorClass &&
          (__tuplecount > 1 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_accumulate) && PyBool_Check((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_accumulate))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THIndexTensor* arg_index = ((THPIndexTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_index))->cdata;
      THTensor* arg_source = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_source))->cdata;
      bool arg_accumulate = ((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_accumulate) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(put)(LIBRARY_STATE arg_self, arg_index, arg_source, arg_accumulate);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)(self);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 2 &&
          (__tuplecount > 0 || __kw_index) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_index)) == THPIndexTensorClass &&
          (__tuplecount > 1 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THIndexTensor* arg_index = ((THPIndexTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_index))->cdata;
      THTensor* arg_source = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(put)(LIBRARY_STATE arg_self, arg_index, arg_source, false);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)(self);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "put_", 2, "(" THPModuleStr "LongTensor index, " THPTensorStr " source)", "(" THPModuleStr "LongTensor index, " THPTensorStr " source, bool accumulate)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_(indexAdd_)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_dim = NULL;
    PyObject *__kw_index = NULL;
    PyObject *__kw_source = NULL;
    if (kwargs) {
      __kw_dim = PyDict_GetItemString(kwargs, "dim");
      __kw_index = PyDict_GetItemString(kwargs, "index");
      __kw_source = PyDict_GetItemString(kwargs, "source");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 3 &&
          (__tuplecount > 0 || __kw_dim) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim)) &&
          (__tuplecount > 1 || __kw_index) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_index)) == THPIndexTensorClass &&
          (__tuplecount > 2 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim));
      THIndexTensor* arg_index = ((THPIndexTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_index))->cdata;
      THTensor* arg_source = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_source))->cdata;
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(indexAdd)(LIBRARY_STATE arg_self, arg_dim, arg_index, arg_source);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)(self);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "index_add_", 1, "(int dim, " THPModuleStr "LongTensor index, " THPTensorStr " source)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_(indexFill_)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_dim = NULL;
    PyObject *__kw_index = NULL;
    PyObject *__kw_value = NULL;
    if (kwargs) {
      __kw_dim = PyDict_GetItemString(kwargs, "dim");
      __kw_index = PyDict_GetItemString(kwargs, "index");
      __kw_value = PyDict_GetItemString(kwargs, "value");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 3 &&
          (__tuplecount > 0 || __kw_dim) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim)) &&
          (__tuplecount > 1 || __kw_index) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_index)) == THPIndexTensorClass &&
          (__tuplecount > 2 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim));
      THIndexTensor* arg_index = ((THPIndexTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_index))->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_value));
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(indexFill)(LIBRARY_STATE arg_self, arg_dim, arg_index, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)(self);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "index_fill_", 1, "(int dim, " THPModuleStr "LongTensor index, " RealStr " value)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


PyObject * THPTensor_(narrow)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_dimension = NULL;
    PyObject *__kw_start = NULL;
    PyObject *__kw_length = NULL;
    if (kwargs) {
      __kw_dimension = PyDict_GetItemString(kwargs, "dimension");
      __kw_start = PyDict_GetItemString(kwargs, "start");
      __kw_length = PyDict_GetItemString(kwargs, "length");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 4 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_dimension) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dimension)) &&
          (__tuplecount > 1 || __kw_start) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_start)) &&
          (__tuplecount > 2 || __kw_length) && THPUtils_checkLong((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_length))) {
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_dimension = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dimension));
      int64_t arg_start = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_start));
      int64_t arg_length = THPUtils_unpackLong((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_length));
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dimension);
      THPUtils_assert(arg_dimension >= -(arg_self->nDimension) && arg_dimension < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dimension);
      if (arg_dimension < 0) arg_dimension += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(narrow)(LIBRARY_STATE arg_result, arg_self, arg_dimension, arg_start, arg_length);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 3 &&
          (__tuplecount > 0 || __kw_dimension) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dimension)) &&
          (__tuplecount > 1 || __kw_start) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_start)) &&
          (__tuplecount > 2 || __kw_length) && THPUtils_checkLong((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_length))) {
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_dimension = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dimension));
      int64_t arg_start = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_start));
      int64_t arg_length = THPUtils_unpackLong((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_length));
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dimension);
      THPUtils_assert(arg_dimension >= -(arg_self->nDimension) && arg_dimension < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dimension);
      if (arg_dimension < 0) arg_dimension += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(narrow)(LIBRARY_STATE arg_result, arg_self, arg_dimension, arg_start, arg_length);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "narrow", 1, "(int dimension, int start, int length, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    

PyObject * THPTensor_(unfold)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_dimension = NULL;
    PyObject *__kw_size = NULL;
    PyObject *__kw_step = NULL;
    if (kwargs) {
      __kw_dimension = PyDict_GetItemString(kwargs, "dimension");
      __kw_size = PyDict_GetItemString(kwargs, "size");
      __kw_step = PyDict_GetItemString(kwargs, "step");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 4 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_dimension) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dimension)) &&
          (__tuplecount > 1 || __kw_size) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_size)) &&
          (__tuplecount > 2 || __kw_step) && THPUtils_checkLong((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_step))) {
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_dimension = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dimension));
      int64_t arg_size = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_size));
      int64_t arg_step = THPUtils_unpackLong((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_step));
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dimension);
      THPUtils_assert(arg_dimension >= -(arg_self->nDimension) && arg_dimension < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dimension);
      if (arg_dimension < 0) arg_dimension += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(unfold)(LIBRARY_STATE arg_result, arg_self, arg_dimension, arg_size, arg_step);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 3 &&
          (__tuplecount > 0 || __kw_dimension) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dimension)) &&
          (__tuplecount > 1 || __kw_size) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_size)) &&
          (__tuplecount > 2 || __kw_step) && THPUtils_checkLong((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_step))) {
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_dimension = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dimension));
      int64_t arg_size = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_size));
      int64_t arg_step = THPUtils_unpackLong((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_step));
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dimension);
      THPUtils_assert(arg_dimension >= -(arg_self->nDimension) && arg_dimension < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dimension);
      if (arg_dimension < 0) arg_dimension += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(unfold)(LIBRARY_STATE arg_result, arg_self, arg_dimension, arg_size, arg_step);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "unfold", 1, "(int dimension, int size, int step, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    

#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_stateless_(range)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_start = NULL;
    PyObject *__kw_end = NULL;
    PyObject *__kw_step = NULL;
    if (kwargs) {
      __kw_start = PyDict_GetItemString(kwargs, "start");
      __kw_end = PyDict_GetItemString(kwargs, "end");
      __kw_step = PyDict_GetItemString(kwargs, "step");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 4 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_start) && THPUtils_(checkAccreal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_start)) &&
          (__tuplecount > 1 || __kw_end) && THPUtils_(checkAccreal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_end)) &&
          (__tuplecount > 2 || __kw_step) && THPUtils_(checkAccreal)((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_step))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      PyErr_WarnEx(PyExc_UserWarning, "torch.range is deprecated in favor of torch.arange "
          "and will be removed in 0.3. Note that arange generates values in [start; end), "
      "not [start; end].", 1);
  
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      accreal arg_start = THPUtils_(unpackAccreal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_start));
      accreal arg_end = THPUtils_(unpackAccreal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_end));
      accreal arg_step = THPUtils_(unpackAccreal)((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_step));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(range)(LIBRARY_STATE arg_result, arg_start, arg_end, arg_step);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 3 &&
          (__tuplecount > 0 || __kw_start) && THPUtils_(checkAccreal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_start)) &&
          (__tuplecount > 1 || __kw_end) && THPUtils_(checkAccreal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_end)) &&
          (__tuplecount > 2 || __kw_step) && THPUtils_(checkAccreal)((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_step))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      PyErr_WarnEx(PyExc_UserWarning, "torch.range is deprecated in favor of torch.arange "
          "and will be removed in 0.3. Note that arange generates values in [start; end), "
      "not [start; end].", 1);
  
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      accreal arg_start = THPUtils_(unpackAccreal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_start));
      accreal arg_end = THPUtils_(unpackAccreal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_end));
      accreal arg_step = THPUtils_(unpackAccreal)((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_step));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(range)(LIBRARY_STATE arg_result, arg_start, arg_end, arg_step);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_start) && THPUtils_(checkAccreal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_start)) &&
          (__tuplecount > 1 || __kw_end) && THPUtils_(checkAccreal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_end))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      PyErr_WarnEx(PyExc_UserWarning, "torch.range is deprecated in favor of torch.arange "
          "and will be removed in 0.3. Note that arange generates values in [start; end), "
      "not [start; end].", 1);
  
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      accreal arg_start = THPUtils_(unpackAccreal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_start));
      accreal arg_end = THPUtils_(unpackAccreal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_end));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(range)(LIBRARY_STATE arg_result, arg_start, arg_end, 1);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_start) && THPUtils_(checkAccreal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_start)) &&
          (__tuplecount > 1 || __kw_end) && THPUtils_(checkAccreal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_end))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      PyErr_WarnEx(PyExc_UserWarning, "torch.range is deprecated in favor of torch.arange "
          "and will be removed in 0.3. Note that arange generates values in [start; end), "
      "not [start; end].", 1);
  
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      accreal arg_start = THPUtils_(unpackAccreal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_start));
      accreal arg_end = THPUtils_(unpackAccreal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_end));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(range)(LIBRARY_STATE arg_result, arg_start, arg_end, 1);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.range", 2, "(" RealStr " start, " RealStr " end, #" THPTensorStr " out)", "(" RealStr " start, " RealStr " end, " RealStr " step, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_stateless_(arange)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_start = NULL;
    PyObject *__kw_end = NULL;
    PyObject *__kw_step = NULL;
    if (kwargs) {
      __kw_start = PyDict_GetItemString(kwargs, "start");
      __kw_end = PyDict_GetItemString(kwargs, "end");
      __kw_step = PyDict_GetItemString(kwargs, "step");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 4 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_start) && THPUtils_(checkAccreal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_start)) &&
          (__tuplecount > 1 || __kw_end) && THPUtils_(checkAccreal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_end)) &&
          (__tuplecount > 2 || __kw_step) && THPUtils_(checkAccreal)((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_step))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      accreal arg_start = THPUtils_(unpackAccreal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_start));
      accreal arg_end = THPUtils_(unpackAccreal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_end));
      accreal arg_step = THPUtils_(unpackAccreal)((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_step));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(arange)(LIBRARY_STATE arg_result, arg_start, arg_end, arg_step);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 3 &&
          (__tuplecount > 0 || __kw_start) && THPUtils_(checkAccreal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_start)) &&
          (__tuplecount > 1 || __kw_end) && THPUtils_(checkAccreal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_end)) &&
          (__tuplecount > 2 || __kw_step) && THPUtils_(checkAccreal)((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_step))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      accreal arg_start = THPUtils_(unpackAccreal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_start));
      accreal arg_end = THPUtils_(unpackAccreal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_end));
      accreal arg_step = THPUtils_(unpackAccreal)((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_step));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(arange)(LIBRARY_STATE arg_result, arg_start, arg_end, arg_step);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_start) && THPUtils_(checkAccreal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_start)) &&
          (__tuplecount > 1 || __kw_end) && THPUtils_(checkAccreal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_end))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      accreal arg_start = THPUtils_(unpackAccreal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_start));
      accreal arg_end = THPUtils_(unpackAccreal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_end));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(arange)(LIBRARY_STATE arg_result, arg_start, arg_end, 1);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_start) && THPUtils_(checkAccreal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_start)) &&
          (__tuplecount > 1 || __kw_end) && THPUtils_(checkAccreal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_end))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      accreal arg_start = THPUtils_(unpackAccreal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_start));
      accreal arg_end = THPUtils_(unpackAccreal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_end));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(arange)(LIBRARY_STATE arg_result, arg_start, arg_end, 1);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_end) && THPUtils_(checkAccreal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_end))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      accreal arg_end = THPUtils_(unpackAccreal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_end));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(arange)(LIBRARY_STATE arg_result, 0, arg_end, 1);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_end) && THPUtils_(checkAccreal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_end))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      accreal arg_end = THPUtils_(unpackAccreal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_end));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(arange)(LIBRARY_STATE arg_result, 0, arg_end, 1);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.arange", 3, "(" RealStr " end, #" THPTensorStr " out)", "(" RealStr " start, " RealStr " end, #" THPTensorStr " out)", "(" RealStr " start, " RealStr " end, " RealStr " step, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_(scatter_)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_dim = NULL;
    PyObject *__kw_index = NULL;
    PyObject *__kw_src = NULL;
    PyObject *__kw_value = NULL;
    if (kwargs) {
      __kw_dim = PyDict_GetItemString(kwargs, "dim");
      __kw_index = PyDict_GetItemString(kwargs, "index");
      __kw_src = PyDict_GetItemString(kwargs, "src");
      __kw_value = PyDict_GetItemString(kwargs, "value");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 3 &&
          (__tuplecount > 0 || __kw_dim) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim)) &&
          (__tuplecount > 1 || __kw_index) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_index)) == THPIndexTensorClass &&
          (__tuplecount > 2 || __kw_src) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_src)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim));
      THIndexTensor* arg_index = ((THPIndexTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_index))->cdata;
      THTensor* arg_src = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_src))->cdata;
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(scatter)(LIBRARY_STATE arg_self, arg_dim, arg_index, arg_src);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)(self);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 3 &&
          (__tuplecount > 0 || __kw_dim) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim)) &&
          (__tuplecount > 1 || __kw_index) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_index)) == THPIndexTensorClass &&
          (__tuplecount > 2 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim));
      THIndexTensor* arg_index = ((THPIndexTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_index))->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_value));
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(scatterFill)(LIBRARY_STATE arg_self, arg_dim, arg_index, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)(self);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "scatter_", 2, "(int dim, " THPModuleStr "LongTensor index, " RealStr " value)", "(int dim, " THPModuleStr "LongTensor index, " THPTensorStr " src)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_(scatter_add_)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_dim = NULL;
    PyObject *__kw_index = NULL;
    PyObject *__kw_src = NULL;
    if (kwargs) {
      __kw_dim = PyDict_GetItemString(kwargs, "dim");
      __kw_index = PyDict_GetItemString(kwargs, "index");
      __kw_src = PyDict_GetItemString(kwargs, "src");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 3 &&
          (__tuplecount > 0 || __kw_dim) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim)) &&
          (__tuplecount > 1 || __kw_index) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_index)) == THPIndexTensorClass &&
          (__tuplecount > 2 || __kw_src) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_src)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim));
      THIndexTensor* arg_index = ((THPIndexTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_index))->cdata;
      THTensor* arg_src = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_src))->cdata;
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(scatterAdd)(LIBRARY_STATE arg_self, arg_dim, arg_index, arg_src);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)(self);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "scatter_add_", 1, "(int dim, " THPModuleStr "LongTensor index, " THPTensorStr " src)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_(gather)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_dim = NULL;
    PyObject *__kw_index = NULL;
    if (kwargs) {
      __kw_dim = PyDict_GetItemString(kwargs, "dim");
      __kw_index = PyDict_GetItemString(kwargs, "index");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_dim) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim)) &&
          (__tuplecount > 1 || __kw_index) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_index)) == THPIndexTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim));
      THIndexTensor* arg_index = ((THPIndexTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_index))->cdata;
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        THLongStoragePtr _size(THIndexTensor_(newSizeOf)(LIBRARY_STATE arg_index));
        THTensor_(resize)(LIBRARY_STATE arg_result, _size, NULL);
        
        Py_UNBLOCK_THREADS;
        THTensor_(gather)(LIBRARY_STATE arg_result, arg_self, arg_dim, arg_index);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_dim) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim)) &&
          (__tuplecount > 1 || __kw_index) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_index)) == THPIndexTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim));
      THIndexTensor* arg_index = ((THPIndexTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_index))->cdata;
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        THLongStoragePtr _size(THIndexTensor_(newSizeOf)(LIBRARY_STATE arg_index));
        THTensor_(resize)(LIBRARY_STATE arg_result, _size, NULL);
        
        Py_UNBLOCK_THREADS;
        THTensor_(gather)(LIBRARY_STATE arg_result, arg_self, arg_dim, arg_index);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "gather", 1, "(int dim, " THPModuleStr "LongTensor index, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_stateless_(gather)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    PyObject *__kw_dim = NULL;
    PyObject *__kw_index = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
      __kw_dim = PyDict_GetItemString(kwargs, "dim");
      __kw_index = PyDict_GetItemString(kwargs, "index");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 4 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_dim) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim)) &&
          (__tuplecount > 2 || __kw_index) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_index)) == THPIndexTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim));
      THIndexTensor* arg_index = ((THPIndexTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_index))->cdata;
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        THLongStoragePtr _size(THIndexTensor_(newSizeOf)(LIBRARY_STATE arg_index));
        THTensor_(resize)(LIBRARY_STATE arg_result, _size, NULL);
        
        Py_UNBLOCK_THREADS;
        THTensor_(gather)(LIBRARY_STATE arg_result, arg_self, arg_dim, arg_index);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 3 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_dim) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim)) &&
          (__tuplecount > 2 || __kw_index) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_index)) == THPIndexTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim));
      THIndexTensor* arg_index = ((THPIndexTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_index))->cdata;
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        THLongStoragePtr _size(THIndexTensor_(newSizeOf)(LIBRARY_STATE arg_index));
        THTensor_(resize)(LIBRARY_STATE arg_result, _size, NULL);
        
        Py_UNBLOCK_THREADS;
        THTensor_(gather)(LIBRARY_STATE arg_result, arg_self, arg_dim, arg_index);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.gather", 1, "(" THPTensorStr " source, int dim, " THPModuleStr "LongTensor index, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#ifndef TH_REAL_IS_HALF
static PyObject * THPTensor_stateless_(cat)(THPTensor *_unused, PyObject *args, PyObject *kwargs)
{
  HANDLE_TH_ERRORS
#if IS_CUDA
  THCPAutoGPU __autogpu_guard(-1);
#endif
  static char* argnames[] = { "seq", "dim", "out", NULL };
  PyObject *_seq = NULL;
  int dim = 0;
  PyObject *___out = NULL;

  THPObjectPtr sequence;
  std::vector<THTensor *> tensors;
  THPTensorPtr result;
  Py_ssize_t len;

  if (!PyArg_ParseTupleAndKeywords(args, kwargs, "O|iO", argnames, &_seq, &dim, &___out)) {
    goto invalid_arguments;
  }

  sequence = PySequence_Fast(_seq, "seq must be a sequence");
  if (!sequence) {
    // NOTE: we use the error message from invalidArguments when _seq is not a sequence
    goto invalid_arguments;
  }

  len = PySequence_Fast_GET_SIZE(sequence.get());
  THPUtils_assert(len > 0, "seq can't be empty");

  if (___out && ___out != Py_None) {
    if (!THPTensor_(Check)(___out)) {
      goto invalid_arguments;
    }
    Py_INCREF(___out);
    result = (THPTensor *)___out;
  } else {
    result = (THPTensor *)THPTensor_(NewEmpty)();
    if (!result) return NULL;
  }

  for (int i = 0; i < len; i++) {
    PyObject *item = PySequence_Fast_GET_ITEM(sequence.get(), i);
    if (!THPTensor_(Check)(item))
      goto invalid_arguments;
    tensors.push_back(((THPTensor*)item)->cdata);
  }

  for (THTensor *t : tensors) {
    auto ndim = THTensor_(nDimension)(LIBRARY_STATE t);
    if (ndim > 0) {
      THPUtils_assert(dim > 0 ? dim < ndim : ndim + dim >= 0,
                      "dim out of range - got %d but the tensor is only %dD",
                      dim, ndim);
      if (dim < 0) dim += ndim;
      break;
    }
  }

#if IS_CUDA
  __autogpu_guard.setDevice(THTensor_(getDevice)(LIBRARY_STATE tensors[0]));
#endif

  THTensor_(catArray)(LIBRARY_STATE result->cdata, tensors.data(), (int) tensors.size(), dim);
  return (PyObject*)result.release();

invalid_arguments:
  THPUtils_invalidArguments(args, kwargs, "cat", 2,
      "(sequence[" THPTensorStr "] seq)",
      "(sequence[" THPTensorStr "] seq, int dim)");
  return NULL;
  END_HANDLE_TH_ERRORS
}
#endif

#if !IS_DISTRIBUTED
PyObject * THPTensor_(data_ptr)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    
    if (kwargs) {
      
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    #if !IS_DISTRIBUTED
          
    if (__argcount == 0) {
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      void* __result = THTensor_(data)(LIBRARY_STATE arg_self);
      return PyLong_FromVoidPtr(__result);
    
    #else
    if (false) {
    #endif

    }

    THPUtils_invalidArguments(args, kwargs, "data_ptr", 1, "no arguments");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_(equal)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_other = NULL;
    if (kwargs) {
      __kw_other = PyDict_GetItemString(kwargs, "other");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 1 &&
          (__tuplecount > 0 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        bool __result = THTensor_(equal)(LIBRARY_STATE arg_self, arg_other);
        Py_BLOCK_THREADS;
        return PyBool_FromLong(__result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "equal", 1, "(" THPTensorStr " other)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_stateless_(equal)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    PyObject *__kw_other = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
      __kw_other = PyDict_GetItemString(kwargs, "other");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 2 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        bool __result = THTensor_(equal)(LIBRARY_STATE arg_self, arg_other);
        Py_BLOCK_THREADS;
        return PyBool_FromLong(__result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.equal", 1, "(" THPTensorStr " source, " THPTensorStr " other)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


PyObject * THPTensor_(copy_)(PyObject *self, PyObject *args, PyObject *kwargs)
{
  HANDLE_TH_ERRORS
  return THPTensorCopyMethod(THTensor_(copy_functions), self, args, kwargs);
  END_HANDLE_TH_ERRORS
}

#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_(__and__)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_value = NULL;
    PyObject *__kw_other = NULL;
    if (kwargs) {
      __kw_value = PyDict_GetItemString(kwargs, "value");
      __kw_other = PyDict_GetItemString(kwargs, "other");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(bitand)(LIBRARY_STATE arg_result, arg_self, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_self_guard.get(), arg_other_guard.get(),
              arg_self, arg_other,
              "self", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cbitand)(LIBRARY_STATE arg_result, arg_self, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_other = arg_other_save;
      
      
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(bitand)(LIBRARY_STATE arg_result, arg_self, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_self_guard.get(), arg_other_guard.get(),
              arg_self, arg_other,
              "self", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cbitand)(LIBRARY_STATE arg_result, arg_self, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_other = arg_other_save;
      
      
    
    }

    THPUtils_invalidArguments(args, kwargs, "__and__", 2, "(" RealStr " value, #" THPTensorStr " out)", "(" THPTensorStr " other, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_stateless_(__and__)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    PyObject *__kw_value = NULL;
    PyObject *__kw_other = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
      __kw_value = PyDict_GetItemString(kwargs, "value");
      __kw_other = PyDict_GetItemString(kwargs, "other");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(bitand)(LIBRARY_STATE arg_result, arg_self, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_self_guard.get(), arg_other_guard.get(),
              arg_self, arg_other,
              "self", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cbitand)(LIBRARY_STATE arg_result, arg_self, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_other = arg_other_save;
      
      
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(bitand)(LIBRARY_STATE arg_result, arg_self, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_self_guard.get(), arg_other_guard.get(),
              arg_self, arg_other,
              "self", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cbitand)(LIBRARY_STATE arg_result, arg_self, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_other = arg_other_save;
      
      
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.__and__", 2, "(" THPTensorStr " source, " RealStr " value, #" THPTensorStr " out)", "(" THPTensorStr " source, " THPTensorStr " other, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_(__iand__)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_value = NULL;
    PyObject *__kw_other = NULL;
    if (kwargs) {
      __kw_value = PyDict_GetItemString(kwargs, "value");
      __kw_other = PyDict_GetItemString(kwargs, "other");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 1 &&
          (__tuplecount > 0 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(bitand)(LIBRARY_STATE arg_self, arg_self, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)(self);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 1 &&
          (__tuplecount > 0 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other))->cdata;
      
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_inplace1(LIBRARY_STATE arg_other_guard.get(), arg_other, arg_self,
              "other", "self", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cbitand)(LIBRARY_STATE arg_self, arg_self, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)(self);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_other = arg_other_save;
      
      
    
    }

    THPUtils_invalidArguments(args, kwargs, "__iand__", 2, "(" RealStr " value)", "(" THPTensorStr " other)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_stateless_(__iand__)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    PyObject *__kw_value = NULL;
    PyObject *__kw_other = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
      __kw_value = PyDict_GetItemString(kwargs, "value");
      __kw_other = PyDict_GetItemString(kwargs, "other");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 3 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(bitand)(LIBRARY_STATE arg_self, arg_self, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source));
        return (PyObject*)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 3 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_other))->cdata;
      
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_inplace1(LIBRARY_STATE arg_other_guard.get(), arg_other, arg_self,
              "other", "self", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cbitand)(LIBRARY_STATE arg_self, arg_self, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source));
        return (PyObject*)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_other = arg_other_save;
      
      
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.__iand__", 2, "(" THPTensorStr " source, " THPTensorStr " source, " RealStr " value)", "(" THPTensorStr " source, " THPTensorStr " source, " THPTensorStr " other)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_(__or__)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_value = NULL;
    PyObject *__kw_other = NULL;
    if (kwargs) {
      __kw_value = PyDict_GetItemString(kwargs, "value");
      __kw_other = PyDict_GetItemString(kwargs, "other");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(bitor)(LIBRARY_STATE arg_result, arg_self, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_self_guard.get(), arg_other_guard.get(),
              arg_self, arg_other,
              "self", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cbitor)(LIBRARY_STATE arg_result, arg_self, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_other = arg_other_save;
      
      
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(bitor)(LIBRARY_STATE arg_result, arg_self, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_self_guard.get(), arg_other_guard.get(),
              arg_self, arg_other,
              "self", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cbitor)(LIBRARY_STATE arg_result, arg_self, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_other = arg_other_save;
      
      
    
    }

    THPUtils_invalidArguments(args, kwargs, "__or__", 2, "(" RealStr " value, #" THPTensorStr " out)", "(" THPTensorStr " other, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_stateless_(__or__)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    PyObject *__kw_value = NULL;
    PyObject *__kw_other = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
      __kw_value = PyDict_GetItemString(kwargs, "value");
      __kw_other = PyDict_GetItemString(kwargs, "other");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(bitor)(LIBRARY_STATE arg_result, arg_self, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_self_guard.get(), arg_other_guard.get(),
              arg_self, arg_other,
              "self", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cbitor)(LIBRARY_STATE arg_result, arg_self, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_other = arg_other_save;
      
      
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(bitor)(LIBRARY_STATE arg_result, arg_self, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_self_guard.get(), arg_other_guard.get(),
              arg_self, arg_other,
              "self", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cbitor)(LIBRARY_STATE arg_result, arg_self, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_other = arg_other_save;
      
      
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.__or__", 2, "(" THPTensorStr " source, " RealStr " value, #" THPTensorStr " out)", "(" THPTensorStr " source, " THPTensorStr " other, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_(__ior__)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_value = NULL;
    PyObject *__kw_other = NULL;
    if (kwargs) {
      __kw_value = PyDict_GetItemString(kwargs, "value");
      __kw_other = PyDict_GetItemString(kwargs, "other");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 1 &&
          (__tuplecount > 0 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(bitor)(LIBRARY_STATE arg_self, arg_self, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)(self);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 1 &&
          (__tuplecount > 0 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other))->cdata;
      
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_inplace1(LIBRARY_STATE arg_other_guard.get(), arg_other, arg_self,
              "other", "self", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cbitor)(LIBRARY_STATE arg_self, arg_self, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)(self);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_other = arg_other_save;
      
      
    
    }

    THPUtils_invalidArguments(args, kwargs, "__ior__", 2, "(" RealStr " value)", "(" THPTensorStr " other)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_stateless_(__ior__)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    PyObject *__kw_value = NULL;
    PyObject *__kw_other = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
      __kw_value = PyDict_GetItemString(kwargs, "value");
      __kw_other = PyDict_GetItemString(kwargs, "other");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 3 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(bitor)(LIBRARY_STATE arg_self, arg_self, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source));
        return (PyObject*)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 3 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_other))->cdata;
      
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_inplace1(LIBRARY_STATE arg_other_guard.get(), arg_other, arg_self,
              "other", "self", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cbitor)(LIBRARY_STATE arg_self, arg_self, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source));
        return (PyObject*)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_other = arg_other_save;
      
      
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.__ior__", 2, "(" THPTensorStr " source, " THPTensorStr " source, " RealStr " value)", "(" THPTensorStr " source, " THPTensorStr " source, " THPTensorStr " other)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_(__xor__)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_value = NULL;
    PyObject *__kw_other = NULL;
    if (kwargs) {
      __kw_value = PyDict_GetItemString(kwargs, "value");
      __kw_other = PyDict_GetItemString(kwargs, "other");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(bitxor)(LIBRARY_STATE arg_result, arg_self, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_self_guard.get(), arg_other_guard.get(),
              arg_self, arg_other,
              "self", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cbitxor)(LIBRARY_STATE arg_result, arg_self, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_other = arg_other_save;
      
      
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(bitxor)(LIBRARY_STATE arg_result, arg_self, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_self_guard.get(), arg_other_guard.get(),
              arg_self, arg_other,
              "self", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cbitxor)(LIBRARY_STATE arg_result, arg_self, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_other = arg_other_save;
      
      
    
    }

    THPUtils_invalidArguments(args, kwargs, "__xor__", 2, "(" RealStr " value, #" THPTensorStr " out)", "(" THPTensorStr " other, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_stateless_(__xor__)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    PyObject *__kw_value = NULL;
    PyObject *__kw_other = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
      __kw_value = PyDict_GetItemString(kwargs, "value");
      __kw_other = PyDict_GetItemString(kwargs, "other");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(bitxor)(LIBRARY_STATE arg_result, arg_self, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_self_guard.get(), arg_other_guard.get(),
              arg_self, arg_other,
              "self", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cbitxor)(LIBRARY_STATE arg_result, arg_self, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_other = arg_other_save;
      
      
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(bitxor)(LIBRARY_STATE arg_result, arg_self, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_self_guard.get(), arg_other_guard.get(),
              arg_self, arg_other,
              "self", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cbitxor)(LIBRARY_STATE arg_result, arg_self, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_other = arg_other_save;
      
      
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.__xor__", 2, "(" THPTensorStr " source, " RealStr " value, #" THPTensorStr " out)", "(" THPTensorStr " source, " THPTensorStr " other, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_(__ixor__)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_value = NULL;
    PyObject *__kw_other = NULL;
    if (kwargs) {
      __kw_value = PyDict_GetItemString(kwargs, "value");
      __kw_other = PyDict_GetItemString(kwargs, "other");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 1 &&
          (__tuplecount > 0 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(bitxor)(LIBRARY_STATE arg_self, arg_self, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)(self);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 1 &&
          (__tuplecount > 0 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other))->cdata;
      
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_inplace1(LIBRARY_STATE arg_other_guard.get(), arg_other, arg_self,
              "other", "self", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cbitxor)(LIBRARY_STATE arg_self, arg_self, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)(self);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_other = arg_other_save;
      
      
    
    }

    THPUtils_invalidArguments(args, kwargs, "__ixor__", 2, "(" RealStr " value)", "(" THPTensorStr " other)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_stateless_(__ixor__)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    PyObject *__kw_value = NULL;
    PyObject *__kw_other = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
      __kw_value = PyDict_GetItemString(kwargs, "value");
      __kw_other = PyDict_GetItemString(kwargs, "other");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 3 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(bitxor)(LIBRARY_STATE arg_self, arg_self, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source));
        return (PyObject*)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 3 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_other))->cdata;
      
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_inplace1(LIBRARY_STATE arg_other_guard.get(), arg_other, arg_self,
              "other", "self", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cbitxor)(LIBRARY_STATE arg_self, arg_self, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source));
        return (PyObject*)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_other = arg_other_save;
      
      
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.__ixor__", 2, "(" THPTensorStr " source, " THPTensorStr " source, " RealStr " value)", "(" THPTensorStr " source, " THPTensorStr " source, " THPTensorStr " other)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_(__lshift__)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_value = NULL;
    PyObject *__kw_other = NULL;
    if (kwargs) {
      __kw_value = PyDict_GetItemString(kwargs, "value");
      __kw_other = PyDict_GetItemString(kwargs, "other");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(lshift)(LIBRARY_STATE arg_result, arg_self, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_self_guard.get(), arg_other_guard.get(),
              arg_self, arg_other,
              "self", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(clshift)(LIBRARY_STATE arg_result, arg_self, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_other = arg_other_save;
      
      
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(lshift)(LIBRARY_STATE arg_result, arg_self, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_self_guard.get(), arg_other_guard.get(),
              arg_self, arg_other,
              "self", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(clshift)(LIBRARY_STATE arg_result, arg_self, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_other = arg_other_save;
      
      
    
    }

    THPUtils_invalidArguments(args, kwargs, "__lshift__", 2, "(" RealStr " value, #" THPTensorStr " out)", "(" THPTensorStr " other, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_stateless_(__lshift__)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    PyObject *__kw_value = NULL;
    PyObject *__kw_other = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
      __kw_value = PyDict_GetItemString(kwargs, "value");
      __kw_other = PyDict_GetItemString(kwargs, "other");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(lshift)(LIBRARY_STATE arg_result, arg_self, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_self_guard.get(), arg_other_guard.get(),
              arg_self, arg_other,
              "self", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(clshift)(LIBRARY_STATE arg_result, arg_self, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_other = arg_other_save;
      
      
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(lshift)(LIBRARY_STATE arg_result, arg_self, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_self_guard.get(), arg_other_guard.get(),
              arg_self, arg_other,
              "self", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(clshift)(LIBRARY_STATE arg_result, arg_self, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_other = arg_other_save;
      
      
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.__lshift__", 2, "(" THPTensorStr " source, " RealStr " value, #" THPTensorStr " out)", "(" THPTensorStr " source, " THPTensorStr " other, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_(__ilshift__)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_value = NULL;
    PyObject *__kw_other = NULL;
    if (kwargs) {
      __kw_value = PyDict_GetItemString(kwargs, "value");
      __kw_other = PyDict_GetItemString(kwargs, "other");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 1 &&
          (__tuplecount > 0 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(lshift)(LIBRARY_STATE arg_self, arg_self, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)(self);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 1 &&
          (__tuplecount > 0 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other))->cdata;
      
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_inplace1(LIBRARY_STATE arg_other_guard.get(), arg_other, arg_self,
              "other", "self", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(clshift)(LIBRARY_STATE arg_self, arg_self, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)(self);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_other = arg_other_save;
      
      
    
    }

    THPUtils_invalidArguments(args, kwargs, "__ilshift__", 2, "(" RealStr " value)", "(" THPTensorStr " other)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_stateless_(__ilshift__)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    PyObject *__kw_value = NULL;
    PyObject *__kw_other = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
      __kw_value = PyDict_GetItemString(kwargs, "value");
      __kw_other = PyDict_GetItemString(kwargs, "other");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 3 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(lshift)(LIBRARY_STATE arg_self, arg_self, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source));
        return (PyObject*)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 3 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_other))->cdata;
      
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_inplace1(LIBRARY_STATE arg_other_guard.get(), arg_other, arg_self,
              "other", "self", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(clshift)(LIBRARY_STATE arg_self, arg_self, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source));
        return (PyObject*)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_other = arg_other_save;
      
      
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.__ilshift__", 2, "(" THPTensorStr " source, " THPTensorStr " source, " RealStr " value)", "(" THPTensorStr " source, " THPTensorStr " source, " THPTensorStr " other)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_(__rshift__)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_value = NULL;
    PyObject *__kw_other = NULL;
    if (kwargs) {
      __kw_value = PyDict_GetItemString(kwargs, "value");
      __kw_other = PyDict_GetItemString(kwargs, "other");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(rshift)(LIBRARY_STATE arg_result, arg_self, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_self_guard.get(), arg_other_guard.get(),
              arg_self, arg_other,
              "self", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(crshift)(LIBRARY_STATE arg_result, arg_self, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_other = arg_other_save;
      
      
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(rshift)(LIBRARY_STATE arg_result, arg_self, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_self_guard.get(), arg_other_guard.get(),
              arg_self, arg_other,
              "self", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(crshift)(LIBRARY_STATE arg_result, arg_self, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_other = arg_other_save;
      
      
    
    }

    THPUtils_invalidArguments(args, kwargs, "__rshift__", 2, "(" RealStr " value, #" THPTensorStr " out)", "(" THPTensorStr " other, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_stateless_(__rshift__)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    PyObject *__kw_value = NULL;
    PyObject *__kw_other = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
      __kw_value = PyDict_GetItemString(kwargs, "value");
      __kw_other = PyDict_GetItemString(kwargs, "other");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(rshift)(LIBRARY_STATE arg_result, arg_self, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_self_guard.get(), arg_other_guard.get(),
              arg_self, arg_other,
              "self", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(crshift)(LIBRARY_STATE arg_result, arg_self, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_other = arg_other_save;
      
      
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(rshift)(LIBRARY_STATE arg_result, arg_self, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_self_guard.get(), arg_other_guard.get(),
              arg_self, arg_other,
              "self", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(crshift)(LIBRARY_STATE arg_result, arg_self, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_other = arg_other_save;
      
      
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.__rshift__", 2, "(" THPTensorStr " source, " RealStr " value, #" THPTensorStr " out)", "(" THPTensorStr " source, " THPTensorStr " other, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_(__irshift__)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_value = NULL;
    PyObject *__kw_other = NULL;
    if (kwargs) {
      __kw_value = PyDict_GetItemString(kwargs, "value");
      __kw_other = PyDict_GetItemString(kwargs, "other");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 1 &&
          (__tuplecount > 0 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(rshift)(LIBRARY_STATE arg_self, arg_self, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)(self);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 1 &&
          (__tuplecount > 0 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other))->cdata;
      
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_inplace1(LIBRARY_STATE arg_other_guard.get(), arg_other, arg_self,
              "other", "self", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(crshift)(LIBRARY_STATE arg_self, arg_self, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)(self);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_other = arg_other_save;
      
      
    
    }

    THPUtils_invalidArguments(args, kwargs, "__irshift__", 2, "(" RealStr " value)", "(" THPTensorStr " other)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_stateless_(__irshift__)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    PyObject *__kw_value = NULL;
    PyObject *__kw_other = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
      __kw_value = PyDict_GetItemString(kwargs, "value");
      __kw_other = PyDict_GetItemString(kwargs, "other");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 3 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(rshift)(LIBRARY_STATE arg_self, arg_self, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source));
        return (PyObject*)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 3 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_other))->cdata;
      
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_inplace1(LIBRARY_STATE arg_other_guard.get(), arg_other, arg_self,
              "other", "self", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(crshift)(LIBRARY_STATE arg_self, arg_self, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source));
        return (PyObject*)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_other = arg_other_save;
      
      
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.__irshift__", 2, "(" THPTensorStr " source, " THPTensorStr " source, " RealStr " value)", "(" THPTensorStr " source, " THPTensorStr " source, " THPTensorStr " other)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT)
#define BUILD_REAL_FMT "d"
#else
#define BUILD_REAL_FMT "L"
#endif

#if !IS_CUDA && !IS_DISTRIBUTED
static PyObject * THPTensor_(apply)(THPTensor *self, PyObject *arg)
{
  HANDLE_TH_ERRORS
  if (!PyCallable_Check(arg)) {
    THPUtils_setError("apply requires a callable as it's first argument");
    return NULL;
  }

  THTensor *tensor = self->cdata;
  TH_TENSOR_APPLY(real, tensor,
                  PyObject *ret =
                      PyObject_CallFunction(arg, (char*)BUILD_REAL_FMT, *tensor_data);
                  if (!ret)
                    return NULL;
                  if (!THPUtils_(checkReal)(ret)) {
                    Py_DECREF(ret);
                    THError("given function should return a number");
                  }
                  *tensor_data = THPUtils_(unpackReal)(ret);
                  Py_DECREF(ret);
                  );

  Py_INCREF(self);
  return (PyObject*)self;
  END_HANDLE_TH_ERRORS
}

static PyObject * THPTensor_(map)(THPTensor *self, PyObject *args)
{
  HANDLE_TH_ERRORS
    PyObject *fn;
    THPTensor *src_object;
    if (!PyArg_ParseTuple(args, "O!O&", THPTensorClass, &src_object, THPUtils_getCallable, &fn))
      return NULL;

  THTensor *tensor = self->cdata;
  THTensor *src = src_object->cdata;

  THTensor *src_save = src;
  THTensorPtr src_guard(THTensor_(new)(LIBRARY_STATE_NOARGS));

  bool expand_success = false;
  try {
    expand_inplace1<THTensor, THTensor>(src_guard.get(), src, tensor, "src", "tensor", true);
    expand_success = true;
  } catch (std::exception &e) {}
  if (expand_success) {
    src = src_guard.get();
  }

  TH_TENSOR_APPLY2(real, tensor, real, src,
                  PyObject *ret =
                      PyObject_CallFunction(fn, (char*)(BUILD_REAL_FMT BUILD_REAL_FMT),
                                            *tensor_data, *src_data);
                  if (!ret)
                    return NULL;
                  if (!THPUtils_(checkReal)(ret)) {
                    Py_DECREF(ret);
                    THError("given function should return a number");
                  }
                  *tensor_data = THPUtils_(unpackReal)(ret);
                  Py_DECREF(ret);
                  );

  src = src_save;

  Py_INCREF(self);
  return (PyObject*)self;
  END_HANDLE_TH_ERRORS
}

static PyObject * THPTensor_(map2)(THPTensor *self, PyObject *args)
{
  HANDLE_TH_ERRORS
    PyObject *fn;
    THPTensor *src1_object;
    THPTensor *src2_object;
    if (!PyArg_ParseTuple(args, "O!O!O&", THPTensorClass, &src1_object, THPTensorClass, &src2_object, THPUtils_getCallable, &fn))
      return NULL;

  THTensor *tensor = self->cdata;
  THTensor *src1 = src1_object->cdata;
  THTensor *src2 = src2_object->cdata;

  THTensor *src1_save = src1;
  THTensorPtr src1_guard(THTensor_(new)(LIBRARY_STATE_NOARGS));
  THTensor *src2_save = src2;
  THTensorPtr src2_guard(THTensor_(new)(LIBRARY_STATE_NOARGS));

  bool expand_success = false;
  try {
    expand_inplace2<THTensor>(src1_guard.get(), src2_guard.get(), src1, src2, tensor, "src1", "src2", "tensor", true);
    expand_success = true;
  } catch (std::exception &e) {}
  if (expand_success) {
    src1 = src1_guard.get();
    src2 = src2_guard.get();
  }

  TH_TENSOR_APPLY3(real, tensor, real, src1, real, src2,
                  PyObject *ret =
                      PyObject_CallFunction(fn, (char*)(BUILD_REAL_FMT BUILD_REAL_FMT BUILD_REAL_FMT),
                                            *tensor_data, *src1_data, *src2_data);
                  if (!ret)
                    return NULL;
                  if (!THPUtils_(checkReal)(ret)) {
                    Py_DECREF(ret);
                    THError("given function should return a number");
                  }
                  *tensor_data = THPUtils_(unpackReal)(ret);
                  Py_DECREF(ret);
                  );

  src1 = src1_save;
  src2 = src2_save;

  Py_INCREF(self);
  return (PyObject*)self;
  END_HANDLE_TH_ERRORS
}
#endif /* !IS_CUDA */

#undef BUILD_REAL_FMT

#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || defined(TH_REAL_IS_LONG) || defined(TH_REAL_IS_INT) || defined(TH_REAL_IS_SHORT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF || CUDA_LONG || CUDA_INT || CUDA_SHORT)
PyObject * THPTensor_(abs)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    
    if (kwargs) {
      
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 1 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(abs)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(abs)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "abs", 1, "(#" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || defined(TH_REAL_IS_LONG) || defined(TH_REAL_IS_INT) || defined(TH_REAL_IS_SHORT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF || CUDA_LONG || CUDA_INT || CUDA_SHORT)
PyObject * THPTensor_stateless_(abs)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(abs)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(abs)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.abs", 1, "(" THPTensorStr " source, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || defined(TH_REAL_IS_LONG) || defined(TH_REAL_IS_INT) || defined(TH_REAL_IS_SHORT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF || CUDA_LONG || CUDA_INT || CUDA_SHORT)
PyObject * THPTensor_(abs_)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    
    if (kwargs) {
      
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(abs)(LIBRARY_STATE arg_self, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "abs_", 1, "no arguments");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif



#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_(sigmoid_)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    
    if (kwargs) {
      
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(sigmoid)(LIBRARY_STATE arg_self, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "sigmoid_", 1, "no arguments");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_(sigmoid)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    
    if (kwargs) {
      
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 1 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(sigmoid)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(sigmoid)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "sigmoid", 1, "(#" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_stateless_(sigmoid)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(sigmoid)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(sigmoid)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.sigmoid", 1, "(" THPTensorStr " source, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif



#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_(log_)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    
    if (kwargs) {
      
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(log)(LIBRARY_STATE arg_self, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "log_", 1, "no arguments");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_(log)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    
    if (kwargs) {
      
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 1 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(log)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(log)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "log", 1, "(#" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_stateless_(log)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(log)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(log)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.log", 1, "(" THPTensorStr " source, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif



#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_(log1p_)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    
    if (kwargs) {
      
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(log1p)(LIBRARY_STATE arg_self, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "log1p_", 1, "no arguments");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_(log1p)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    
    if (kwargs) {
      
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 1 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(log1p)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(log1p)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "log1p", 1, "(#" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_stateless_(log1p)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(log1p)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(log1p)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.log1p", 1, "(" THPTensorStr " source, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_(lgamma)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    
    if (kwargs) {
      
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 1 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(lgamma)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(lgamma)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "lgamma", 1, "(#" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_stateless_(lgamma)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(lgamma)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(lgamma)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.lgamma", 1, "(" THPTensorStr " source, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_(lgamma_)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    
    if (kwargs) {
      
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(lgamma)(LIBRARY_STATE arg_self, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "lgamma_", 1, "no arguments");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_(digamma)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    
    if (kwargs) {
      
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 1 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(digamma)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(digamma)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "digamma", 1, "(#" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_stateless_(digamma)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(digamma)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(digamma)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.digamma", 1, "(" THPTensorStr " source, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_(digamma_)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    
    if (kwargs) {
      
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(digamma)(LIBRARY_STATE arg_self, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "digamma_", 1, "no arguments");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_(polygamma)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_n = NULL;
    if (kwargs) {
      __kw_n = PyDict_GetItemString(kwargs, "n");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_n) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_n))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      int64_t arg_n = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_n));
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(polygamma)(LIBRARY_STATE arg_result, arg_n, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_n) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_n))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      int64_t arg_n = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_n));
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(polygamma)(LIBRARY_STATE arg_result, arg_n, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "polygamma", 1, "(int n, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_stateless_(polygamma)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_n = NULL;
    PyObject *__kw_source = NULL;
    if (kwargs) {
      __kw_n = PyDict_GetItemString(kwargs, "n");
      __kw_source = PyDict_GetItemString(kwargs, "source");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_n) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_n)) &&
          (__tuplecount > 1 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      int64_t arg_n = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_n));
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(polygamma)(LIBRARY_STATE arg_result, arg_n, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_n) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_n)) &&
          (__tuplecount > 1 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      int64_t arg_n = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_n));
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(polygamma)(LIBRARY_STATE arg_result, arg_n, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.polygamma", 1, "(int n, " THPTensorStr " source, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_(polygamma_)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_n = NULL;
    if (kwargs) {
      __kw_n = PyDict_GetItemString(kwargs, "n");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 1 &&
          (__tuplecount > 0 || __kw_n) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_n))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_n = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_n));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(polygamma)(LIBRARY_STATE arg_self, arg_n, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "polygamma_", 1, "(int n)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_(exp_)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    
    if (kwargs) {
      
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(exp)(LIBRARY_STATE arg_self, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "exp_", 1, "no arguments");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_(exp)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    
    if (kwargs) {
      
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 1 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(exp)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(exp)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "exp", 1, "(#" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_stateless_(exp)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(exp)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(exp)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.exp", 1, "(" THPTensorStr " source, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_(expm1_)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    
    if (kwargs) {
      
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(expm1)(LIBRARY_STATE arg_self, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "expm1_", 1, "no arguments");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_(expm1)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    
    if (kwargs) {
      
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 1 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(expm1)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(expm1)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "expm1", 1, "(#" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_stateless_(expm1)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(expm1)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(expm1)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.expm1", 1, "(" THPTensorStr " source, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_(cos_)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    
    if (kwargs) {
      
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cos)(LIBRARY_STATE arg_self, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "cos_", 1, "no arguments");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_(cos)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    
    if (kwargs) {
      
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 1 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cos)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cos)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "cos", 1, "(#" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_stateless_(cos)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cos)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cos)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.cos", 1, "(" THPTensorStr " source, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif



#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_(acos_)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    
    if (kwargs) {
      
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(acos)(LIBRARY_STATE arg_self, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "acos_", 1, "no arguments");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_(acos)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    
    if (kwargs) {
      
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 1 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(acos)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(acos)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "acos", 1, "(#" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_stateless_(acos)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(acos)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(acos)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.acos", 1, "(" THPTensorStr " source, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif



#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_(cosh_)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    
    if (kwargs) {
      
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cosh)(LIBRARY_STATE arg_self, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "cosh_", 1, "no arguments");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_(cosh)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    
    if (kwargs) {
      
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 1 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cosh)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cosh)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "cosh", 1, "(#" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_stateless_(cosh)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cosh)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cosh)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.cosh", 1, "(" THPTensorStr " source, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif



#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_(sin_)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    
    if (kwargs) {
      
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(sin)(LIBRARY_STATE arg_self, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "sin_", 1, "no arguments");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_(sin)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    
    if (kwargs) {
      
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 1 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(sin)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(sin)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "sin", 1, "(#" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_stateless_(sin)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(sin)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(sin)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.sin", 1, "(" THPTensorStr " source, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif



#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_(asin_)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    
    if (kwargs) {
      
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(asin)(LIBRARY_STATE arg_self, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "asin_", 1, "no arguments");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_(asin)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    
    if (kwargs) {
      
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 1 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(asin)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(asin)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "asin", 1, "(#" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_stateless_(asin)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(asin)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(asin)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.asin", 1, "(" THPTensorStr " source, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif



#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_(sinh_)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    
    if (kwargs) {
      
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(sinh)(LIBRARY_STATE arg_self, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "sinh_", 1, "no arguments");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_(sinh)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    
    if (kwargs) {
      
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 1 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(sinh)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(sinh)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "sinh", 1, "(#" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_stateless_(sinh)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(sinh)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(sinh)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.sinh", 1, "(" THPTensorStr " source, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif



#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_(tan_)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    
    if (kwargs) {
      
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(tan)(LIBRARY_STATE arg_self, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "tan_", 1, "no arguments");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_(tan)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    
    if (kwargs) {
      
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 1 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(tan)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(tan)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "tan", 1, "(#" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_stateless_(tan)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(tan)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(tan)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.tan", 1, "(" THPTensorStr " source, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif



#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_(atan_)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    
    if (kwargs) {
      
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(atan)(LIBRARY_STATE arg_self, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "atan_", 1, "no arguments");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_(atan)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    
    if (kwargs) {
      
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 1 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(atan)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(atan)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "atan", 1, "(#" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_stateless_(atan)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(atan)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(atan)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.atan", 1, "(" THPTensorStr " source, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif



#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_(tanh_)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    
    if (kwargs) {
      
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(tanh)(LIBRARY_STATE arg_self, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "tanh_", 1, "no arguments");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_(tanh)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    
    if (kwargs) {
      
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 1 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(tanh)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(tanh)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "tanh", 1, "(#" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_stateless_(tanh)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(tanh)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(tanh)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.tanh", 1, "(" THPTensorStr " source, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_(erf_)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    
    if (kwargs) {
      
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(erf)(LIBRARY_STATE arg_self, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "erf_", 1, "no arguments");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_(erf)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    
    if (kwargs) {
      
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 1 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(erf)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(erf)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "erf", 1, "(#" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_stateless_(erf)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(erf)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(erf)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.erf", 1, "(" THPTensorStr " source, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_(erfinv_)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    
    if (kwargs) {
      
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(erfinv)(LIBRARY_STATE arg_self, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "erfinv_", 1, "no arguments");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_(erfinv)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    
    if (kwargs) {
      
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 1 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(erfinv)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(erfinv)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "erfinv", 1, "(#" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_stateless_(erfinv)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(erfinv)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(erfinv)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.erfinv", 1, "(" THPTensorStr " source, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif



#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_(sqrt_)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    
    if (kwargs) {
      
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(sqrt)(LIBRARY_STATE arg_self, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "sqrt_", 1, "no arguments");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_(sqrt)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    
    if (kwargs) {
      
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 1 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(sqrt)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(sqrt)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "sqrt", 1, "(#" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_stateless_(sqrt)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(sqrt)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(sqrt)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.sqrt", 1, "(" THPTensorStr " source, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif



#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_(rsqrt_)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    
    if (kwargs) {
      
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(rsqrt)(LIBRARY_STATE arg_self, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "rsqrt_", 1, "no arguments");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_(rsqrt)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    
    if (kwargs) {
      
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 1 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(rsqrt)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(rsqrt)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "rsqrt", 1, "(#" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_stateless_(rsqrt)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(rsqrt)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(rsqrt)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.rsqrt", 1, "(" THPTensorStr " source, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif



#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_(ceil_)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    
    if (kwargs) {
      
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(ceil)(LIBRARY_STATE arg_self, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "ceil_", 1, "no arguments");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_(ceil)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    
    if (kwargs) {
      
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 1 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(ceil)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(ceil)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "ceil", 1, "(#" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_stateless_(ceil)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(ceil)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(ceil)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.ceil", 1, "(" THPTensorStr " source, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif



#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_(floor_)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    
    if (kwargs) {
      
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(floor)(LIBRARY_STATE arg_self, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "floor_", 1, "no arguments");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_(floor)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    
    if (kwargs) {
      
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 1 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(floor)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(floor)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "floor", 1, "(#" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_stateless_(floor)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(floor)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(floor)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.floor", 1, "(" THPTensorStr " source, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif



#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_(round_)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    
    if (kwargs) {
      
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(round)(LIBRARY_STATE arg_self, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "round_", 1, "no arguments");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_(round)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    
    if (kwargs) {
      
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 1 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(round)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(round)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "round", 1, "(#" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_stateless_(round)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(round)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(round)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.round", 1, "(" THPTensorStr " source, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif



#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_(trunc_)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    
    if (kwargs) {
      
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(trunc)(LIBRARY_STATE arg_self, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "trunc_", 1, "no arguments");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_(trunc)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    
    if (kwargs) {
      
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 1 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(trunc)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(trunc)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "trunc", 1, "(#" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_stateless_(trunc)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(trunc)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(trunc)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.trunc", 1, "(" THPTensorStr " source, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif



#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_(frac_)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    
    if (kwargs) {
      
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(frac)(LIBRARY_STATE arg_self, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "frac_", 1, "no arguments");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_(frac)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    
    if (kwargs) {
      
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 1 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(frac)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(frac)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "frac", 1, "(#" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_stateless_(frac)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(frac)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(frac)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.frac", 1, "(" THPTensorStr " source, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_(mean)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_dim = NULL;
    PyObject *__kw_keepdim = NULL;
    if (kwargs) {
      __kw_dim = PyDict_GetItemString(kwargs, "dim");
      __kw_keepdim = PyDict_GetItemString(kwargs, "keepdim");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_dim) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim)) &&
          (__tuplecount > 1 || __kw_keepdim) && PyBool_Check((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_keepdim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim));
      bool arg_keepdim = ((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_keepdim) == Py_True ? true : false);
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(mean)(LIBRARY_STATE arg_result, arg_self, arg_dim, arg_keepdim);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_dim) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim)) &&
          (__tuplecount > 1 || __kw_keepdim) && PyBool_Check((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_keepdim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim));
      bool arg_keepdim = ((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_keepdim) == Py_True ? true : false);
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(mean)(LIBRARY_STATE arg_result, arg_self, arg_dim, arg_keepdim);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_dim) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim));
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        maybeThrowBackCompatKeepdimWarn("mean");
        Py_UNBLOCK_THREADS;
        THTensor_(mean)(LIBRARY_STATE arg_result, arg_self, arg_dim, false);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_dim) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim));
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        maybeThrowBackCompatKeepdimWarn("mean");
        Py_UNBLOCK_THREADS;
        THTensor_(mean)(LIBRARY_STATE arg_result, arg_self, arg_dim, false);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        accreal __result = THTensor_(meanall)(LIBRARY_STATE arg_self);
        Py_BLOCK_THREADS;
        return THPUtils_(newAccreal)(__result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "mean", 3, "no arguments", "(int dim, #" THPTensorStr " out)", "(int dim, bool keepdim, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_stateless_(mean)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    PyObject *__kw_dim = NULL;
    PyObject *__kw_keepdim = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
      __kw_dim = PyDict_GetItemString(kwargs, "dim");
      __kw_keepdim = PyDict_GetItemString(kwargs, "keepdim");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 4 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_dim) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim)) &&
          (__tuplecount > 2 || __kw_keepdim) && PyBool_Check((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_keepdim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim));
      bool arg_keepdim = ((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_keepdim) == Py_True ? true : false);
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(mean)(LIBRARY_STATE arg_result, arg_self, arg_dim, arg_keepdim);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 3 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_dim) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim)) &&
          (__tuplecount > 2 || __kw_keepdim) && PyBool_Check((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_keepdim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim));
      bool arg_keepdim = ((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_keepdim) == Py_True ? true : false);
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(mean)(LIBRARY_STATE arg_result, arg_self, arg_dim, arg_keepdim);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_dim) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim));
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        maybeThrowBackCompatKeepdimWarn("mean");
        Py_UNBLOCK_THREADS;
        THTensor_(mean)(LIBRARY_STATE arg_result, arg_self, arg_dim, false);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_dim) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim));
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        maybeThrowBackCompatKeepdimWarn("mean");
        Py_UNBLOCK_THREADS;
        THTensor_(mean)(LIBRARY_STATE arg_result, arg_self, arg_dim, false);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        accreal __result = THTensor_(meanall)(LIBRARY_STATE arg_self);
        Py_BLOCK_THREADS;
        return THPUtils_(newAccreal)(__result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.mean", 3, "(" THPTensorStr " source)", "(" THPTensorStr " source, int dim, #" THPTensorStr " out)", "(" THPTensorStr " source, int dim, bool keepdim, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_(var)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_dim = NULL;
    PyObject *__kw_unbiased = NULL;
    PyObject *__kw_keepdim = NULL;
    if (kwargs) {
      __kw_dim = PyDict_GetItemString(kwargs, "dim");
      __kw_unbiased = PyDict_GetItemString(kwargs, "unbiased");
      __kw_keepdim = PyDict_GetItemString(kwargs, "keepdim");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 4 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_dim) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim)) &&
          (__tuplecount > 1 || __kw_unbiased) && PyBool_Check((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_unbiased)) &&
          (__tuplecount > 2 || __kw_keepdim) && PyBool_Check((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_keepdim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim));
      bool arg_unbiased = (__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_unbiased) == Py_True ? 0 : 1;
      bool arg_keepdim = ((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_keepdim) == Py_True ? true : false);
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(var)(LIBRARY_STATE arg_result, arg_self, arg_dim, arg_unbiased, arg_keepdim);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 3 &&
          (__tuplecount > 0 || __kw_dim) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim)) &&
          (__tuplecount > 1 || __kw_unbiased) && PyBool_Check((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_unbiased)) &&
          (__tuplecount > 2 || __kw_keepdim) && PyBool_Check((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_keepdim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim));
      bool arg_unbiased = (__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_unbiased) == Py_True ? 0 : 1;
      bool arg_keepdim = ((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_keepdim) == Py_True ? true : false);
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(var)(LIBRARY_STATE arg_result, arg_self, arg_dim, arg_unbiased, arg_keepdim);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_dim) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim)) &&
          (__tuplecount > 1 || __kw_keepdim) && PyBool_Check((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_keepdim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim));
      bool arg_keepdim = ((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_keepdim) == Py_True ? true : false);
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(var)(LIBRARY_STATE arg_result, arg_self, arg_dim, 0, arg_keepdim);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_dim) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim)) &&
          (__tuplecount > 1 || __kw_unbiased) && PyBool_Check((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_unbiased))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim));
      bool arg_unbiased = (__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_unbiased) == Py_True ? 0 : 1;
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        maybeThrowBackCompatKeepdimWarn("var");
        Py_UNBLOCK_THREADS;
        THTensor_(var)(LIBRARY_STATE arg_result, arg_self, arg_dim, arg_unbiased, false);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_dim) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim)) &&
          (__tuplecount > 1 || __kw_keepdim) && PyBool_Check((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_keepdim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim));
      bool arg_keepdim = ((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_keepdim) == Py_True ? true : false);
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(var)(LIBRARY_STATE arg_result, arg_self, arg_dim, 0, arg_keepdim);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_dim) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim)) &&
          (__tuplecount > 1 || __kw_unbiased) && PyBool_Check((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_unbiased))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim));
      bool arg_unbiased = (__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_unbiased) == Py_True ? 0 : 1;
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        maybeThrowBackCompatKeepdimWarn("var");
        Py_UNBLOCK_THREADS;
        THTensor_(var)(LIBRARY_STATE arg_result, arg_self, arg_dim, arg_unbiased, false);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_dim) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim));
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        maybeThrowBackCompatKeepdimWarn("var");
        Py_UNBLOCK_THREADS;
        THTensor_(var)(LIBRARY_STATE arg_result, arg_self, arg_dim, 0, false);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_unbiased) && PyBool_Check((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_unbiased))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      bool arg_unbiased = (__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_unbiased) == Py_True ? 0 : 1;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        accreal __result = THTensor_(varall)(LIBRARY_STATE arg_self, arg_unbiased);
        Py_BLOCK_THREADS;
        return THPUtils_(newAccreal)(__result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_dim) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim));
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        maybeThrowBackCompatKeepdimWarn("var");
        Py_UNBLOCK_THREADS;
        THTensor_(var)(LIBRARY_STATE arg_result, arg_self, arg_dim, 0, false);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        accreal __result = THTensor_(varall)(LIBRARY_STATE arg_self, 0);
        Py_BLOCK_THREADS;
        return THPUtils_(newAccreal)(__result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "var", 6, "no arguments", "(bool unbiased)", "(int dim, #" THPTensorStr " out)", "(int dim, bool keepdim, #" THPTensorStr " out)", "(int dim, bool unbiased, #" THPTensorStr " out)", "(int dim, bool unbiased, bool keepdim, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_stateless_(var)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    PyObject *__kw_dim = NULL;
    PyObject *__kw_unbiased = NULL;
    PyObject *__kw_keepdim = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
      __kw_dim = PyDict_GetItemString(kwargs, "dim");
      __kw_unbiased = PyDict_GetItemString(kwargs, "unbiased");
      __kw_keepdim = PyDict_GetItemString(kwargs, "keepdim");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 5 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_dim) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim)) &&
          (__tuplecount > 2 || __kw_unbiased) && PyBool_Check((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_unbiased)) &&
          (__tuplecount > 3 || __kw_keepdim) && PyBool_Check((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_keepdim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim));
      bool arg_unbiased = (__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_unbiased) == Py_True ? 0 : 1;
      bool arg_keepdim = ((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_keepdim) == Py_True ? true : false);
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(var)(LIBRARY_STATE arg_result, arg_self, arg_dim, arg_unbiased, arg_keepdim);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 4 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_dim) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim)) &&
          (__tuplecount > 2 || __kw_unbiased) && PyBool_Check((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_unbiased)) &&
          (__tuplecount > 3 || __kw_keepdim) && PyBool_Check((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_keepdim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim));
      bool arg_unbiased = (__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_unbiased) == Py_True ? 0 : 1;
      bool arg_keepdim = ((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_keepdim) == Py_True ? true : false);
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(var)(LIBRARY_STATE arg_result, arg_self, arg_dim, arg_unbiased, arg_keepdim);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 4 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_dim) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim)) &&
          (__tuplecount > 2 || __kw_keepdim) && PyBool_Check((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_keepdim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim));
      bool arg_keepdim = ((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_keepdim) == Py_True ? true : false);
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(var)(LIBRARY_STATE arg_result, arg_self, arg_dim, 0, arg_keepdim);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 4 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_dim) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim)) &&
          (__tuplecount > 2 || __kw_unbiased) && PyBool_Check((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_unbiased))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim));
      bool arg_unbiased = (__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_unbiased) == Py_True ? 0 : 1;
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        maybeThrowBackCompatKeepdimWarn("var");
        Py_UNBLOCK_THREADS;
        THTensor_(var)(LIBRARY_STATE arg_result, arg_self, arg_dim, arg_unbiased, false);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 3 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_dim) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim)) &&
          (__tuplecount > 2 || __kw_keepdim) && PyBool_Check((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_keepdim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim));
      bool arg_keepdim = ((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_keepdim) == Py_True ? true : false);
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(var)(LIBRARY_STATE arg_result, arg_self, arg_dim, 0, arg_keepdim);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 3 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_dim) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim)) &&
          (__tuplecount > 2 || __kw_unbiased) && PyBool_Check((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_unbiased))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim));
      bool arg_unbiased = (__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_unbiased) == Py_True ? 0 : 1;
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        maybeThrowBackCompatKeepdimWarn("var");
        Py_UNBLOCK_THREADS;
        THTensor_(var)(LIBRARY_STATE arg_result, arg_self, arg_dim, arg_unbiased, false);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_dim) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim));
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        maybeThrowBackCompatKeepdimWarn("var");
        Py_UNBLOCK_THREADS;
        THTensor_(var)(LIBRARY_STATE arg_result, arg_self, arg_dim, 0, false);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_unbiased) && PyBool_Check((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_unbiased))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      bool arg_unbiased = (__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_unbiased) == Py_True ? 0 : 1;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        accreal __result = THTensor_(varall)(LIBRARY_STATE arg_self, arg_unbiased);
        Py_BLOCK_THREADS;
        return THPUtils_(newAccreal)(__result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_dim) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim));
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        maybeThrowBackCompatKeepdimWarn("var");
        Py_UNBLOCK_THREADS;
        THTensor_(var)(LIBRARY_STATE arg_result, arg_self, arg_dim, 0, false);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        accreal __result = THTensor_(varall)(LIBRARY_STATE arg_self, 0);
        Py_BLOCK_THREADS;
        return THPUtils_(newAccreal)(__result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.var", 6, "(" THPTensorStr " source)", "(" THPTensorStr " source, bool unbiased)", "(" THPTensorStr " source, int dim, #" THPTensorStr " out)", "(" THPTensorStr " source, int dim, bool keepdim, #" THPTensorStr " out)", "(" THPTensorStr " source, int dim, bool unbiased, #" THPTensorStr " out)", "(" THPTensorStr " source, int dim, bool unbiased, bool keepdim, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_(std)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_dim = NULL;
    PyObject *__kw_unbiased = NULL;
    PyObject *__kw_keepdim = NULL;
    if (kwargs) {
      __kw_dim = PyDict_GetItemString(kwargs, "dim");
      __kw_unbiased = PyDict_GetItemString(kwargs, "unbiased");
      __kw_keepdim = PyDict_GetItemString(kwargs, "keepdim");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 4 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_dim) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim)) &&
          (__tuplecount > 1 || __kw_unbiased) && PyBool_Check((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_unbiased)) &&
          (__tuplecount > 2 || __kw_keepdim) && PyBool_Check((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_keepdim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim));
      bool arg_unbiased = (__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_unbiased) == Py_True ? 0 : 1;
      bool arg_keepdim = ((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_keepdim) == Py_True ? true : false);
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(std)(LIBRARY_STATE arg_result, arg_self, arg_dim, arg_unbiased, arg_keepdim);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 3 &&
          (__tuplecount > 0 || __kw_dim) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim)) &&
          (__tuplecount > 1 || __kw_unbiased) && PyBool_Check((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_unbiased)) &&
          (__tuplecount > 2 || __kw_keepdim) && PyBool_Check((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_keepdim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim));
      bool arg_unbiased = (__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_unbiased) == Py_True ? 0 : 1;
      bool arg_keepdim = ((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_keepdim) == Py_True ? true : false);
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(std)(LIBRARY_STATE arg_result, arg_self, arg_dim, arg_unbiased, arg_keepdim);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_dim) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim)) &&
          (__tuplecount > 1 || __kw_keepdim) && PyBool_Check((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_keepdim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim));
      bool arg_keepdim = ((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_keepdim) == Py_True ? true : false);
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(std)(LIBRARY_STATE arg_result, arg_self, arg_dim, 0, arg_keepdim);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_dim) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim)) &&
          (__tuplecount > 1 || __kw_unbiased) && PyBool_Check((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_unbiased))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim));
      bool arg_unbiased = (__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_unbiased) == Py_True ? 0 : 1;
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        maybeThrowBackCompatKeepdimWarn("std");
        Py_UNBLOCK_THREADS;
        THTensor_(std)(LIBRARY_STATE arg_result, arg_self, arg_dim, arg_unbiased, false);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_dim) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim)) &&
          (__tuplecount > 1 || __kw_keepdim) && PyBool_Check((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_keepdim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim));
      bool arg_keepdim = ((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_keepdim) == Py_True ? true : false);
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(std)(LIBRARY_STATE arg_result, arg_self, arg_dim, 0, arg_keepdim);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_dim) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim)) &&
          (__tuplecount > 1 || __kw_unbiased) && PyBool_Check((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_unbiased))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim));
      bool arg_unbiased = (__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_unbiased) == Py_True ? 0 : 1;
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        maybeThrowBackCompatKeepdimWarn("std");
        Py_UNBLOCK_THREADS;
        THTensor_(std)(LIBRARY_STATE arg_result, arg_self, arg_dim, arg_unbiased, false);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_dim) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim));
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        maybeThrowBackCompatKeepdimWarn("std");
        Py_UNBLOCK_THREADS;
        THTensor_(std)(LIBRARY_STATE arg_result, arg_self, arg_dim, 0, false);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_unbiased) && PyBool_Check((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_unbiased))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      bool arg_unbiased = (__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_unbiased) == Py_True ? 0 : 1;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        accreal __result = THTensor_(stdall)(LIBRARY_STATE arg_self, arg_unbiased);
        Py_BLOCK_THREADS;
        return THPUtils_(newAccreal)(__result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_dim) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim));
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        maybeThrowBackCompatKeepdimWarn("std");
        Py_UNBLOCK_THREADS;
        THTensor_(std)(LIBRARY_STATE arg_result, arg_self, arg_dim, 0, false);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        accreal __result = THTensor_(stdall)(LIBRARY_STATE arg_self, 0);
        Py_BLOCK_THREADS;
        return THPUtils_(newAccreal)(__result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "std", 6, "no arguments", "(bool unbiased)", "(int dim, #" THPTensorStr " out)", "(int dim, bool keepdim, #" THPTensorStr " out)", "(int dim, bool unbiased, #" THPTensorStr " out)", "(int dim, bool unbiased, bool keepdim, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_stateless_(std)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    PyObject *__kw_dim = NULL;
    PyObject *__kw_unbiased = NULL;
    PyObject *__kw_keepdim = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
      __kw_dim = PyDict_GetItemString(kwargs, "dim");
      __kw_unbiased = PyDict_GetItemString(kwargs, "unbiased");
      __kw_keepdim = PyDict_GetItemString(kwargs, "keepdim");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 5 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_dim) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim)) &&
          (__tuplecount > 2 || __kw_unbiased) && PyBool_Check((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_unbiased)) &&
          (__tuplecount > 3 || __kw_keepdim) && PyBool_Check((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_keepdim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim));
      bool arg_unbiased = (__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_unbiased) == Py_True ? 0 : 1;
      bool arg_keepdim = ((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_keepdim) == Py_True ? true : false);
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(std)(LIBRARY_STATE arg_result, arg_self, arg_dim, arg_unbiased, arg_keepdim);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 4 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_dim) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim)) &&
          (__tuplecount > 2 || __kw_unbiased) && PyBool_Check((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_unbiased)) &&
          (__tuplecount > 3 || __kw_keepdim) && PyBool_Check((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_keepdim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim));
      bool arg_unbiased = (__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_unbiased) == Py_True ? 0 : 1;
      bool arg_keepdim = ((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_keepdim) == Py_True ? true : false);
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(std)(LIBRARY_STATE arg_result, arg_self, arg_dim, arg_unbiased, arg_keepdim);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 4 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_dim) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim)) &&
          (__tuplecount > 2 || __kw_keepdim) && PyBool_Check((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_keepdim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim));
      bool arg_keepdim = ((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_keepdim) == Py_True ? true : false);
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(std)(LIBRARY_STATE arg_result, arg_self, arg_dim, 0, arg_keepdim);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 4 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_dim) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim)) &&
          (__tuplecount > 2 || __kw_unbiased) && PyBool_Check((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_unbiased))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim));
      bool arg_unbiased = (__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_unbiased) == Py_True ? 0 : 1;
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        maybeThrowBackCompatKeepdimWarn("std");
        Py_UNBLOCK_THREADS;
        THTensor_(std)(LIBRARY_STATE arg_result, arg_self, arg_dim, arg_unbiased, false);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 3 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_dim) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim)) &&
          (__tuplecount > 2 || __kw_keepdim) && PyBool_Check((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_keepdim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim));
      bool arg_keepdim = ((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_keepdim) == Py_True ? true : false);
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(std)(LIBRARY_STATE arg_result, arg_self, arg_dim, 0, arg_keepdim);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 3 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_dim) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim)) &&
          (__tuplecount > 2 || __kw_unbiased) && PyBool_Check((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_unbiased))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim));
      bool arg_unbiased = (__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_unbiased) == Py_True ? 0 : 1;
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        maybeThrowBackCompatKeepdimWarn("std");
        Py_UNBLOCK_THREADS;
        THTensor_(std)(LIBRARY_STATE arg_result, arg_self, arg_dim, arg_unbiased, false);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_dim) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim));
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        maybeThrowBackCompatKeepdimWarn("std");
        Py_UNBLOCK_THREADS;
        THTensor_(std)(LIBRARY_STATE arg_result, arg_self, arg_dim, 0, false);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_unbiased) && PyBool_Check((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_unbiased))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      bool arg_unbiased = (__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_unbiased) == Py_True ? 0 : 1;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        accreal __result = THTensor_(stdall)(LIBRARY_STATE arg_self, arg_unbiased);
        Py_BLOCK_THREADS;
        return THPUtils_(newAccreal)(__result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_dim) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim));
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        maybeThrowBackCompatKeepdimWarn("std");
        Py_UNBLOCK_THREADS;
        THTensor_(std)(LIBRARY_STATE arg_result, arg_self, arg_dim, 0, false);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        accreal __result = THTensor_(stdall)(LIBRARY_STATE arg_self, 0);
        Py_BLOCK_THREADS;
        return THPUtils_(newAccreal)(__result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.std", 6, "(" THPTensorStr " source)", "(" THPTensorStr " source, bool unbiased)", "(" THPTensorStr " source, int dim, #" THPTensorStr " out)", "(" THPTensorStr " source, int dim, bool keepdim, #" THPTensorStr " out)", "(" THPTensorStr " source, int dim, bool unbiased, #" THPTensorStr " out)", "(" THPTensorStr " source, int dim, bool unbiased, bool keepdim, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_(norm)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_p = NULL;
    PyObject *__kw_dim = NULL;
    PyObject *__kw_keepdim = NULL;
    if (kwargs) {
      __kw_p = PyDict_GetItemString(kwargs, "p");
      __kw_dim = PyDict_GetItemString(kwargs, "dim");
      __kw_keepdim = PyDict_GetItemString(kwargs, "keepdim");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 4 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_p) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_p)) &&
          (__tuplecount > 1 || __kw_dim) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim)) &&
          (__tuplecount > 2 || __kw_keepdim) && PyBool_Check((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_keepdim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_p = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_p));
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim));
      bool arg_keepdim = ((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_keepdim) == Py_True ? true : false);
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(norm)(LIBRARY_STATE arg_result, arg_self, arg_p, arg_dim, arg_keepdim);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 3 &&
          (__tuplecount > 0 || __kw_p) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_p)) &&
          (__tuplecount > 1 || __kw_dim) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim)) &&
          (__tuplecount > 2 || __kw_keepdim) && PyBool_Check((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_keepdim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_p = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_p));
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim));
      bool arg_keepdim = ((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_keepdim) == Py_True ? true : false);
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(norm)(LIBRARY_STATE arg_result, arg_self, arg_p, arg_dim, arg_keepdim);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_p) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_p)) &&
          (__tuplecount > 1 || __kw_dim) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_p = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_p));
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim));
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        maybeThrowBackCompatKeepdimWarn("norm");
        Py_UNBLOCK_THREADS;
        THTensor_(norm)(LIBRARY_STATE arg_result, arg_self, arg_p, arg_dim, false);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_p) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_p)) &&
          (__tuplecount > 1 || __kw_dim) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_p = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_p));
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim));
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        maybeThrowBackCompatKeepdimWarn("norm");
        Py_UNBLOCK_THREADS;
        THTensor_(norm)(LIBRARY_STATE arg_result, arg_self, arg_p, arg_dim, false);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_p) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_p))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_p = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_p));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        accreal __result = THTensor_(normall)(LIBRARY_STATE arg_self, arg_p);
        Py_BLOCK_THREADS;
        return THPUtils_(newAccreal)(__result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        accreal __result = THTensor_(normall)(LIBRARY_STATE arg_self, AS_REAL(2));
        Py_BLOCK_THREADS;
        return THPUtils_(newAccreal)(__result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "norm", 4, "no arguments", "(" RealStr " p)", "(" RealStr " p, int dim, #" THPTensorStr " out)", "(" RealStr " p, int dim, bool keepdim, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_stateless_(norm)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    PyObject *__kw_p = NULL;
    PyObject *__kw_dim = NULL;
    PyObject *__kw_keepdim = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
      __kw_p = PyDict_GetItemString(kwargs, "p");
      __kw_dim = PyDict_GetItemString(kwargs, "dim");
      __kw_keepdim = PyDict_GetItemString(kwargs, "keepdim");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 5 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_p) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_p)) &&
          (__tuplecount > 2 || __kw_dim) && THPUtils_checkLong((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_dim)) &&
          (__tuplecount > 3 || __kw_keepdim) && PyBool_Check((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_keepdim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      real arg_p = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_p));
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_dim));
      bool arg_keepdim = ((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_keepdim) == Py_True ? true : false);
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(norm)(LIBRARY_STATE arg_result, arg_self, arg_p, arg_dim, arg_keepdim);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 4 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_p) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_p)) &&
          (__tuplecount > 2 || __kw_dim) && THPUtils_checkLong((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_dim)) &&
          (__tuplecount > 3 || __kw_keepdim) && PyBool_Check((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_keepdim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      real arg_p = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_p));
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_dim));
      bool arg_keepdim = ((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_keepdim) == Py_True ? true : false);
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(norm)(LIBRARY_STATE arg_result, arg_self, arg_p, arg_dim, arg_keepdim);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 4 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_p) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_p)) &&
          (__tuplecount > 2 || __kw_dim) && THPUtils_checkLong((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_dim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      real arg_p = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_p));
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_dim));
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        maybeThrowBackCompatKeepdimWarn("norm");
        Py_UNBLOCK_THREADS;
        THTensor_(norm)(LIBRARY_STATE arg_result, arg_self, arg_p, arg_dim, false);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 3 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_p) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_p)) &&
          (__tuplecount > 2 || __kw_dim) && THPUtils_checkLong((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_dim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      real arg_p = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_p));
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_dim));
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        maybeThrowBackCompatKeepdimWarn("norm");
        Py_UNBLOCK_THREADS;
        THTensor_(norm)(LIBRARY_STATE arg_result, arg_self, arg_p, arg_dim, false);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_p) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_p))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      real arg_p = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_p));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        accreal __result = THTensor_(normall)(LIBRARY_STATE arg_self, arg_p);
        Py_BLOCK_THREADS;
        return THPUtils_(newAccreal)(__result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        accreal __result = THTensor_(normall)(LIBRARY_STATE arg_self, AS_REAL(2));
        Py_BLOCK_THREADS;
        return THPUtils_(newAccreal)(__result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.norm", 4, "(" THPTensorStr " source)", "(" THPTensorStr " source, " RealStr " p)", "(" THPTensorStr " source, " RealStr " p, int dim, #" THPTensorStr " out)", "(" THPTensorStr " source, " RealStr " p, int dim, bool keepdim, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_(renorm)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_p = NULL;
    PyObject *__kw_dim = NULL;
    PyObject *__kw_maxnorm = NULL;
    if (kwargs) {
      __kw_p = PyDict_GetItemString(kwargs, "p");
      __kw_dim = PyDict_GetItemString(kwargs, "dim");
      __kw_maxnorm = PyDict_GetItemString(kwargs, "maxnorm");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 4 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_p) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_p)) &&
          (__tuplecount > 1 || __kw_dim) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim)) &&
          (__tuplecount > 2 || __kw_maxnorm) && THPUtils_(checkReal)((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_maxnorm))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_p = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_p));
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim));
      real arg_maxnorm = THPUtils_(unpackReal)((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_maxnorm));
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(renorm)(LIBRARY_STATE arg_result, arg_self, arg_p, arg_dim, arg_maxnorm);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 3 &&
          (__tuplecount > 0 || __kw_p) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_p)) &&
          (__tuplecount > 1 || __kw_dim) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim)) &&
          (__tuplecount > 2 || __kw_maxnorm) && THPUtils_(checkReal)((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_maxnorm))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_p = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_p));
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim));
      real arg_maxnorm = THPUtils_(unpackReal)((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_maxnorm));
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(renorm)(LIBRARY_STATE arg_result, arg_self, arg_p, arg_dim, arg_maxnorm);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "renorm", 1, "(" RealStr " p, int dim, " RealStr " maxnorm, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_stateless_(renorm)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    PyObject *__kw_p = NULL;
    PyObject *__kw_dim = NULL;
    PyObject *__kw_maxnorm = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
      __kw_p = PyDict_GetItemString(kwargs, "p");
      __kw_dim = PyDict_GetItemString(kwargs, "dim");
      __kw_maxnorm = PyDict_GetItemString(kwargs, "maxnorm");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 5 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_p) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_p)) &&
          (__tuplecount > 2 || __kw_dim) && THPUtils_checkLong((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_dim)) &&
          (__tuplecount > 3 || __kw_maxnorm) && THPUtils_(checkReal)((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_maxnorm))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      real arg_p = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_p));
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_dim));
      real arg_maxnorm = THPUtils_(unpackReal)((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_maxnorm));
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(renorm)(LIBRARY_STATE arg_result, arg_self, arg_p, arg_dim, arg_maxnorm);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 4 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_p) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_p)) &&
          (__tuplecount > 2 || __kw_dim) && THPUtils_checkLong((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_dim)) &&
          (__tuplecount > 3 || __kw_maxnorm) && THPUtils_(checkReal)((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_maxnorm))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      real arg_p = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_p));
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_dim));
      real arg_maxnorm = THPUtils_(unpackReal)((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_maxnorm));
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(renorm)(LIBRARY_STATE arg_result, arg_self, arg_p, arg_dim, arg_maxnorm);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.renorm", 1, "(" THPTensorStr " source, " RealStr " p, int dim, " RealStr " maxnorm, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_(renorm_)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_p = NULL;
    PyObject *__kw_dim = NULL;
    PyObject *__kw_maxnorm = NULL;
    if (kwargs) {
      __kw_p = PyDict_GetItemString(kwargs, "p");
      __kw_dim = PyDict_GetItemString(kwargs, "dim");
      __kw_maxnorm = PyDict_GetItemString(kwargs, "maxnorm");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 3 &&
          (__tuplecount > 0 || __kw_p) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_p)) &&
          (__tuplecount > 1 || __kw_dim) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim)) &&
          (__tuplecount > 2 || __kw_maxnorm) && THPUtils_(checkReal)((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_maxnorm))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_p = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_p));
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim));
      real arg_maxnorm = THPUtils_(unpackReal)((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_maxnorm));
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(renorm)(LIBRARY_STATE arg_self, arg_self, arg_p, arg_dim, arg_maxnorm);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "renorm_", 1, "(" RealStr " p, int dim, " RealStr " maxnorm)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_(dist)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_other = NULL;
    PyObject *__kw_p = NULL;
    if (kwargs) {
      __kw_other = PyDict_GetItemString(kwargs, "other");
      __kw_p = PyDict_GetItemString(kwargs, "p");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 2 &&
          (__tuplecount > 0 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_p) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_p))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other))->cdata;
      real arg_p = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_p));
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_self_guard.get(), arg_other_guard.get(),
              arg_self, arg_other,
              "self", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        accreal __result = THTensor_(dist)(LIBRARY_STATE arg_self, arg_other, arg_p);
        Py_BLOCK_THREADS;
        return THPUtils_(newAccreal)(__result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_other = arg_other_save;
      
      
    
    } else if (__argcount == 1 &&
          (__tuplecount > 0 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_self_guard.get(), arg_other_guard.get(),
              arg_self, arg_other,
              "self", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        accreal __result = THTensor_(dist)(LIBRARY_STATE arg_self, arg_other, AS_REAL(2));
        Py_BLOCK_THREADS;
        return THPUtils_(newAccreal)(__result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_other = arg_other_save;
      
      
    
    }

    THPUtils_invalidArguments(args, kwargs, "dist", 2, "(" THPTensorStr " other)", "(" THPTensorStr " other, " RealStr " p)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_stateless_(dist)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    PyObject *__kw_other = NULL;
    PyObject *__kw_p = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
      __kw_other = PyDict_GetItemString(kwargs, "other");
      __kw_p = PyDict_GetItemString(kwargs, "p");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 3 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_p) && THPUtils_(checkReal)((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_p))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other))->cdata;
      real arg_p = THPUtils_(unpackReal)((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_p));
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_self_guard.get(), arg_other_guard.get(),
              arg_self, arg_other,
              "self", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        accreal __result = THTensor_(dist)(LIBRARY_STATE arg_self, arg_other, arg_p);
        Py_BLOCK_THREADS;
        return THPUtils_(newAccreal)(__result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_other = arg_other_save;
      
      
    
    } else if (__argcount == 2 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_self_guard.get(), arg_other_guard.get(),
              arg_self, arg_other,
              "self", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        accreal __result = THTensor_(dist)(LIBRARY_STATE arg_self, arg_other, AS_REAL(2));
        Py_BLOCK_THREADS;
        return THPUtils_(newAccreal)(__result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_other = arg_other_save;
      
      
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.dist", 2, "(" THPTensorStr " source, " THPTensorStr " other)", "(" THPTensorStr " source, " THPTensorStr " other, " RealStr " p)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_(reciprocal)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    
    if (kwargs) {
      
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 1 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cinv)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cinv)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "reciprocal", 1, "(#" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_stateless_(reciprocal)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cinv)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cinv)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.reciprocal", 1, "(" THPTensorStr " source, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_(reciprocal_)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    
    if (kwargs) {
      
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cinv)(LIBRARY_STATE arg_self, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "reciprocal_", 1, "no arguments");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_(neg)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    
    if (kwargs) {
      
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 1 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(neg)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(neg)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "neg", 1, "(#" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_stateless_(neg)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(neg)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(neg)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.neg", 1, "(" THPTensorStr " source, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_(neg_)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    
    if (kwargs) {
      
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(neg)(LIBRARY_STATE arg_self, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "neg_", 1, "no arguments");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_(atan2)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_other = NULL;
    if (kwargs) {
      __kw_other = PyDict_GetItemString(kwargs, "other");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_self_guard.get(), arg_other_guard.get(),
              arg_self, arg_other,
              "self", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(atan2)(LIBRARY_STATE arg_result, arg_self, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_other = arg_other_save;
      
      
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_self_guard.get(), arg_other_guard.get(),
              arg_self, arg_other,
              "self", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(atan2)(LIBRARY_STATE arg_result, arg_self, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_other = arg_other_save;
      
      
    
    }

    THPUtils_invalidArguments(args, kwargs, "atan2", 1, "(" THPTensorStr " other, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_stateless_(atan2)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    PyObject *__kw_other = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
      __kw_other = PyDict_GetItemString(kwargs, "other");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_self_guard.get(), arg_other_guard.get(),
              arg_self, arg_other,
              "self", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(atan2)(LIBRARY_STATE arg_result, arg_self, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_other = arg_other_save;
      
      
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_self_guard.get(), arg_other_guard.get(),
              arg_self, arg_other,
              "self", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(atan2)(LIBRARY_STATE arg_result, arg_self, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_other = arg_other_save;
      
      
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.atan2", 1, "(" THPTensorStr " source, " THPTensorStr " other, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_(atan2_)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_other = NULL;
    if (kwargs) {
      __kw_other = PyDict_GetItemString(kwargs, "other");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 1 &&
          (__tuplecount > 0 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other))->cdata;
      
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_inplace1(LIBRARY_STATE arg_other_guard.get(), arg_other, arg_self,
              "other", "self", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(atan2)(LIBRARY_STATE arg_self, arg_self, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)(self);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_other = arg_other_save;
      
      
    
    }

    THPUtils_invalidArguments(args, kwargs, "atan2_", 1, "(" THPTensorStr " other)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif




// These options look the same in stateful method - only the first one will
// be available. Still, they differ in torch.pow.
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_(pow)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_exponent = NULL;
    PyObject *__kw_base = NULL;
    if (kwargs) {
      __kw_exponent = PyDict_GetItemString(kwargs, "exponent");
      __kw_base = PyDict_GetItemString(kwargs, "base");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_exponent) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_exponent))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_exponent = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_exponent));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(pow)(LIBRARY_STATE arg_result, arg_self, arg_exponent);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_exponent) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_exponent)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_exponent = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_exponent))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_exponent_save = arg_exponent;
      THTensorPtr arg_exponent_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_exponent->size, arg_exponent->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_exponent_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_self_guard.get(), arg_exponent_guard.get(),
              arg_self, arg_exponent,
              "self", "exponent", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_exponent = arg_exponent_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cpow)(LIBRARY_STATE arg_result, arg_self, arg_exponent);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_exponent = arg_exponent_save;
      
      
    
    } else if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_base) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_base))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      real arg_base = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_base));
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(tpow)(LIBRARY_STATE arg_result, arg_base, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_exponent) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_exponent))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_exponent = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_exponent));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(pow)(LIBRARY_STATE arg_result, arg_self, arg_exponent);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_exponent) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_exponent)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_exponent = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_exponent))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_exponent_save = arg_exponent;
      THTensorPtr arg_exponent_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_exponent->size, arg_exponent->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_exponent_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_self_guard.get(), arg_exponent_guard.get(),
              arg_self, arg_exponent,
              "self", "exponent", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_exponent = arg_exponent_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cpow)(LIBRARY_STATE arg_result, arg_self, arg_exponent);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_exponent = arg_exponent_save;
      
      
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_base) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_base))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      real arg_base = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_base));
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(tpow)(LIBRARY_STATE arg_result, arg_base, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "pow", 3, "(" RealStr " base, #" THPTensorStr " out)", "(" RealStr " exponent, #" THPTensorStr " out)", "(" THPTensorStr " exponent, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_stateless_(pow)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    PyObject *__kw_exponent = NULL;
    PyObject *__kw_base = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
      __kw_exponent = PyDict_GetItemString(kwargs, "exponent");
      __kw_base = PyDict_GetItemString(kwargs, "base");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_exponent) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_exponent))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      real arg_exponent = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_exponent));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(pow)(LIBRARY_STATE arg_result, arg_self, arg_exponent);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_exponent) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_exponent)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_exponent = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_exponent))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_exponent_save = arg_exponent;
      THTensorPtr arg_exponent_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_exponent->size, arg_exponent->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_exponent_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_self_guard.get(), arg_exponent_guard.get(),
              arg_self, arg_exponent,
              "self", "exponent", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_exponent = arg_exponent_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cpow)(LIBRARY_STATE arg_result, arg_self, arg_exponent);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_exponent = arg_exponent_save;
      
      
    
    } else if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_base) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_base)) &&
          (__tuplecount > 1 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      real arg_base = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_base));
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(tpow)(LIBRARY_STATE arg_result, arg_base, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_exponent) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_exponent))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      real arg_exponent = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_exponent));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(pow)(LIBRARY_STATE arg_result, arg_self, arg_exponent);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_exponent) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_exponent)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_exponent = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_exponent))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_exponent_save = arg_exponent;
      THTensorPtr arg_exponent_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_exponent->size, arg_exponent->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_exponent_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_self_guard.get(), arg_exponent_guard.get(),
              arg_self, arg_exponent,
              "self", "exponent", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_exponent = arg_exponent_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cpow)(LIBRARY_STATE arg_result, arg_self, arg_exponent);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_exponent = arg_exponent_save;
      
      
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_base) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_base)) &&
          (__tuplecount > 1 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      real arg_base = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_base));
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(tpow)(LIBRARY_STATE arg_result, arg_base, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.pow", 3, "(" RealStr " base, " THPTensorStr " source, #" THPTensorStr " out)", "(" THPTensorStr " source, " RealStr " exponent, #" THPTensorStr " out)", "(" THPTensorStr " source, " THPTensorStr " exponent, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_(pow_)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_exponent = NULL;
    if (kwargs) {
      __kw_exponent = PyDict_GetItemString(kwargs, "exponent");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 1 &&
          (__tuplecount > 0 || __kw_exponent) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_exponent))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_exponent = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_exponent));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(pow)(LIBRARY_STATE arg_self, arg_self, arg_exponent);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)(self);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 1 &&
          (__tuplecount > 0 || __kw_exponent) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_exponent)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_exponent = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_exponent))->cdata;
      
      THTensor *arg_exponent_save = arg_exponent;
      THTensorPtr arg_exponent_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_exponent->size, arg_exponent->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_exponent_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_inplace1(LIBRARY_STATE arg_exponent_guard.get(), arg_exponent, arg_self,
              "exponent", "self", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_exponent = arg_exponent_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cpow)(LIBRARY_STATE arg_self, arg_self, arg_exponent);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)(self);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_exponent = arg_exponent_save;
      
      
    
    }

    THPUtils_invalidArguments(args, kwargs, "pow_", 2, "(" RealStr " exponent)", "(" THPTensorStr " exponent)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_(lerp)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_end = NULL;
    PyObject *__kw_weight = NULL;
    if (kwargs) {
      __kw_end = PyDict_GetItemString(kwargs, "end");
      __kw_weight = PyDict_GetItemString(kwargs, "weight");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_end) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_end)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_weight) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_weight))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_end = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_end))->cdata;
      real arg_weight = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_weight));
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_end_save = arg_end;
      THTensorPtr arg_end_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_end->size, arg_end->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_end_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_self_guard.get(), arg_end_guard.get(),
              arg_self, arg_end,
              "self", "end", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_end = arg_end_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(lerp)(LIBRARY_STATE arg_result, arg_self, arg_end, arg_weight);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_end = arg_end_save;
      
      
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_end) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_end)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_weight) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_weight))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_end = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_end))->cdata;
      real arg_weight = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_weight));
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_end_save = arg_end;
      THTensorPtr arg_end_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_end->size, arg_end->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_end_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_self_guard.get(), arg_end_guard.get(),
              arg_self, arg_end,
              "self", "end", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_end = arg_end_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(lerp)(LIBRARY_STATE arg_result, arg_self, arg_end, arg_weight);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_end = arg_end_save;
      
      
    
    }

    THPUtils_invalidArguments(args, kwargs, "lerp", 1, "(" THPTensorStr " end, " RealStr " weight, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_stateless_(lerp)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    PyObject *__kw_end = NULL;
    PyObject *__kw_weight = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
      __kw_end = PyDict_GetItemString(kwargs, "end");
      __kw_weight = PyDict_GetItemString(kwargs, "weight");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 4 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_end) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_end)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_weight) && THPUtils_(checkReal)((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_weight))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_end = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_end))->cdata;
      real arg_weight = THPUtils_(unpackReal)((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_weight));
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_end_save = arg_end;
      THTensorPtr arg_end_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_end->size, arg_end->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_end_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_self_guard.get(), arg_end_guard.get(),
              arg_self, arg_end,
              "self", "end", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_end = arg_end_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(lerp)(LIBRARY_STATE arg_result, arg_self, arg_end, arg_weight);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_end = arg_end_save;
      
      
    
    } else if (___out == NULL &&
          __argcount == 3 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_end) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_end)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_weight) && THPUtils_(checkReal)((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_weight))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_end = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_end))->cdata;
      real arg_weight = THPUtils_(unpackReal)((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_weight));
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_end_save = arg_end;
      THTensorPtr arg_end_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_end->size, arg_end->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_end_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_self_guard.get(), arg_end_guard.get(),
              arg_self, arg_end,
              "self", "end", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_end = arg_end_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(lerp)(LIBRARY_STATE arg_result, arg_self, arg_end, arg_weight);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_end = arg_end_save;
      
      
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.lerp", 1, "(" THPTensorStr " source, " THPTensorStr " end, " RealStr " weight, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_(lerp_)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_end = NULL;
    PyObject *__kw_weight = NULL;
    if (kwargs) {
      __kw_end = PyDict_GetItemString(kwargs, "end");
      __kw_weight = PyDict_GetItemString(kwargs, "weight");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 2 &&
          (__tuplecount > 0 || __kw_end) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_end)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_weight) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_weight))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_end = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_end))->cdata;
      real arg_weight = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_weight));
      
      THTensor *arg_end_save = arg_end;
      THTensorPtr arg_end_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_end->size, arg_end->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_end_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_inplace1(LIBRARY_STATE arg_end_guard.get(), arg_end, arg_self,
              "end", "self", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_end = arg_end_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(lerp)(LIBRARY_STATE arg_self, arg_self, arg_end, arg_weight);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_end = arg_end_save;
      
      
    
    }

    THPUtils_invalidArguments(args, kwargs, "lerp_", 1, "(" THPTensorStr " end, " RealStr " weight)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_FLOAT) || defined(TH_REAL_IS_DOUBLE))
PyObject * THPTensor_stateless_(linspace)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_start = NULL;
    PyObject *__kw_end = NULL;
    PyObject *__kw_steps = NULL;
    if (kwargs) {
      __kw_start = PyDict_GetItemString(kwargs, "start");
      __kw_end = PyDict_GetItemString(kwargs, "end");
      __kw_steps = PyDict_GetItemString(kwargs, "steps");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 4 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_start) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_start)) &&
          (__tuplecount > 1 || __kw_end) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_end)) &&
          (__tuplecount > 2 || __kw_steps) && THPUtils_checkLong((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_steps))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      real arg_start = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_start));
      real arg_end = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_end));
      int64_t arg_steps = THPUtils_unpackLong((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_steps));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(linspace)(LIBRARY_STATE arg_result, arg_start, arg_end, arg_steps);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 3 &&
          (__tuplecount > 0 || __kw_start) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_start)) &&
          (__tuplecount > 1 || __kw_end) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_end)) &&
          (__tuplecount > 2 || __kw_steps) && THPUtils_checkLong((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_steps))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      real arg_start = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_start));
      real arg_end = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_end));
      int64_t arg_steps = THPUtils_unpackLong((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_steps));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(linspace)(LIBRARY_STATE arg_result, arg_start, arg_end, arg_steps);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_start) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_start)) &&
          (__tuplecount > 1 || __kw_end) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_end))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      real arg_start = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_start));
      real arg_end = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_end));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(linspace)(LIBRARY_STATE arg_result, arg_start, arg_end, 100);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_start) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_start)) &&
          (__tuplecount > 1 || __kw_end) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_end))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      real arg_start = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_start));
      real arg_end = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_end));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(linspace)(LIBRARY_STATE arg_result, arg_start, arg_end, 100);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.linspace", 2, "(" RealStr " start, " RealStr " end, #" THPTensorStr " out)", "(" RealStr " start, " RealStr " end, int steps, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_FLOAT) || defined(TH_REAL_IS_DOUBLE))
PyObject * THPTensor_stateless_(logspace)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_start = NULL;
    PyObject *__kw_end = NULL;
    PyObject *__kw_steps = NULL;
    if (kwargs) {
      __kw_start = PyDict_GetItemString(kwargs, "start");
      __kw_end = PyDict_GetItemString(kwargs, "end");
      __kw_steps = PyDict_GetItemString(kwargs, "steps");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 4 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_start) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_start)) &&
          (__tuplecount > 1 || __kw_end) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_end)) &&
          (__tuplecount > 2 || __kw_steps) && THPUtils_checkLong((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_steps))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      real arg_start = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_start));
      real arg_end = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_end));
      int64_t arg_steps = THPUtils_unpackLong((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_steps));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(logspace)(LIBRARY_STATE arg_result, arg_start, arg_end, arg_steps);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 3 &&
          (__tuplecount > 0 || __kw_start) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_start)) &&
          (__tuplecount > 1 || __kw_end) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_end)) &&
          (__tuplecount > 2 || __kw_steps) && THPUtils_checkLong((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_steps))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      real arg_start = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_start));
      real arg_end = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_end));
      int64_t arg_steps = THPUtils_unpackLong((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_steps));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(logspace)(LIBRARY_STATE arg_result, arg_start, arg_end, arg_steps);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_start) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_start)) &&
          (__tuplecount > 1 || __kw_end) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_end))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      real arg_start = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_start));
      real arg_end = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_end));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(logspace)(LIBRARY_STATE arg_result, arg_start, arg_end, 100);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_start) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_start)) &&
          (__tuplecount > 1 || __kw_end) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_end))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      real arg_start = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_start));
      real arg_end = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_end));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(logspace)(LIBRARY_STATE arg_result, arg_start, arg_end, 100);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.logspace", 2, "(" RealStr " start, " RealStr " end, #" THPTensorStr " out)", "(" RealStr " start, " RealStr " end, int steps, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_FLOAT) || defined(TH_REAL_IS_DOUBLE))
PyObject * THPTensor_(histc)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_bins = NULL;
    PyObject *__kw_min = NULL;
    PyObject *__kw_max = NULL;
    if (kwargs) {
      __kw_bins = PyDict_GetItemString(kwargs, "bins");
      __kw_min = PyDict_GetItemString(kwargs, "min");
      __kw_max = PyDict_GetItemString(kwargs, "max");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 4 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_bins) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_bins)) &&
          (__tuplecount > 1 || __kw_min) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_min)) &&
          (__tuplecount > 2 || __kw_max) && THPUtils_(checkReal)((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_max))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_bins = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_bins));
      real arg_min = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_min));
      real arg_max = THPUtils_(unpackReal)((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_max));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(histc)(LIBRARY_STATE arg_result, arg_self, arg_bins, arg_min, arg_max);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_bins) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_bins)) &&
          (__tuplecount > 1 || __kw_min) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_min))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_bins = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_bins));
      real arg_min = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_min));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(histc)(LIBRARY_STATE arg_result, arg_self, arg_bins, arg_min, 0);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 3 &&
          (__tuplecount > 0 || __kw_bins) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_bins)) &&
          (__tuplecount > 1 || __kw_min) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_min)) &&
          (__tuplecount > 2 || __kw_max) && THPUtils_(checkReal)((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_max))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_bins = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_bins));
      real arg_min = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_min));
      real arg_max = THPUtils_(unpackReal)((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_max));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(histc)(LIBRARY_STATE arg_result, arg_self, arg_bins, arg_min, arg_max);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_bins) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_bins))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_bins = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_bins));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(histc)(LIBRARY_STATE arg_result, arg_self, arg_bins, 0, 0);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_bins) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_bins)) &&
          (__tuplecount > 1 || __kw_min) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_min))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_bins = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_bins));
      real arg_min = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_min));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(histc)(LIBRARY_STATE arg_result, arg_self, arg_bins, arg_min, 0);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 1 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(histc)(LIBRARY_STATE arg_result, arg_self, 100, 0, 0);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_bins) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_bins))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_bins = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_bins));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(histc)(LIBRARY_STATE arg_result, arg_self, arg_bins, 0, 0);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(histc)(LIBRARY_STATE arg_result, arg_self, 100, 0, 0);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "histc", 4, "(#" THPTensorStr " out)", "(int bins, #" THPTensorStr " out)", "(int bins, " RealStr " min, #" THPTensorStr " out)", "(int bins, " RealStr " min, " RealStr " max, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_FLOAT) || defined(TH_REAL_IS_DOUBLE))
PyObject * THPTensor_stateless_(histc)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    PyObject *__kw_bins = NULL;
    PyObject *__kw_min = NULL;
    PyObject *__kw_max = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
      __kw_bins = PyDict_GetItemString(kwargs, "bins");
      __kw_min = PyDict_GetItemString(kwargs, "min");
      __kw_max = PyDict_GetItemString(kwargs, "max");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 5 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_bins) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_bins)) &&
          (__tuplecount > 2 || __kw_min) && THPUtils_(checkReal)((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_min)) &&
          (__tuplecount > 3 || __kw_max) && THPUtils_(checkReal)((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_max))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int64_t arg_bins = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_bins));
      real arg_min = THPUtils_(unpackReal)((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_min));
      real arg_max = THPUtils_(unpackReal)((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_max));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(histc)(LIBRARY_STATE arg_result, arg_self, arg_bins, arg_min, arg_max);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 4 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_bins) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_bins)) &&
          (__tuplecount > 2 || __kw_min) && THPUtils_(checkReal)((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_min))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int64_t arg_bins = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_bins));
      real arg_min = THPUtils_(unpackReal)((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_min));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(histc)(LIBRARY_STATE arg_result, arg_self, arg_bins, arg_min, 0);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 4 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_bins) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_bins)) &&
          (__tuplecount > 2 || __kw_min) && THPUtils_(checkReal)((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_min)) &&
          (__tuplecount > 3 || __kw_max) && THPUtils_(checkReal)((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_max))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int64_t arg_bins = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_bins));
      real arg_min = THPUtils_(unpackReal)((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_min));
      real arg_max = THPUtils_(unpackReal)((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_max));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(histc)(LIBRARY_STATE arg_result, arg_self, arg_bins, arg_min, arg_max);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_bins) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_bins))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int64_t arg_bins = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_bins));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(histc)(LIBRARY_STATE arg_result, arg_self, arg_bins, 0, 0);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 3 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_bins) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_bins)) &&
          (__tuplecount > 2 || __kw_min) && THPUtils_(checkReal)((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_min))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int64_t arg_bins = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_bins));
      real arg_min = THPUtils_(unpackReal)((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_min));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(histc)(LIBRARY_STATE arg_result, arg_self, arg_bins, arg_min, 0);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(histc)(LIBRARY_STATE arg_result, arg_self, 100, 0, 0);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_bins) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_bins))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int64_t arg_bins = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_bins));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(histc)(LIBRARY_STATE arg_result, arg_self, arg_bins, 0, 0);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(histc)(LIBRARY_STATE arg_result, arg_self, 100, 0, 0);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.histc", 4, "(" THPTensorStr " source, #" THPTensorStr " out)", "(" THPTensorStr " source, int bins, #" THPTensorStr " out)", "(" THPTensorStr " source, int bins, " RealStr " min, #" THPTensorStr " out)", "(" THPTensorStr " source, int bins, " RealStr " min, " RealStr " max, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_(zero_)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    
    if (kwargs) {
      
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(zero)(LIBRARY_STATE arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "zero_", 1, "no arguments");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_(sum)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_dim = NULL;
    PyObject *__kw_keepdim = NULL;
    if (kwargs) {
      __kw_dim = PyDict_GetItemString(kwargs, "dim");
      __kw_keepdim = PyDict_GetItemString(kwargs, "keepdim");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_dim) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim)) &&
          (__tuplecount > 1 || __kw_keepdim) && PyBool_Check((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_keepdim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim));
      bool arg_keepdim = ((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_keepdim) == Py_True ? true : false);
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(sum)(LIBRARY_STATE arg_result, arg_self, arg_dim, arg_keepdim);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_dim) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim)) &&
          (__tuplecount > 1 || __kw_keepdim) && PyBool_Check((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_keepdim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim));
      bool arg_keepdim = ((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_keepdim) == Py_True ? true : false);
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(sum)(LIBRARY_STATE arg_result, arg_self, arg_dim, arg_keepdim);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_dim) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim));
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        maybeThrowBackCompatKeepdimWarn("sum");
        Py_UNBLOCK_THREADS;
        THTensor_(sum)(LIBRARY_STATE arg_result, arg_self, arg_dim, false);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_dim) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim));
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        maybeThrowBackCompatKeepdimWarn("sum");
        Py_UNBLOCK_THREADS;
        THTensor_(sum)(LIBRARY_STATE arg_result, arg_self, arg_dim, false);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        accreal __result = THTensor_(sumall)(LIBRARY_STATE arg_self);
        Py_BLOCK_THREADS;
        return THPUtils_(newAccreal)(__result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "sum", 3, "no arguments", "(int dim, #" THPTensorStr " out)", "(int dim, bool keepdim, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_stateless_(sum)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    PyObject *__kw_dim = NULL;
    PyObject *__kw_keepdim = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
      __kw_dim = PyDict_GetItemString(kwargs, "dim");
      __kw_keepdim = PyDict_GetItemString(kwargs, "keepdim");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 4 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_dim) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim)) &&
          (__tuplecount > 2 || __kw_keepdim) && PyBool_Check((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_keepdim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim));
      bool arg_keepdim = ((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_keepdim) == Py_True ? true : false);
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(sum)(LIBRARY_STATE arg_result, arg_self, arg_dim, arg_keepdim);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 3 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_dim) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim)) &&
          (__tuplecount > 2 || __kw_keepdim) && PyBool_Check((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_keepdim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim));
      bool arg_keepdim = ((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_keepdim) == Py_True ? true : false);
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(sum)(LIBRARY_STATE arg_result, arg_self, arg_dim, arg_keepdim);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_dim) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim));
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        maybeThrowBackCompatKeepdimWarn("sum");
        Py_UNBLOCK_THREADS;
        THTensor_(sum)(LIBRARY_STATE arg_result, arg_self, arg_dim, false);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_dim) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim));
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        maybeThrowBackCompatKeepdimWarn("sum");
        Py_UNBLOCK_THREADS;
        THTensor_(sum)(LIBRARY_STATE arg_result, arg_self, arg_dim, false);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        accreal __result = THTensor_(sumall)(LIBRARY_STATE arg_self);
        Py_BLOCK_THREADS;
        return THPUtils_(newAccreal)(__result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.sum", 3, "(" THPTensorStr " source)", "(" THPTensorStr " source, int dim, #" THPTensorStr " out)", "(" THPTensorStr " source, int dim, bool keepdim, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_(prod)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_dim = NULL;
    PyObject *__kw_keepdim = NULL;
    if (kwargs) {
      __kw_dim = PyDict_GetItemString(kwargs, "dim");
      __kw_keepdim = PyDict_GetItemString(kwargs, "keepdim");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_dim) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim)) &&
          (__tuplecount > 1 || __kw_keepdim) && PyBool_Check((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_keepdim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim));
      bool arg_keepdim = ((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_keepdim) == Py_True ? true : false);
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(prod)(LIBRARY_STATE arg_result, arg_self, arg_dim, arg_keepdim);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_dim) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim)) &&
          (__tuplecount > 1 || __kw_keepdim) && PyBool_Check((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_keepdim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim));
      bool arg_keepdim = ((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_keepdim) == Py_True ? true : false);
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(prod)(LIBRARY_STATE arg_result, arg_self, arg_dim, arg_keepdim);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_dim) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim));
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        maybeThrowBackCompatKeepdimWarn("prod");
        Py_UNBLOCK_THREADS;
        THTensor_(prod)(LIBRARY_STATE arg_result, arg_self, arg_dim, false);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_dim) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim));
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        maybeThrowBackCompatKeepdimWarn("prod");
        Py_UNBLOCK_THREADS;
        THTensor_(prod)(LIBRARY_STATE arg_result, arg_self, arg_dim, false);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        accreal __result = THTensor_(prodall)(LIBRARY_STATE arg_self);
        Py_BLOCK_THREADS;
        return THPUtils_(newAccreal)(__result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "prod", 3, "no arguments", "(int dim, #" THPTensorStr " out)", "(int dim, bool keepdim, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_stateless_(prod)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    PyObject *__kw_dim = NULL;
    PyObject *__kw_keepdim = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
      __kw_dim = PyDict_GetItemString(kwargs, "dim");
      __kw_keepdim = PyDict_GetItemString(kwargs, "keepdim");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 4 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_dim) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim)) &&
          (__tuplecount > 2 || __kw_keepdim) && PyBool_Check((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_keepdim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim));
      bool arg_keepdim = ((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_keepdim) == Py_True ? true : false);
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(prod)(LIBRARY_STATE arg_result, arg_self, arg_dim, arg_keepdim);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 3 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_dim) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim)) &&
          (__tuplecount > 2 || __kw_keepdim) && PyBool_Check((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_keepdim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim));
      bool arg_keepdim = ((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_keepdim) == Py_True ? true : false);
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(prod)(LIBRARY_STATE arg_result, arg_self, arg_dim, arg_keepdim);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_dim) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim));
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        maybeThrowBackCompatKeepdimWarn("prod");
        Py_UNBLOCK_THREADS;
        THTensor_(prod)(LIBRARY_STATE arg_result, arg_self, arg_dim, false);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_dim) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim));
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        maybeThrowBackCompatKeepdimWarn("prod");
        Py_UNBLOCK_THREADS;
        THTensor_(prod)(LIBRARY_STATE arg_result, arg_self, arg_dim, false);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        accreal __result = THTensor_(prodall)(LIBRARY_STATE arg_self);
        Py_BLOCK_THREADS;
        return THPUtils_(newAccreal)(__result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.prod", 3, "(" THPTensorStr " source)", "(" THPTensorStr " source, int dim, #" THPTensorStr " out)", "(" THPTensorStr " source, int dim, bool keepdim, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_(cumsum)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_dim = NULL;
    if (kwargs) {
      __kw_dim = PyDict_GetItemString(kwargs, "dim");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_dim) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim));
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cumsum)(LIBRARY_STATE arg_result, arg_self, arg_dim);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_dim) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim));
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cumsum)(LIBRARY_STATE arg_result, arg_self, arg_dim);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "cumsum", 1, "(int dim, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_stateless_(cumsum)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    PyObject *__kw_dim = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
      __kw_dim = PyDict_GetItemString(kwargs, "dim");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_dim) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim));
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cumsum)(LIBRARY_STATE arg_result, arg_self, arg_dim);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_dim) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim));
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cumsum)(LIBRARY_STATE arg_result, arg_self, arg_dim);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.cumsum", 1, "(" THPTensorStr " source, int dim, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_(cumprod)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_dim = NULL;
    if (kwargs) {
      __kw_dim = PyDict_GetItemString(kwargs, "dim");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_dim) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim));
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cumprod)(LIBRARY_STATE arg_result, arg_self, arg_dim);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_dim) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim));
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cumprod)(LIBRARY_STATE arg_result, arg_self, arg_dim);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "cumprod", 1, "(int dim, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_stateless_(cumprod)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    PyObject *__kw_dim = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
      __kw_dim = PyDict_GetItemString(kwargs, "dim");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_dim) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim));
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cumprod)(LIBRARY_STATE arg_result, arg_self, arg_dim);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_dim) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim));
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cumprod)(LIBRARY_STATE arg_result, arg_self, arg_dim);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.cumprod", 1, "(" THPTensorStr " source, int dim, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_(sign)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    
    if (kwargs) {
      
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 1 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(sign)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(sign)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "sign", 1, "(#" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_stateless_(sign)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(sign)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(sign)(LIBRARY_STATE arg_result, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.sign", 1, "(" THPTensorStr " source, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_(sign_)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    
    if (kwargs) {
      
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(sign)(LIBRARY_STATE arg_self, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "sign_", 1, "no arguments");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_(trace)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    
    if (kwargs) {
      
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        accreal __result = THTensor_(trace)(LIBRARY_STATE arg_self);
        Py_BLOCK_THREADS;
        return THPUtils_(newAccreal)(__result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "trace", 1, "no arguments");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_stateless_(trace)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 1 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        accreal __result = THTensor_(trace)(LIBRARY_STATE arg_self);
        Py_BLOCK_THREADS;
        return THPUtils_(newAccreal)(__result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.trace", 1, "(" THPTensorStr " source)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_(add)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_value = NULL;
    PyObject *__kw_other = NULL;
    if (kwargs) {
      __kw_value = PyDict_GetItemString(kwargs, "value");
      __kw_other = PyDict_GetItemString(kwargs, "other");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value)) &&
          (__tuplecount > 1 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value));
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_self_guard.get(), arg_other_guard.get(),
              arg_self, arg_other,
              "self", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cadd)(LIBRARY_STATE arg_result, arg_self, arg_value, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_other = arg_other_save;
      
      
    #if !IS_DISTRIBUTED
          
    } else if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value)) &&
          (__tuplecount > 1 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other)) == THSPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value));
      THSTensor* arg_other = ((THSPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(spcadd)(LIBRARY_STATE arg_result, arg_self, arg_value, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif

    } else if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(add)(LIBRARY_STATE arg_result, arg_self, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value)) &&
          (__tuplecount > 1 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value));
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_self_guard.get(), arg_other_guard.get(),
              arg_self, arg_other,
              "self", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cadd)(LIBRARY_STATE arg_result, arg_self, arg_value, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_other = arg_other_save;
      
      
    
    } else if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_self_guard.get(), arg_other_guard.get(),
              arg_self, arg_other,
              "self", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cadd)(LIBRARY_STATE arg_result, arg_self, AS_REAL(1), arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_other = arg_other_save;
      
      
    #if !IS_DISTRIBUTED
          
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value)) &&
          (__tuplecount > 1 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other)) == THSPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value));
      THSTensor* arg_other = ((THSPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(spcadd)(LIBRARY_STATE arg_result, arg_self, arg_value, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif
#if !IS_DISTRIBUTED
          
    } else if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other)) == THSPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THSTensor* arg_other = ((THSPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(spcadd)(LIBRARY_STATE arg_result, arg_self, AS_REAL(1), arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif

    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(add)(LIBRARY_STATE arg_result, arg_self, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_self_guard.get(), arg_other_guard.get(),
              arg_self, arg_other,
              "self", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cadd)(LIBRARY_STATE arg_result, arg_self, AS_REAL(1), arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_other = arg_other_save;
      
      
    #if !IS_DISTRIBUTED
          
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other)) == THSPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THSTensor* arg_other = ((THSPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(spcadd)(LIBRARY_STATE arg_result, arg_self, AS_REAL(1), arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif

    }

    THPUtils_invalidArguments(args, kwargs, "add", 5, "(" RealStr " value, #" THPTensorStr " out)", "(" THPTensorStr " other, #" THPTensorStr " out)", "(" THSPTensorStr " other, #" THPTensorStr " out)", "(" RealStr " value, " THPTensorStr " other, #" THPTensorStr " out)", "(" RealStr " value, " THSPTensorStr " other, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_stateless_(add)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    PyObject *__kw_value = NULL;
    PyObject *__kw_other = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
      __kw_value = PyDict_GetItemString(kwargs, "value");
      __kw_other = PyDict_GetItemString(kwargs, "other");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 4 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value)) &&
          (__tuplecount > 2 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value));
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_other))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_self_guard.get(), arg_other_guard.get(),
              arg_self, arg_other,
              "self", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cadd)(LIBRARY_STATE arg_result, arg_self, arg_value, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_other = arg_other_save;
      
      
    #if !IS_DISTRIBUTED
          
    } else if (___out != NULL &&
          __argcount == 4 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value)) &&
          (__tuplecount > 2 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_other)) == THSPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value));
      THSTensor* arg_other = ((THSPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_other))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(spcadd)(LIBRARY_STATE arg_result, arg_self, arg_value, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif

    } else if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(add)(LIBRARY_STATE arg_result, arg_self, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 3 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value)) &&
          (__tuplecount > 2 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value));
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_other))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_self_guard.get(), arg_other_guard.get(),
              arg_self, arg_other,
              "self", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cadd)(LIBRARY_STATE arg_result, arg_self, arg_value, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_other = arg_other_save;
      
      
    
    } else if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_self_guard.get(), arg_other_guard.get(),
              arg_self, arg_other,
              "self", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cadd)(LIBRARY_STATE arg_result, arg_self, AS_REAL(1), arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_other = arg_other_save;
      
      
    #if !IS_DISTRIBUTED
          
    } else if (___out == NULL &&
          __argcount == 3 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value)) &&
          (__tuplecount > 2 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_other)) == THSPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value));
      THSTensor* arg_other = ((THSPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_other))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(spcadd)(LIBRARY_STATE arg_result, arg_self, arg_value, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif
#if !IS_DISTRIBUTED
          
    } else if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other)) == THSPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THSTensor* arg_other = ((THSPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(spcadd)(LIBRARY_STATE arg_result, arg_self, AS_REAL(1), arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif

    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(add)(LIBRARY_STATE arg_result, arg_self, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_self_guard.get(), arg_other_guard.get(),
              arg_self, arg_other,
              "self", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cadd)(LIBRARY_STATE arg_result, arg_self, AS_REAL(1), arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_other = arg_other_save;
      
      
    #if !IS_DISTRIBUTED
          
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other)) == THSPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THSTensor* arg_other = ((THSPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(spcadd)(LIBRARY_STATE arg_result, arg_self, AS_REAL(1), arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif

    }

    THPUtils_invalidArguments(args, kwargs, "torch.add", 5, "(" THPTensorStr " source, " RealStr " value, #" THPTensorStr " out)", "(" THPTensorStr " source, " THPTensorStr " other, #" THPTensorStr " out)", "(" THPTensorStr " source, " THSPTensorStr " other, #" THPTensorStr " out)", "(" THPTensorStr " source, " RealStr " value, " THPTensorStr " other, #" THPTensorStr " out)", "(" THPTensorStr " source, " RealStr " value, " THSPTensorStr " other, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_(add_)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_value = NULL;
    PyObject *__kw_other = NULL;
    if (kwargs) {
      __kw_value = PyDict_GetItemString(kwargs, "value");
      __kw_other = PyDict_GetItemString(kwargs, "other");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 2 &&
          (__tuplecount > 0 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value)) &&
          (__tuplecount > 1 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value));
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other))->cdata;
      
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_inplace1(LIBRARY_STATE arg_other_guard.get(), arg_other, arg_self,
              "other", "self", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cadd)(LIBRARY_STATE arg_self, arg_self, arg_value, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)(self);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_other = arg_other_save;
      
      
    #if !IS_DISTRIBUTED
          
    } else if (__argcount == 2 &&
          (__tuplecount > 0 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value)) &&
          (__tuplecount > 1 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other)) == THSPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value));
      THSTensor* arg_other = ((THSPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(spcadd)(LIBRARY_STATE arg_self, arg_self, arg_value, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)(self);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif

    } else if (__argcount == 1 &&
          (__tuplecount > 0 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(add)(LIBRARY_STATE arg_self, arg_self, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)(self);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 1 &&
          (__tuplecount > 0 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other))->cdata;
      
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_inplace1(LIBRARY_STATE arg_other_guard.get(), arg_other, arg_self,
              "other", "self", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cadd)(LIBRARY_STATE arg_self, arg_self, AS_REAL(1), arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)(self);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_other = arg_other_save;
      
      
    #if !IS_DISTRIBUTED
          
    } else if (__argcount == 1 &&
          (__tuplecount > 0 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other)) == THSPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THSTensor* arg_other = ((THSPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(spcadd)(LIBRARY_STATE arg_self, arg_self, AS_REAL(1), arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)(self);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif

    }

    THPUtils_invalidArguments(args, kwargs, "add_", 5, "(" RealStr " value)", "(" THPTensorStr " other)", "(" THSPTensorStr " other)", "(" RealStr " value, " THPTensorStr " other)", "(" RealStr " value, " THSPTensorStr " other)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif



#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_(sub)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_value = NULL;
    PyObject *__kw_other = NULL;
    if (kwargs) {
      __kw_value = PyDict_GetItemString(kwargs, "value");
      __kw_other = PyDict_GetItemString(kwargs, "other");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value)) &&
          (__tuplecount > 1 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value));
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_self_guard.get(), arg_other_guard.get(),
              arg_self, arg_other,
              "self", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(csub)(LIBRARY_STATE arg_result, arg_self, arg_value, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_other = arg_other_save;
      
      
    
    } else if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(sub)(LIBRARY_STATE arg_result, arg_self, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value)) &&
          (__tuplecount > 1 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value));
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_self_guard.get(), arg_other_guard.get(),
              arg_self, arg_other,
              "self", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(csub)(LIBRARY_STATE arg_result, arg_self, arg_value, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_other = arg_other_save;
      
      
    
    } else if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_self_guard.get(), arg_other_guard.get(),
              arg_self, arg_other,
              "self", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(csub)(LIBRARY_STATE arg_result, arg_self, AS_REAL(1), arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_other = arg_other_save;
      
      
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(sub)(LIBRARY_STATE arg_result, arg_self, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_self_guard.get(), arg_other_guard.get(),
              arg_self, arg_other,
              "self", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(csub)(LIBRARY_STATE arg_result, arg_self, AS_REAL(1), arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_other = arg_other_save;
      
      
    
    }

    THPUtils_invalidArguments(args, kwargs, "sub", 3, "(" RealStr " value, #" THPTensorStr " out)", "(" THPTensorStr " other, #" THPTensorStr " out)", "(" RealStr " value, " THPTensorStr " other, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_stateless_(sub)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    PyObject *__kw_value = NULL;
    PyObject *__kw_other = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
      __kw_value = PyDict_GetItemString(kwargs, "value");
      __kw_other = PyDict_GetItemString(kwargs, "other");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 4 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value)) &&
          (__tuplecount > 2 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value));
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_other))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_self_guard.get(), arg_other_guard.get(),
              arg_self, arg_other,
              "self", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(csub)(LIBRARY_STATE arg_result, arg_self, arg_value, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_other = arg_other_save;
      
      
    
    } else if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(sub)(LIBRARY_STATE arg_result, arg_self, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 3 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value)) &&
          (__tuplecount > 2 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value));
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_other))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_self_guard.get(), arg_other_guard.get(),
              arg_self, arg_other,
              "self", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(csub)(LIBRARY_STATE arg_result, arg_self, arg_value, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_other = arg_other_save;
      
      
    
    } else if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_self_guard.get(), arg_other_guard.get(),
              arg_self, arg_other,
              "self", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(csub)(LIBRARY_STATE arg_result, arg_self, AS_REAL(1), arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_other = arg_other_save;
      
      
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(sub)(LIBRARY_STATE arg_result, arg_self, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_self_guard.get(), arg_other_guard.get(),
              arg_self, arg_other,
              "self", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(csub)(LIBRARY_STATE arg_result, arg_self, AS_REAL(1), arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_other = arg_other_save;
      
      
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.sub", 3, "(" THPTensorStr " source, " RealStr " value, #" THPTensorStr " out)", "(" THPTensorStr " source, " THPTensorStr " other, #" THPTensorStr " out)", "(" THPTensorStr " source, " RealStr " value, " THPTensorStr " other, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_(sub_)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_value = NULL;
    PyObject *__kw_other = NULL;
    if (kwargs) {
      __kw_value = PyDict_GetItemString(kwargs, "value");
      __kw_other = PyDict_GetItemString(kwargs, "other");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 2 &&
          (__tuplecount > 0 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value)) &&
          (__tuplecount > 1 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value));
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other))->cdata;
      
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_inplace1(LIBRARY_STATE arg_other_guard.get(), arg_other, arg_self,
              "other", "self", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(csub)(LIBRARY_STATE arg_self, arg_self, arg_value, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)(self);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_other = arg_other_save;
      
      
    
    } else if (__argcount == 1 &&
          (__tuplecount > 0 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(sub)(LIBRARY_STATE arg_self, arg_self, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)(self);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 1 &&
          (__tuplecount > 0 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other))->cdata;
      
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_inplace1(LIBRARY_STATE arg_other_guard.get(), arg_other, arg_self,
              "other", "self", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(csub)(LIBRARY_STATE arg_self, arg_self, AS_REAL(1), arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)(self);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_other = arg_other_save;
      
      
    
    }

    THPUtils_invalidArguments(args, kwargs, "sub_", 3, "(" RealStr " value)", "(" THPTensorStr " other)", "(" RealStr " value, " THPTensorStr " other)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif



#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_(mul)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_value = NULL;
    PyObject *__kw_other = NULL;
    if (kwargs) {
      __kw_value = PyDict_GetItemString(kwargs, "value");
      __kw_other = PyDict_GetItemString(kwargs, "other");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(mul)(LIBRARY_STATE arg_result, arg_self, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_self_guard.get(), arg_other_guard.get(),
              arg_self, arg_other,
              "self", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cmul)(LIBRARY_STATE arg_result, arg_self, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_other = arg_other_save;
      
      
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(mul)(LIBRARY_STATE arg_result, arg_self, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_self_guard.get(), arg_other_guard.get(),
              arg_self, arg_other,
              "self", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cmul)(LIBRARY_STATE arg_result, arg_self, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_other = arg_other_save;
      
      
    
    }

    THPUtils_invalidArguments(args, kwargs, "mul", 2, "(" RealStr " value, #" THPTensorStr " out)", "(" THPTensorStr " other, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_stateless_(mul)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    PyObject *__kw_value = NULL;
    PyObject *__kw_other = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
      __kw_value = PyDict_GetItemString(kwargs, "value");
      __kw_other = PyDict_GetItemString(kwargs, "other");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(mul)(LIBRARY_STATE arg_result, arg_self, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_self_guard.get(), arg_other_guard.get(),
              arg_self, arg_other,
              "self", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cmul)(LIBRARY_STATE arg_result, arg_self, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_other = arg_other_save;
      
      
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(mul)(LIBRARY_STATE arg_result, arg_self, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_self_guard.get(), arg_other_guard.get(),
              arg_self, arg_other,
              "self", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cmul)(LIBRARY_STATE arg_result, arg_self, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_other = arg_other_save;
      
      
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.mul", 2, "(" THPTensorStr " source, " RealStr " value, #" THPTensorStr " out)", "(" THPTensorStr " source, " THPTensorStr " other, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_(mul_)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_value = NULL;
    PyObject *__kw_other = NULL;
    if (kwargs) {
      __kw_value = PyDict_GetItemString(kwargs, "value");
      __kw_other = PyDict_GetItemString(kwargs, "other");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 1 &&
          (__tuplecount > 0 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(mul)(LIBRARY_STATE arg_self, arg_self, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)(self);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 1 &&
          (__tuplecount > 0 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other))->cdata;
      
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_inplace1(LIBRARY_STATE arg_other_guard.get(), arg_other, arg_self,
              "other", "self", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cmul)(LIBRARY_STATE arg_self, arg_self, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)(self);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_other = arg_other_save;
      
      
    
    }

    THPUtils_invalidArguments(args, kwargs, "mul_", 2, "(" RealStr " value)", "(" THPTensorStr " other)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif



#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_(div)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_value = NULL;
    PyObject *__kw_other = NULL;
    if (kwargs) {
      __kw_value = PyDict_GetItemString(kwargs, "value");
      __kw_other = PyDict_GetItemString(kwargs, "other");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(div)(LIBRARY_STATE arg_result, arg_self, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_self_guard.get(), arg_other_guard.get(),
              arg_self, arg_other,
              "self", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cdiv)(LIBRARY_STATE arg_result, arg_self, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_other = arg_other_save;
      
      
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(div)(LIBRARY_STATE arg_result, arg_self, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_self_guard.get(), arg_other_guard.get(),
              arg_self, arg_other,
              "self", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cdiv)(LIBRARY_STATE arg_result, arg_self, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_other = arg_other_save;
      
      
    
    }

    THPUtils_invalidArguments(args, kwargs, "div", 2, "(" RealStr " value, #" THPTensorStr " out)", "(" THPTensorStr " other, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_stateless_(div)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    PyObject *__kw_value = NULL;
    PyObject *__kw_other = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
      __kw_value = PyDict_GetItemString(kwargs, "value");
      __kw_other = PyDict_GetItemString(kwargs, "other");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(div)(LIBRARY_STATE arg_result, arg_self, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_self_guard.get(), arg_other_guard.get(),
              arg_self, arg_other,
              "self", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cdiv)(LIBRARY_STATE arg_result, arg_self, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_other = arg_other_save;
      
      
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(div)(LIBRARY_STATE arg_result, arg_self, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_self_guard.get(), arg_other_guard.get(),
              arg_self, arg_other,
              "self", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cdiv)(LIBRARY_STATE arg_result, arg_self, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_other = arg_other_save;
      
      
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.div", 2, "(" THPTensorStr " source, " RealStr " value, #" THPTensorStr " out)", "(" THPTensorStr " source, " THPTensorStr " other, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_(div_)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_value = NULL;
    PyObject *__kw_other = NULL;
    if (kwargs) {
      __kw_value = PyDict_GetItemString(kwargs, "value");
      __kw_other = PyDict_GetItemString(kwargs, "other");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 1 &&
          (__tuplecount > 0 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(div)(LIBRARY_STATE arg_self, arg_self, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)(self);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 1 &&
          (__tuplecount > 0 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other))->cdata;
      
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_inplace1(LIBRARY_STATE arg_other_guard.get(), arg_other, arg_self,
              "other", "self", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cdiv)(LIBRARY_STATE arg_self, arg_self, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)(self);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_other = arg_other_save;
      
      
    
    }

    THPUtils_invalidArguments(args, kwargs, "div_", 2, "(" RealStr " value)", "(" THPTensorStr " other)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif



#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_(fmod)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_value = NULL;
    PyObject *__kw_other = NULL;
    if (kwargs) {
      __kw_value = PyDict_GetItemString(kwargs, "value");
      __kw_other = PyDict_GetItemString(kwargs, "other");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(fmod)(LIBRARY_STATE arg_result, arg_self, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_self_guard.get(), arg_other_guard.get(),
              arg_self, arg_other,
              "self", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cfmod)(LIBRARY_STATE arg_result, arg_self, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_other = arg_other_save;
      
      
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(fmod)(LIBRARY_STATE arg_result, arg_self, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_self_guard.get(), arg_other_guard.get(),
              arg_self, arg_other,
              "self", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cfmod)(LIBRARY_STATE arg_result, arg_self, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_other = arg_other_save;
      
      
    
    }

    THPUtils_invalidArguments(args, kwargs, "fmod", 2, "(" RealStr " value, #" THPTensorStr " out)", "(" THPTensorStr " other, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_stateless_(fmod)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    PyObject *__kw_value = NULL;
    PyObject *__kw_other = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
      __kw_value = PyDict_GetItemString(kwargs, "value");
      __kw_other = PyDict_GetItemString(kwargs, "other");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(fmod)(LIBRARY_STATE arg_result, arg_self, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_self_guard.get(), arg_other_guard.get(),
              arg_self, arg_other,
              "self", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cfmod)(LIBRARY_STATE arg_result, arg_self, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_other = arg_other_save;
      
      
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(fmod)(LIBRARY_STATE arg_result, arg_self, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_self_guard.get(), arg_other_guard.get(),
              arg_self, arg_other,
              "self", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cfmod)(LIBRARY_STATE arg_result, arg_self, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_other = arg_other_save;
      
      
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.fmod", 2, "(" THPTensorStr " source, " RealStr " value, #" THPTensorStr " out)", "(" THPTensorStr " source, " THPTensorStr " other, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_(fmod_)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_value = NULL;
    PyObject *__kw_other = NULL;
    if (kwargs) {
      __kw_value = PyDict_GetItemString(kwargs, "value");
      __kw_other = PyDict_GetItemString(kwargs, "other");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 1 &&
          (__tuplecount > 0 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(fmod)(LIBRARY_STATE arg_self, arg_self, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)(self);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 1 &&
          (__tuplecount > 0 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other))->cdata;
      
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_inplace1(LIBRARY_STATE arg_other_guard.get(), arg_other, arg_self,
              "other", "self", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cfmod)(LIBRARY_STATE arg_self, arg_self, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)(self);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_other = arg_other_save;
      
      
    
    }

    THPUtils_invalidArguments(args, kwargs, "fmod_", 2, "(" RealStr " value)", "(" THPTensorStr " other)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif



#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_(remainder)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_value = NULL;
    PyObject *__kw_other = NULL;
    if (kwargs) {
      __kw_value = PyDict_GetItemString(kwargs, "value");
      __kw_other = PyDict_GetItemString(kwargs, "other");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(remainder)(LIBRARY_STATE arg_result, arg_self, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_self_guard.get(), arg_other_guard.get(),
              arg_self, arg_other,
              "self", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cremainder)(LIBRARY_STATE arg_result, arg_self, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_other = arg_other_save;
      
      
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(remainder)(LIBRARY_STATE arg_result, arg_self, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_self_guard.get(), arg_other_guard.get(),
              arg_self, arg_other,
              "self", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cremainder)(LIBRARY_STATE arg_result, arg_self, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_other = arg_other_save;
      
      
    
    }

    THPUtils_invalidArguments(args, kwargs, "remainder", 2, "(" RealStr " value, #" THPTensorStr " out)", "(" THPTensorStr " other, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_stateless_(remainder)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    PyObject *__kw_value = NULL;
    PyObject *__kw_other = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
      __kw_value = PyDict_GetItemString(kwargs, "value");
      __kw_other = PyDict_GetItemString(kwargs, "other");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(remainder)(LIBRARY_STATE arg_result, arg_self, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_self_guard.get(), arg_other_guard.get(),
              arg_self, arg_other,
              "self", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cremainder)(LIBRARY_STATE arg_result, arg_self, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_other = arg_other_save;
      
      
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(remainder)(LIBRARY_STATE arg_result, arg_self, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_self_guard.get(), arg_other_guard.get(),
              arg_self, arg_other,
              "self", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cremainder)(LIBRARY_STATE arg_result, arg_self, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_other = arg_other_save;
      
      
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.remainder", 2, "(" THPTensorStr " source, " RealStr " value, #" THPTensorStr " out)", "(" THPTensorStr " source, " THPTensorStr " other, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_(remainder_)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_value = NULL;
    PyObject *__kw_other = NULL;
    if (kwargs) {
      __kw_value = PyDict_GetItemString(kwargs, "value");
      __kw_other = PyDict_GetItemString(kwargs, "other");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 1 &&
          (__tuplecount > 0 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(remainder)(LIBRARY_STATE arg_self, arg_self, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)(self);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 1 &&
          (__tuplecount > 0 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other))->cdata;
      
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_inplace1(LIBRARY_STATE arg_other_guard.get(), arg_other, arg_self,
              "other", "self", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cremainder)(LIBRARY_STATE arg_self, arg_self, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)(self);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_other = arg_other_save;
      
      
    
    }

    THPUtils_invalidArguments(args, kwargs, "remainder_", 2, "(" RealStr " value)", "(" THPTensorStr " other)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_(clamp)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_min = NULL;
    PyObject *__kw_max = NULL;
    if (kwargs) {
      __kw_min = PyDict_GetItemString(kwargs, "min");
      __kw_max = PyDict_GetItemString(kwargs, "max");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_min) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_min)) &&
          (__tuplecount > 1 || __kw_max) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_max))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_min = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_min));
      real arg_max = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_max));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(clamp)(LIBRARY_STATE arg_result, arg_self, arg_min, arg_max);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_min) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_min)) &&
          (__tuplecount > 1 || __kw_max) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_max))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_min = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_min));
      real arg_max = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_max));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(clamp)(LIBRARY_STATE arg_result, arg_self, arg_min, arg_max);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          __kw_min && THPUtils_(checkReal)(__kw_min)) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_min = THPUtils_(unpackReal)(__kw_min);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cmaxValue)(LIBRARY_STATE arg_result, arg_self, arg_min);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          __kw_max && THPUtils_(checkReal)(__kw_max)) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_max = THPUtils_(unpackReal)(__kw_max);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cminValue)(LIBRARY_STATE arg_result, arg_self, arg_max);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          __kw_min && THPUtils_(checkReal)(__kw_min)) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_min = THPUtils_(unpackReal)(__kw_min);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cmaxValue)(LIBRARY_STATE arg_result, arg_self, arg_min);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          __kw_max && THPUtils_(checkReal)(__kw_max)) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_max = THPUtils_(unpackReal)(__kw_max);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cminValue)(LIBRARY_STATE arg_result, arg_self, arg_max);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "clamp", 3, "(" RealStr " min, #" THPTensorStr " out)", "(" RealStr " max, #" THPTensorStr " out)", "(" RealStr " min, " RealStr " max, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_stateless_(clamp)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    PyObject *__kw_min = NULL;
    PyObject *__kw_max = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
      __kw_min = PyDict_GetItemString(kwargs, "min");
      __kw_max = PyDict_GetItemString(kwargs, "max");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 4 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_min) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_min)) &&
          (__tuplecount > 2 || __kw_max) && THPUtils_(checkReal)((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_max))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      real arg_min = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_min));
      real arg_max = THPUtils_(unpackReal)((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_max));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(clamp)(LIBRARY_STATE arg_result, arg_self, arg_min, arg_max);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 3 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_min) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_min)) &&
          (__tuplecount > 2 || __kw_max) && THPUtils_(checkReal)((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_max))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      real arg_min = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_min));
      real arg_max = THPUtils_(unpackReal)((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_max));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(clamp)(LIBRARY_STATE arg_result, arg_self, arg_min, arg_max);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          __kw_min && THPUtils_(checkReal)(__kw_min)) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      real arg_min = THPUtils_(unpackReal)(__kw_min);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cmaxValue)(LIBRARY_STATE arg_result, arg_self, arg_min);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          __kw_max && THPUtils_(checkReal)(__kw_max)) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      real arg_max = THPUtils_(unpackReal)(__kw_max);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cminValue)(LIBRARY_STATE arg_result, arg_self, arg_max);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          __kw_min && THPUtils_(checkReal)(__kw_min)) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      real arg_min = THPUtils_(unpackReal)(__kw_min);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cmaxValue)(LIBRARY_STATE arg_result, arg_self, arg_min);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          __kw_max && THPUtils_(checkReal)(__kw_max)) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      real arg_max = THPUtils_(unpackReal)(__kw_max);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cminValue)(LIBRARY_STATE arg_result, arg_self, arg_max);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.clamp", 3, "(" THPTensorStr " source, " RealStr " min, #" THPTensorStr " out)", "(" THPTensorStr " source, " RealStr " max, #" THPTensorStr " out)", "(" THPTensorStr " source, " RealStr " min, " RealStr " max, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_(clamp_)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_min = NULL;
    PyObject *__kw_max = NULL;
    if (kwargs) {
      __kw_min = PyDict_GetItemString(kwargs, "min");
      __kw_max = PyDict_GetItemString(kwargs, "max");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 2 &&
          (__tuplecount > 0 || __kw_min) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_min)) &&
          (__tuplecount > 1 || __kw_max) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_max))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_min = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_min));
      real arg_max = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_max));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(clamp)(LIBRARY_STATE arg_self, arg_self, arg_min, arg_max);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 1 &&
          __kw_min && THPUtils_(checkReal)(__kw_min)) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_min = THPUtils_(unpackReal)(__kw_min);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cmaxValue)(LIBRARY_STATE arg_self, arg_self, arg_min);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 1 &&
          __kw_max && THPUtils_(checkReal)(__kw_max)) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_max = THPUtils_(unpackReal)(__kw_max);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cminValue)(LIBRARY_STATE arg_self, arg_self, arg_max);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "clamp_", 3, "(" RealStr " min)", "(" RealStr " max)", "(" RealStr " min, " RealStr " max)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF) && (CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF || !IS_CUDA)
PyObject * THPTensor_(dot)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_tensor = NULL;
    if (kwargs) {
      __kw_tensor = PyDict_GetItemString(kwargs, "tensor");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 1 &&
          (__tuplecount > 0 || __kw_tensor) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_tensor = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor))->cdata;
      
      if(THTensor_(nDimension)(LIBRARY_STATE arg_self) != 1) {
        THError("Expected argument %s to have %d dimension(s), but has %d",
            "self", 1, THTensor_(nDimension)(LIBRARY_STATE arg_self));
      }
      
      if(THTensor_(nDimension)(LIBRARY_STATE arg_tensor) != 1) {
        THError("Expected argument %s to have %d dimension(s), but has %d",
            "tensor", 1, THTensor_(nDimension)(LIBRARY_STATE arg_tensor));
      }
      
      if(THTensor_(nDimension)(LIBRARY_STATE arg_self) != 1) {
        THError("Expected argument %s to have %d dimension(s), but has %d",
            "self", 1, THTensor_(nDimension)(LIBRARY_STATE arg_self));
      }
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        accreal __result = THTensor_(dot)(LIBRARY_STATE arg_self, arg_tensor);
        Py_BLOCK_THREADS;
        return THPUtils_(newAccreal)(__result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "dot", 1, "(" THPTensorStr " tensor)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF) && (CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF || !IS_CUDA)
PyObject * THPTensor_stateless_(dot)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    PyObject *__kw_tensor = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
      __kw_tensor = PyDict_GetItemString(kwargs, "tensor");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 2 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_tensor) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_tensor)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_tensor = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_tensor))->cdata;
      
      if(THTensor_(nDimension)(LIBRARY_STATE arg_self) != 1) {
        THError("Expected argument %s to have %d dimension(s), but has %d",
            "self", 1, THTensor_(nDimension)(LIBRARY_STATE arg_self));
      }
      
      if(THTensor_(nDimension)(LIBRARY_STATE arg_tensor) != 1) {
        THError("Expected argument %s to have %d dimension(s), but has %d",
            "tensor", 1, THTensor_(nDimension)(LIBRARY_STATE arg_tensor));
      }
      
      if(THTensor_(nDimension)(LIBRARY_STATE arg_self) != 1) {
        THError("Expected argument %s to have %d dimension(s), but has %d",
            "self", 1, THTensor_(nDimension)(LIBRARY_STATE arg_self));
      }
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        accreal __result = THTensor_(dot)(LIBRARY_STATE arg_self, arg_tensor);
        Py_BLOCK_THREADS;
        return THPUtils_(newAccreal)(__result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.dot", 1, "(" THPTensorStr " source, " THPTensorStr " tensor)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_(tril)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_diagonal = NULL;
    if (kwargs) {
      __kw_diagonal = PyDict_GetItemString(kwargs, "diagonal");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_diagonal) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_diagonal))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_diagonal = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_diagonal));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(tril)(LIBRARY_STATE arg_result, arg_self, arg_diagonal);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_diagonal) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_diagonal))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_diagonal = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_diagonal));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(tril)(LIBRARY_STATE arg_result, arg_self, arg_diagonal);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 1 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(tril)(LIBRARY_STATE arg_result, arg_self, 0);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(tril)(LIBRARY_STATE arg_result, arg_self, 0);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "tril", 2, "(#" THPTensorStr " out)", "(int diagonal, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_stateless_(tril)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    PyObject *__kw_diagonal = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
      __kw_diagonal = PyDict_GetItemString(kwargs, "diagonal");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_diagonal) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_diagonal))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int64_t arg_diagonal = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_diagonal));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(tril)(LIBRARY_STATE arg_result, arg_self, arg_diagonal);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_diagonal) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_diagonal))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int64_t arg_diagonal = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_diagonal));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(tril)(LIBRARY_STATE arg_result, arg_self, arg_diagonal);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(tril)(LIBRARY_STATE arg_result, arg_self, 0);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(tril)(LIBRARY_STATE arg_result, arg_self, 0);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.tril", 2, "(" THPTensorStr " source, #" THPTensorStr " out)", "(" THPTensorStr " source, int diagonal, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_(tril_)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_diagonal = NULL;
    if (kwargs) {
      __kw_diagonal = PyDict_GetItemString(kwargs, "diagonal");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 1 &&
          (__tuplecount > 0 || __kw_diagonal) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_diagonal))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_diagonal = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_diagonal));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(tril)(LIBRARY_STATE arg_self, arg_self, arg_diagonal);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(tril)(LIBRARY_STATE arg_self, arg_self, 0);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "tril_", 2, "no arguments", "(int diagonal)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_(triu)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_diagonal = NULL;
    if (kwargs) {
      __kw_diagonal = PyDict_GetItemString(kwargs, "diagonal");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_diagonal) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_diagonal))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_diagonal = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_diagonal));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(triu)(LIBRARY_STATE arg_result, arg_self, arg_diagonal);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_diagonal) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_diagonal))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_diagonal = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_diagonal));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(triu)(LIBRARY_STATE arg_result, arg_self, arg_diagonal);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 1 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(triu)(LIBRARY_STATE arg_result, arg_self, 0);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(triu)(LIBRARY_STATE arg_result, arg_self, 0);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "triu", 2, "(#" THPTensorStr " out)", "(int diagonal, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_stateless_(triu)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    PyObject *__kw_diagonal = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
      __kw_diagonal = PyDict_GetItemString(kwargs, "diagonal");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_diagonal) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_diagonal))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int64_t arg_diagonal = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_diagonal));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(triu)(LIBRARY_STATE arg_result, arg_self, arg_diagonal);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_diagonal) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_diagonal))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int64_t arg_diagonal = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_diagonal));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(triu)(LIBRARY_STATE arg_result, arg_self, arg_diagonal);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(triu)(LIBRARY_STATE arg_result, arg_self, 0);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(triu)(LIBRARY_STATE arg_result, arg_self, 0);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.triu", 2, "(" THPTensorStr " source, #" THPTensorStr " out)", "(" THPTensorStr " source, int diagonal, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_(triu_)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_diagonal = NULL;
    if (kwargs) {
      __kw_diagonal = PyDict_GetItemString(kwargs, "diagonal");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 1 &&
          (__tuplecount > 0 || __kw_diagonal) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_diagonal))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_diagonal = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_diagonal));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(triu)(LIBRARY_STATE arg_self, arg_self, arg_diagonal);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(triu)(LIBRARY_STATE arg_self, arg_self, 0);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "triu_", 2, "no arguments", "(int diagonal)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_(cross)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_other = NULL;
    PyObject *__kw_dim = NULL;
    if (kwargs) {
      __kw_other = PyDict_GetItemString(kwargs, "other");
      __kw_dim = PyDict_GetItemString(kwargs, "dim");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_dim) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other))->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cross)(LIBRARY_STATE arg_result, arg_self, arg_other, arg_dim);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_dim) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other))->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cross)(LIBRARY_STATE arg_result, arg_self, arg_other, arg_dim);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cross)(LIBRARY_STATE arg_result, arg_self, arg_other, -1);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cross)(LIBRARY_STATE arg_result, arg_self, arg_other, -1);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "cross", 2, "(" THPTensorStr " other, #" THPTensorStr " out)", "(" THPTensorStr " other, int dim, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_stateless_(cross)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    PyObject *__kw_other = NULL;
    PyObject *__kw_dim = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
      __kw_other = PyDict_GetItemString(kwargs, "other");
      __kw_dim = PyDict_GetItemString(kwargs, "dim");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 4 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_dim) && THPUtils_checkLong((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_dim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other))->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_dim));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cross)(LIBRARY_STATE arg_result, arg_self, arg_other, arg_dim);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 3 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_dim) && THPUtils_checkLong((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_dim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other))->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_dim));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cross)(LIBRARY_STATE arg_result, arg_self, arg_other, arg_dim);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cross)(LIBRARY_STATE arg_result, arg_self, arg_other, -1);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cross)(LIBRARY_STATE arg_result, arg_self, arg_other, -1);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.cross", 2, "(" THPTensorStr " source, " THPTensorStr " other, #" THPTensorStr " out)", "(" THPTensorStr " source, " THPTensorStr " other, int dim, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_stateless_(eye)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_n = NULL;
    PyObject *__kw_m = NULL;
    if (kwargs) {
      __kw_n = PyDict_GetItemString(kwargs, "n");
      __kw_m = PyDict_GetItemString(kwargs, "m");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_n) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_n)) &&
          (__tuplecount > 1 || __kw_m) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_m))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      int64_t arg_n = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_n));
      int64_t arg_m = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_m));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(eye)(LIBRARY_STATE arg_result, arg_n, arg_m);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_n) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_n))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      int64_t arg_n = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_n));
      int64_t arg_1 = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_n));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(eye)(LIBRARY_STATE arg_result, arg_n, arg_1);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_n) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_n)) &&
          (__tuplecount > 1 || __kw_m) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_m))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      int64_t arg_n = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_n));
      int64_t arg_m = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_m));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(eye)(LIBRARY_STATE arg_result, arg_n, arg_m);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_n) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_n))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      int64_t arg_n = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_n));
      int64_t arg_1 = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_n));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(eye)(LIBRARY_STATE arg_result, arg_n, arg_1);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.eye", 2, "(int n, #" THPTensorStr " out)", "(int n, int m, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_(diag)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_diagonal = NULL;
    if (kwargs) {
      __kw_diagonal = PyDict_GetItemString(kwargs, "diagonal");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_diagonal) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_diagonal))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_diagonal = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_diagonal));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(diag)(LIBRARY_STATE arg_result, arg_self, arg_diagonal);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_diagonal) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_diagonal))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_diagonal = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_diagonal));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(diag)(LIBRARY_STATE arg_result, arg_self, arg_diagonal);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 1 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(diag)(LIBRARY_STATE arg_result, arg_self, 0);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(diag)(LIBRARY_STATE arg_result, arg_self, 0);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "diag", 2, "(#" THPTensorStr " out)", "(int diagonal, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_stateless_(diag)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    PyObject *__kw_diagonal = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
      __kw_diagonal = PyDict_GetItemString(kwargs, "diagonal");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_diagonal) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_diagonal))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int64_t arg_diagonal = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_diagonal));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(diag)(LIBRARY_STATE arg_result, arg_self, arg_diagonal);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_diagonal) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_diagonal))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int64_t arg_diagonal = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_diagonal));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(diag)(LIBRARY_STATE arg_result, arg_self, arg_diagonal);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(diag)(LIBRARY_STATE arg_result, arg_self, 0);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(diag)(LIBRARY_STATE arg_result, arg_self, 0);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.diag", 2, "(" THPTensorStr " source, #" THPTensorStr " out)", "(" THPTensorStr " source, int diagonal, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_(addmm)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_beta = NULL;
    PyObject *__kw_alpha = NULL;
    PyObject *__kw_mat1 = NULL;
    PyObject *__kw_mat2 = NULL;
    if (kwargs) {
      __kw_beta = PyDict_GetItemString(kwargs, "beta");
      __kw_alpha = PyDict_GetItemString(kwargs, "alpha");
      __kw_mat1 = PyDict_GetItemString(kwargs, "mat1");
      __kw_mat2 = PyDict_GetItemString(kwargs, "mat2");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 5 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_beta) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta)) &&
          (__tuplecount > 1 || __kw_alpha) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_alpha)) &&
          (__tuplecount > 2 || __kw_mat1) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat1)) == THPTensorClass &&
          (__tuplecount > 3 || __kw_mat2) && (PyObject*)Py_TYPE((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_mat2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      real arg_beta = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta));
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_alpha = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_alpha));
      THTensor* arg_mat1 = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat1))->cdata;
      THTensor* arg_mat2 = ((THPTensor*)(__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_mat2))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      
      if(THTensor_(nDimension)(LIBRARY_STATE arg_mat1) <= 0) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "mat1", 0 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_mat1));
      }
      int64_t arg_self_dim0_size = THTensor_(size)(LIBRARY_STATE arg_mat1, 0);
      if(THTensor_(nDimension)(LIBRARY_STATE arg_mat2) <= 1) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "mat2", 1 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_mat2));
      }
      int64_t arg_self_dim1_size = THTensor_(size)(LIBRARY_STATE arg_mat2, 1);
      THLongStoragePtr arg_self_storage(
          THLongStorage_newWithSize2(arg_self_dim0_size, arg_self_dim1_size));
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_self_storage->data, arg_self_storage->size);
      if (try_expand) {
        arg_self_guard =
        THTensor_(new)(LIBRARY_STATE_NOARGS);
        
        expand(LIBRARY_STATE arg_self_guard.get(), arg_self, arg_self_storage);
        arg_self = arg_self_guard.get();
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addmm)(LIBRARY_STATE arg_result, arg_beta, arg_self, arg_alpha, arg_mat1, arg_mat2);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      
      
    #if !IS_DISTRIBUTED
          
    } else if (___out != NULL &&
          __argcount == 5 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_beta) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta)) &&
          (__tuplecount > 1 || __kw_alpha) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_alpha)) &&
          (__tuplecount > 2 || __kw_mat1) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat1)) == THSPTensorClass &&
          (__tuplecount > 3 || __kw_mat2) && (PyObject*)Py_TYPE((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_mat2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      real arg_beta = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta));
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_alpha = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_alpha));
      THSTensor* arg_mat1 = ((THSPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat1))->cdata;
      THTensor* arg_mat2 = ((THPTensor*)(__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_mat2))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(spaddmm)(LIBRARY_STATE arg_result, arg_beta, arg_self, arg_alpha, arg_mat1, arg_mat2);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif

    } else if (___out == NULL &&
          __argcount == 4 &&
          (__tuplecount > 0 || __kw_beta) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta)) &&
          (__tuplecount > 1 || __kw_alpha) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_alpha)) &&
          (__tuplecount > 2 || __kw_mat1) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat1)) == THPTensorClass &&
          (__tuplecount > 3 || __kw_mat2) && (PyObject*)Py_TYPE((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_mat2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      real arg_beta = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta));
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_alpha = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_alpha));
      THTensor* arg_mat1 = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat1))->cdata;
      THTensor* arg_mat2 = ((THPTensor*)(__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_mat2))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      
      if(THTensor_(nDimension)(LIBRARY_STATE arg_mat1) <= 0) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "mat1", 0 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_mat1));
      }
      int64_t arg_self_dim0_size = THTensor_(size)(LIBRARY_STATE arg_mat1, 0);
      if(THTensor_(nDimension)(LIBRARY_STATE arg_mat2) <= 1) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "mat2", 1 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_mat2));
      }
      int64_t arg_self_dim1_size = THTensor_(size)(LIBRARY_STATE arg_mat2, 1);
      THLongStoragePtr arg_self_storage(
          THLongStorage_newWithSize2(arg_self_dim0_size, arg_self_dim1_size));
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_self_storage->data, arg_self_storage->size);
      if (try_expand) {
        arg_self_guard =
        THTensor_(new)(LIBRARY_STATE_NOARGS);
        
        expand(LIBRARY_STATE arg_self_guard.get(), arg_self, arg_self_storage);
        arg_self = arg_self_guard.get();
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addmm)(LIBRARY_STATE arg_result, arg_beta, arg_self, arg_alpha, arg_mat1, arg_mat2);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      
      
    
    } else if (___out != NULL &&
          __argcount == 4 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_beta) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta)) &&
          (__tuplecount > 1 || __kw_mat1) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat1)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_mat2) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      real arg_beta = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta));
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_mat1 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat1))->cdata;
      THTensor* arg_mat2 = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat2))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      
      if(THTensor_(nDimension)(LIBRARY_STATE arg_mat1) <= 0) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "mat1", 0 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_mat1));
      }
      int64_t arg_self_dim0_size = THTensor_(size)(LIBRARY_STATE arg_mat1, 0);
      if(THTensor_(nDimension)(LIBRARY_STATE arg_mat2) <= 1) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "mat2", 1 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_mat2));
      }
      int64_t arg_self_dim1_size = THTensor_(size)(LIBRARY_STATE arg_mat2, 1);
      THLongStoragePtr arg_self_storage(
          THLongStorage_newWithSize2(arg_self_dim0_size, arg_self_dim1_size));
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_self_storage->data, arg_self_storage->size);
      if (try_expand) {
        arg_self_guard =
        THTensor_(new)(LIBRARY_STATE_NOARGS);
        
        expand(LIBRARY_STATE arg_self_guard.get(), arg_self, arg_self_storage);
        arg_self = arg_self_guard.get();
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addmm)(LIBRARY_STATE arg_result, arg_beta, arg_self, AS_REAL(1), arg_mat1, arg_mat2);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      
      
    
    } else if (___out != NULL &&
          __argcount == 4 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_alpha) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_alpha)) &&
          (__tuplecount > 1 || __kw_mat1) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat1)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_mat2) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_alpha = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_alpha));
      THTensor* arg_mat1 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat1))->cdata;
      THTensor* arg_mat2 = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat2))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      
      if(THTensor_(nDimension)(LIBRARY_STATE arg_mat1) <= 0) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "mat1", 0 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_mat1));
      }
      int64_t arg_self_dim0_size = THTensor_(size)(LIBRARY_STATE arg_mat1, 0);
      if(THTensor_(nDimension)(LIBRARY_STATE arg_mat2) <= 1) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "mat2", 1 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_mat2));
      }
      int64_t arg_self_dim1_size = THTensor_(size)(LIBRARY_STATE arg_mat2, 1);
      THLongStoragePtr arg_self_storage(
          THLongStorage_newWithSize2(arg_self_dim0_size, arg_self_dim1_size));
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_self_storage->data, arg_self_storage->size);
      if (try_expand) {
        arg_self_guard =
        THTensor_(new)(LIBRARY_STATE_NOARGS);
        
        expand(LIBRARY_STATE arg_self_guard.get(), arg_self, arg_self_storage);
        arg_self = arg_self_guard.get();
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addmm)(LIBRARY_STATE arg_result, AS_REAL(1), arg_self, arg_alpha, arg_mat1, arg_mat2);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      
      
    #if !IS_DISTRIBUTED
          
    } else if (___out == NULL &&
          __argcount == 4 &&
          (__tuplecount > 0 || __kw_beta) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta)) &&
          (__tuplecount > 1 || __kw_alpha) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_alpha)) &&
          (__tuplecount > 2 || __kw_mat1) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat1)) == THSPTensorClass &&
          (__tuplecount > 3 || __kw_mat2) && (PyObject*)Py_TYPE((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_mat2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      real arg_beta = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta));
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_alpha = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_alpha));
      THSTensor* arg_mat1 = ((THSPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat1))->cdata;
      THTensor* arg_mat2 = ((THPTensor*)(__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_mat2))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(spaddmm)(LIBRARY_STATE arg_result, arg_beta, arg_self, arg_alpha, arg_mat1, arg_mat2);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif
#if !IS_DISTRIBUTED
          
    } else if (___out != NULL &&
          __argcount == 4 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_beta) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta)) &&
          (__tuplecount > 1 || __kw_mat1) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat1)) == THSPTensorClass &&
          (__tuplecount > 2 || __kw_mat2) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      real arg_beta = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta));
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THSTensor* arg_mat1 = ((THSPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat1))->cdata;
      THTensor* arg_mat2 = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat2))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(spaddmm)(LIBRARY_STATE arg_result, arg_beta, arg_self, AS_REAL(1), arg_mat1, arg_mat2);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif
#if !IS_DISTRIBUTED
          
    } else if (___out != NULL &&
          __argcount == 4 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_alpha) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_alpha)) &&
          (__tuplecount > 1 || __kw_mat1) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat1)) == THSPTensorClass &&
          (__tuplecount > 2 || __kw_mat2) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_alpha = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_alpha));
      THSTensor* arg_mat1 = ((THSPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat1))->cdata;
      THTensor* arg_mat2 = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat2))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(spaddmm)(LIBRARY_STATE arg_result, AS_REAL(1), arg_self, arg_alpha, arg_mat1, arg_mat2);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif

    } else if (___out == NULL &&
          __argcount == 3 &&
          (__tuplecount > 0 || __kw_beta) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta)) &&
          (__tuplecount > 1 || __kw_mat1) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat1)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_mat2) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      real arg_beta = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta));
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_mat1 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat1))->cdata;
      THTensor* arg_mat2 = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat2))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      
      if(THTensor_(nDimension)(LIBRARY_STATE arg_mat1) <= 0) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "mat1", 0 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_mat1));
      }
      int64_t arg_self_dim0_size = THTensor_(size)(LIBRARY_STATE arg_mat1, 0);
      if(THTensor_(nDimension)(LIBRARY_STATE arg_mat2) <= 1) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "mat2", 1 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_mat2));
      }
      int64_t arg_self_dim1_size = THTensor_(size)(LIBRARY_STATE arg_mat2, 1);
      THLongStoragePtr arg_self_storage(
          THLongStorage_newWithSize2(arg_self_dim0_size, arg_self_dim1_size));
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_self_storage->data, arg_self_storage->size);
      if (try_expand) {
        arg_self_guard =
        THTensor_(new)(LIBRARY_STATE_NOARGS);
        
        expand(LIBRARY_STATE arg_self_guard.get(), arg_self, arg_self_storage);
        arg_self = arg_self_guard.get();
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addmm)(LIBRARY_STATE arg_result, arg_beta, arg_self, AS_REAL(1), arg_mat1, arg_mat2);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      
      
    
    } else if (___out == NULL &&
          __argcount == 3 &&
          (__tuplecount > 0 || __kw_alpha) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_alpha)) &&
          (__tuplecount > 1 || __kw_mat1) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat1)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_mat2) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_alpha = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_alpha));
      THTensor* arg_mat1 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat1))->cdata;
      THTensor* arg_mat2 = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat2))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      
      if(THTensor_(nDimension)(LIBRARY_STATE arg_mat1) <= 0) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "mat1", 0 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_mat1));
      }
      int64_t arg_self_dim0_size = THTensor_(size)(LIBRARY_STATE arg_mat1, 0);
      if(THTensor_(nDimension)(LIBRARY_STATE arg_mat2) <= 1) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "mat2", 1 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_mat2));
      }
      int64_t arg_self_dim1_size = THTensor_(size)(LIBRARY_STATE arg_mat2, 1);
      THLongStoragePtr arg_self_storage(
          THLongStorage_newWithSize2(arg_self_dim0_size, arg_self_dim1_size));
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_self_storage->data, arg_self_storage->size);
      if (try_expand) {
        arg_self_guard =
        THTensor_(new)(LIBRARY_STATE_NOARGS);
        
        expand(LIBRARY_STATE arg_self_guard.get(), arg_self, arg_self_storage);
        arg_self = arg_self_guard.get();
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addmm)(LIBRARY_STATE arg_result, AS_REAL(1), arg_self, arg_alpha, arg_mat1, arg_mat2);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      
      
    
    } else if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_mat1) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mat1)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_mat2) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_mat1 = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mat1))->cdata;
      THTensor* arg_mat2 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat2))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      
      if(THTensor_(nDimension)(LIBRARY_STATE arg_mat1) <= 0) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "mat1", 0 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_mat1));
      }
      int64_t arg_self_dim0_size = THTensor_(size)(LIBRARY_STATE arg_mat1, 0);
      if(THTensor_(nDimension)(LIBRARY_STATE arg_mat2) <= 1) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "mat2", 1 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_mat2));
      }
      int64_t arg_self_dim1_size = THTensor_(size)(LIBRARY_STATE arg_mat2, 1);
      THLongStoragePtr arg_self_storage(
          THLongStorage_newWithSize2(arg_self_dim0_size, arg_self_dim1_size));
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_self_storage->data, arg_self_storage->size);
      if (try_expand) {
        arg_self_guard =
        THTensor_(new)(LIBRARY_STATE_NOARGS);
        
        expand(LIBRARY_STATE arg_self_guard.get(), arg_self, arg_self_storage);
        arg_self = arg_self_guard.get();
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addmm)(LIBRARY_STATE arg_result, AS_REAL(1), arg_self, AS_REAL(1), arg_mat1, arg_mat2);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      
      
    #if !IS_DISTRIBUTED
          
    } else if (___out == NULL &&
          __argcount == 3 &&
          (__tuplecount > 0 || __kw_beta) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta)) &&
          (__tuplecount > 1 || __kw_mat1) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat1)) == THSPTensorClass &&
          (__tuplecount > 2 || __kw_mat2) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      real arg_beta = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta));
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THSTensor* arg_mat1 = ((THSPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat1))->cdata;
      THTensor* arg_mat2 = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat2))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(spaddmm)(LIBRARY_STATE arg_result, arg_beta, arg_self, AS_REAL(1), arg_mat1, arg_mat2);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif
#if !IS_DISTRIBUTED
          
    } else if (___out == NULL &&
          __argcount == 3 &&
          (__tuplecount > 0 || __kw_alpha) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_alpha)) &&
          (__tuplecount > 1 || __kw_mat1) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat1)) == THSPTensorClass &&
          (__tuplecount > 2 || __kw_mat2) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_alpha = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_alpha));
      THSTensor* arg_mat1 = ((THSPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat1))->cdata;
      THTensor* arg_mat2 = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat2))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(spaddmm)(LIBRARY_STATE arg_result, AS_REAL(1), arg_self, arg_alpha, arg_mat1, arg_mat2);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif
#if !IS_DISTRIBUTED
          
    } else if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_mat1) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mat1)) == THSPTensorClass &&
          (__tuplecount > 1 || __kw_mat2) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THSTensor* arg_mat1 = ((THSPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mat1))->cdata;
      THTensor* arg_mat2 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat2))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(spaddmm)(LIBRARY_STATE arg_result, AS_REAL(1), arg_self, AS_REAL(1), arg_mat1, arg_mat2);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif

    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_mat1) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mat1)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_mat2) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_mat1 = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mat1))->cdata;
      THTensor* arg_mat2 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat2))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      
      if(THTensor_(nDimension)(LIBRARY_STATE arg_mat1) <= 0) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "mat1", 0 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_mat1));
      }
      int64_t arg_self_dim0_size = THTensor_(size)(LIBRARY_STATE arg_mat1, 0);
      if(THTensor_(nDimension)(LIBRARY_STATE arg_mat2) <= 1) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "mat2", 1 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_mat2));
      }
      int64_t arg_self_dim1_size = THTensor_(size)(LIBRARY_STATE arg_mat2, 1);
      THLongStoragePtr arg_self_storage(
          THLongStorage_newWithSize2(arg_self_dim0_size, arg_self_dim1_size));
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_self_storage->data, arg_self_storage->size);
      if (try_expand) {
        arg_self_guard =
        THTensor_(new)(LIBRARY_STATE_NOARGS);
        
        expand(LIBRARY_STATE arg_self_guard.get(), arg_self, arg_self_storage);
        arg_self = arg_self_guard.get();
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addmm)(LIBRARY_STATE arg_result, AS_REAL(1), arg_self, AS_REAL(1), arg_mat1, arg_mat2);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      
      
    #if !IS_DISTRIBUTED
          
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_mat1) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mat1)) == THSPTensorClass &&
          (__tuplecount > 1 || __kw_mat2) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THSTensor* arg_mat1 = ((THSPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mat1))->cdata;
      THTensor* arg_mat2 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat2))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(spaddmm)(LIBRARY_STATE arg_result, AS_REAL(1), arg_self, AS_REAL(1), arg_mat1, arg_mat2);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif

    }

    THPUtils_invalidArguments(args, kwargs, "addmm", 8, "(" THPTensorStr " mat1, " THPTensorStr " mat2, #" THPTensorStr " out)", "(" THSPTensorStr " mat1, " THPTensorStr " mat2, #" THPTensorStr " out)", "(" RealStr " beta, " THPTensorStr " mat1, " THPTensorStr " mat2, #" THPTensorStr " out)", "(" RealStr " alpha, " THPTensorStr " mat1, " THPTensorStr " mat2, #" THPTensorStr " out)", "(" RealStr " beta, " THSPTensorStr " mat1, " THPTensorStr " mat2, #" THPTensorStr " out)", "(" RealStr " alpha, " THSPTensorStr " mat1, " THPTensorStr " mat2, #" THPTensorStr " out)", "(" RealStr " beta, " RealStr " alpha, " THPTensorStr " mat1, " THPTensorStr " mat2, #" THPTensorStr " out)", "(" RealStr " beta, " RealStr " alpha, " THSPTensorStr " mat1, " THPTensorStr " mat2, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_stateless_(addmm)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_beta = NULL;
    PyObject *__kw_source = NULL;
    PyObject *__kw_alpha = NULL;
    PyObject *__kw_mat1 = NULL;
    PyObject *__kw_mat2 = NULL;
    if (kwargs) {
      __kw_beta = PyDict_GetItemString(kwargs, "beta");
      __kw_source = PyDict_GetItemString(kwargs, "source");
      __kw_alpha = PyDict_GetItemString(kwargs, "alpha");
      __kw_mat1 = PyDict_GetItemString(kwargs, "mat1");
      __kw_mat2 = PyDict_GetItemString(kwargs, "mat2");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 6 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_beta) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta)) &&
          (__tuplecount > 1 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_alpha) && THPUtils_(checkReal)((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_alpha)) &&
          (__tuplecount > 3 || __kw_mat1) && (PyObject*)Py_TYPE((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_mat1)) == THPTensorClass &&
          (__tuplecount > 4 || __kw_mat2) && (PyObject*)Py_TYPE((__tuplecount > 4 ? PyTuple_GET_ITEM(args, 4) : __kw_mat2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      real arg_beta = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta));
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_source))->cdata;
      real arg_alpha = THPUtils_(unpackReal)((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_alpha));
      THTensor* arg_mat1 = ((THPTensor*)(__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_mat1))->cdata;
      THTensor* arg_mat2 = ((THPTensor*)(__tuplecount > 4 ? PyTuple_GET_ITEM(args, 4) : __kw_mat2))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      
      if(THTensor_(nDimension)(LIBRARY_STATE arg_mat1) <= 0) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "mat1", 0 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_mat1));
      }
      int64_t arg_self_dim0_size = THTensor_(size)(LIBRARY_STATE arg_mat1, 0);
      if(THTensor_(nDimension)(LIBRARY_STATE arg_mat2) <= 1) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "mat2", 1 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_mat2));
      }
      int64_t arg_self_dim1_size = THTensor_(size)(LIBRARY_STATE arg_mat2, 1);
      THLongStoragePtr arg_self_storage(
          THLongStorage_newWithSize2(arg_self_dim0_size, arg_self_dim1_size));
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_self_storage->data, arg_self_storage->size);
      if (try_expand) {
        arg_self_guard =
        THTensor_(new)(LIBRARY_STATE_NOARGS);
        
        expand(LIBRARY_STATE arg_self_guard.get(), arg_self, arg_self_storage);
        arg_self = arg_self_guard.get();
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addmm)(LIBRARY_STATE arg_result, arg_beta, arg_self, arg_alpha, arg_mat1, arg_mat2);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      
      
    #if !IS_DISTRIBUTED
          
    } else if (___out != NULL &&
          __argcount == 6 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_beta) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta)) &&
          (__tuplecount > 1 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_alpha) && THPUtils_(checkReal)((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_alpha)) &&
          (__tuplecount > 3 || __kw_mat1) && (PyObject*)Py_TYPE((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_mat1)) == THSPTensorClass &&
          (__tuplecount > 4 || __kw_mat2) && (PyObject*)Py_TYPE((__tuplecount > 4 ? PyTuple_GET_ITEM(args, 4) : __kw_mat2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      real arg_beta = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta));
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_source))->cdata;
      real arg_alpha = THPUtils_(unpackReal)((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_alpha));
      THSTensor* arg_mat1 = ((THSPTensor*)(__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_mat1))->cdata;
      THTensor* arg_mat2 = ((THPTensor*)(__tuplecount > 4 ? PyTuple_GET_ITEM(args, 4) : __kw_mat2))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(spaddmm)(LIBRARY_STATE arg_result, arg_beta, arg_self, arg_alpha, arg_mat1, arg_mat2);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif

    } else if (___out == NULL &&
          __argcount == 5 &&
          (__tuplecount > 0 || __kw_beta) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta)) &&
          (__tuplecount > 1 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_alpha) && THPUtils_(checkReal)((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_alpha)) &&
          (__tuplecount > 3 || __kw_mat1) && (PyObject*)Py_TYPE((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_mat1)) == THPTensorClass &&
          (__tuplecount > 4 || __kw_mat2) && (PyObject*)Py_TYPE((__tuplecount > 4 ? PyTuple_GET_ITEM(args, 4) : __kw_mat2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      real arg_beta = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta));
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_source))->cdata;
      real arg_alpha = THPUtils_(unpackReal)((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_alpha));
      THTensor* arg_mat1 = ((THPTensor*)(__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_mat1))->cdata;
      THTensor* arg_mat2 = ((THPTensor*)(__tuplecount > 4 ? PyTuple_GET_ITEM(args, 4) : __kw_mat2))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      
      if(THTensor_(nDimension)(LIBRARY_STATE arg_mat1) <= 0) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "mat1", 0 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_mat1));
      }
      int64_t arg_self_dim0_size = THTensor_(size)(LIBRARY_STATE arg_mat1, 0);
      if(THTensor_(nDimension)(LIBRARY_STATE arg_mat2) <= 1) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "mat2", 1 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_mat2));
      }
      int64_t arg_self_dim1_size = THTensor_(size)(LIBRARY_STATE arg_mat2, 1);
      THLongStoragePtr arg_self_storage(
          THLongStorage_newWithSize2(arg_self_dim0_size, arg_self_dim1_size));
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_self_storage->data, arg_self_storage->size);
      if (try_expand) {
        arg_self_guard =
        THTensor_(new)(LIBRARY_STATE_NOARGS);
        
        expand(LIBRARY_STATE arg_self_guard.get(), arg_self, arg_self_storage);
        arg_self = arg_self_guard.get();
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addmm)(LIBRARY_STATE arg_result, arg_beta, arg_self, arg_alpha, arg_mat1, arg_mat2);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      
      
    
    } else if (___out != NULL &&
          __argcount == 5 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_beta) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta)) &&
          (__tuplecount > 1 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_mat1) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat1)) == THPTensorClass &&
          (__tuplecount > 3 || __kw_mat2) && (PyObject*)Py_TYPE((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_mat2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      real arg_beta = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta));
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_source))->cdata;
      THTensor* arg_mat1 = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat1))->cdata;
      THTensor* arg_mat2 = ((THPTensor*)(__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_mat2))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      
      if(THTensor_(nDimension)(LIBRARY_STATE arg_mat1) <= 0) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "mat1", 0 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_mat1));
      }
      int64_t arg_self_dim0_size = THTensor_(size)(LIBRARY_STATE arg_mat1, 0);
      if(THTensor_(nDimension)(LIBRARY_STATE arg_mat2) <= 1) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "mat2", 1 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_mat2));
      }
      int64_t arg_self_dim1_size = THTensor_(size)(LIBRARY_STATE arg_mat2, 1);
      THLongStoragePtr arg_self_storage(
          THLongStorage_newWithSize2(arg_self_dim0_size, arg_self_dim1_size));
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_self_storage->data, arg_self_storage->size);
      if (try_expand) {
        arg_self_guard =
        THTensor_(new)(LIBRARY_STATE_NOARGS);
        
        expand(LIBRARY_STATE arg_self_guard.get(), arg_self, arg_self_storage);
        arg_self = arg_self_guard.get();
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addmm)(LIBRARY_STATE arg_result, arg_beta, arg_self, AS_REAL(1), arg_mat1, arg_mat2);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      
      
    
    } else if (___out != NULL &&
          __argcount == 5 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_alpha) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_alpha)) &&
          (__tuplecount > 2 || __kw_mat1) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat1)) == THPTensorClass &&
          (__tuplecount > 3 || __kw_mat2) && (PyObject*)Py_TYPE((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_mat2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      real arg_alpha = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_alpha));
      THTensor* arg_mat1 = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat1))->cdata;
      THTensor* arg_mat2 = ((THPTensor*)(__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_mat2))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      
      if(THTensor_(nDimension)(LIBRARY_STATE arg_mat1) <= 0) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "mat1", 0 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_mat1));
      }
      int64_t arg_self_dim0_size = THTensor_(size)(LIBRARY_STATE arg_mat1, 0);
      if(THTensor_(nDimension)(LIBRARY_STATE arg_mat2) <= 1) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "mat2", 1 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_mat2));
      }
      int64_t arg_self_dim1_size = THTensor_(size)(LIBRARY_STATE arg_mat2, 1);
      THLongStoragePtr arg_self_storage(
          THLongStorage_newWithSize2(arg_self_dim0_size, arg_self_dim1_size));
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_self_storage->data, arg_self_storage->size);
      if (try_expand) {
        arg_self_guard =
        THTensor_(new)(LIBRARY_STATE_NOARGS);
        
        expand(LIBRARY_STATE arg_self_guard.get(), arg_self, arg_self_storage);
        arg_self = arg_self_guard.get();
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addmm)(LIBRARY_STATE arg_result, AS_REAL(1), arg_self, arg_alpha, arg_mat1, arg_mat2);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      
      
    #if !IS_DISTRIBUTED
          
    } else if (___out == NULL &&
          __argcount == 5 &&
          (__tuplecount > 0 || __kw_beta) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta)) &&
          (__tuplecount > 1 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_alpha) && THPUtils_(checkReal)((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_alpha)) &&
          (__tuplecount > 3 || __kw_mat1) && (PyObject*)Py_TYPE((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_mat1)) == THSPTensorClass &&
          (__tuplecount > 4 || __kw_mat2) && (PyObject*)Py_TYPE((__tuplecount > 4 ? PyTuple_GET_ITEM(args, 4) : __kw_mat2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      real arg_beta = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta));
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_source))->cdata;
      real arg_alpha = THPUtils_(unpackReal)((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_alpha));
      THSTensor* arg_mat1 = ((THSPTensor*)(__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_mat1))->cdata;
      THTensor* arg_mat2 = ((THPTensor*)(__tuplecount > 4 ? PyTuple_GET_ITEM(args, 4) : __kw_mat2))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(spaddmm)(LIBRARY_STATE arg_result, arg_beta, arg_self, arg_alpha, arg_mat1, arg_mat2);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif
#if !IS_DISTRIBUTED
          
    } else if (___out != NULL &&
          __argcount == 5 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_beta) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta)) &&
          (__tuplecount > 1 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_mat1) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat1)) == THSPTensorClass &&
          (__tuplecount > 3 || __kw_mat2) && (PyObject*)Py_TYPE((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_mat2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      real arg_beta = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta));
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_source))->cdata;
      THSTensor* arg_mat1 = ((THSPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat1))->cdata;
      THTensor* arg_mat2 = ((THPTensor*)(__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_mat2))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(spaddmm)(LIBRARY_STATE arg_result, arg_beta, arg_self, AS_REAL(1), arg_mat1, arg_mat2);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif
#if !IS_DISTRIBUTED
          
    } else if (___out != NULL &&
          __argcount == 5 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_alpha) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_alpha)) &&
          (__tuplecount > 2 || __kw_mat1) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat1)) == THSPTensorClass &&
          (__tuplecount > 3 || __kw_mat2) && (PyObject*)Py_TYPE((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_mat2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      real arg_alpha = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_alpha));
      THSTensor* arg_mat1 = ((THSPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat1))->cdata;
      THTensor* arg_mat2 = ((THPTensor*)(__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_mat2))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(spaddmm)(LIBRARY_STATE arg_result, AS_REAL(1), arg_self, arg_alpha, arg_mat1, arg_mat2);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif

    } else if (___out == NULL &&
          __argcount == 4 &&
          (__tuplecount > 0 || __kw_beta) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta)) &&
          (__tuplecount > 1 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_mat1) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat1)) == THPTensorClass &&
          (__tuplecount > 3 || __kw_mat2) && (PyObject*)Py_TYPE((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_mat2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      real arg_beta = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta));
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_source))->cdata;
      THTensor* arg_mat1 = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat1))->cdata;
      THTensor* arg_mat2 = ((THPTensor*)(__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_mat2))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      
      if(THTensor_(nDimension)(LIBRARY_STATE arg_mat1) <= 0) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "mat1", 0 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_mat1));
      }
      int64_t arg_self_dim0_size = THTensor_(size)(LIBRARY_STATE arg_mat1, 0);
      if(THTensor_(nDimension)(LIBRARY_STATE arg_mat2) <= 1) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "mat2", 1 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_mat2));
      }
      int64_t arg_self_dim1_size = THTensor_(size)(LIBRARY_STATE arg_mat2, 1);
      THLongStoragePtr arg_self_storage(
          THLongStorage_newWithSize2(arg_self_dim0_size, arg_self_dim1_size));
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_self_storage->data, arg_self_storage->size);
      if (try_expand) {
        arg_self_guard =
        THTensor_(new)(LIBRARY_STATE_NOARGS);
        
        expand(LIBRARY_STATE arg_self_guard.get(), arg_self, arg_self_storage);
        arg_self = arg_self_guard.get();
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addmm)(LIBRARY_STATE arg_result, arg_beta, arg_self, AS_REAL(1), arg_mat1, arg_mat2);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      
      
    
    } else if (___out == NULL &&
          __argcount == 4 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_alpha) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_alpha)) &&
          (__tuplecount > 2 || __kw_mat1) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat1)) == THPTensorClass &&
          (__tuplecount > 3 || __kw_mat2) && (PyObject*)Py_TYPE((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_mat2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      real arg_alpha = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_alpha));
      THTensor* arg_mat1 = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat1))->cdata;
      THTensor* arg_mat2 = ((THPTensor*)(__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_mat2))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      
      if(THTensor_(nDimension)(LIBRARY_STATE arg_mat1) <= 0) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "mat1", 0 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_mat1));
      }
      int64_t arg_self_dim0_size = THTensor_(size)(LIBRARY_STATE arg_mat1, 0);
      if(THTensor_(nDimension)(LIBRARY_STATE arg_mat2) <= 1) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "mat2", 1 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_mat2));
      }
      int64_t arg_self_dim1_size = THTensor_(size)(LIBRARY_STATE arg_mat2, 1);
      THLongStoragePtr arg_self_storage(
          THLongStorage_newWithSize2(arg_self_dim0_size, arg_self_dim1_size));
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_self_storage->data, arg_self_storage->size);
      if (try_expand) {
        arg_self_guard =
        THTensor_(new)(LIBRARY_STATE_NOARGS);
        
        expand(LIBRARY_STATE arg_self_guard.get(), arg_self, arg_self_storage);
        arg_self = arg_self_guard.get();
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addmm)(LIBRARY_STATE arg_result, AS_REAL(1), arg_self, arg_alpha, arg_mat1, arg_mat2);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      
      
    
    } else if (___out != NULL &&
          __argcount == 4 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_mat1) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat1)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_mat2) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_mat1 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat1))->cdata;
      THTensor* arg_mat2 = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat2))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      
      if(THTensor_(nDimension)(LIBRARY_STATE arg_mat1) <= 0) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "mat1", 0 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_mat1));
      }
      int64_t arg_self_dim0_size = THTensor_(size)(LIBRARY_STATE arg_mat1, 0);
      if(THTensor_(nDimension)(LIBRARY_STATE arg_mat2) <= 1) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "mat2", 1 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_mat2));
      }
      int64_t arg_self_dim1_size = THTensor_(size)(LIBRARY_STATE arg_mat2, 1);
      THLongStoragePtr arg_self_storage(
          THLongStorage_newWithSize2(arg_self_dim0_size, arg_self_dim1_size));
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_self_storage->data, arg_self_storage->size);
      if (try_expand) {
        arg_self_guard =
        THTensor_(new)(LIBRARY_STATE_NOARGS);
        
        expand(LIBRARY_STATE arg_self_guard.get(), arg_self, arg_self_storage);
        arg_self = arg_self_guard.get();
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addmm)(LIBRARY_STATE arg_result, AS_REAL(1), arg_self, AS_REAL(1), arg_mat1, arg_mat2);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      
      
    #if !IS_DISTRIBUTED
          
    } else if (___out == NULL &&
          __argcount == 4 &&
          (__tuplecount > 0 || __kw_beta) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta)) &&
          (__tuplecount > 1 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_mat1) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat1)) == THSPTensorClass &&
          (__tuplecount > 3 || __kw_mat2) && (PyObject*)Py_TYPE((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_mat2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      real arg_beta = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta));
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_source))->cdata;
      THSTensor* arg_mat1 = ((THSPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat1))->cdata;
      THTensor* arg_mat2 = ((THPTensor*)(__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_mat2))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(spaddmm)(LIBRARY_STATE arg_result, arg_beta, arg_self, AS_REAL(1), arg_mat1, arg_mat2);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif
#if !IS_DISTRIBUTED
          
    } else if (___out == NULL &&
          __argcount == 4 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_alpha) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_alpha)) &&
          (__tuplecount > 2 || __kw_mat1) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat1)) == THSPTensorClass &&
          (__tuplecount > 3 || __kw_mat2) && (PyObject*)Py_TYPE((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_mat2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      real arg_alpha = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_alpha));
      THSTensor* arg_mat1 = ((THSPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat1))->cdata;
      THTensor* arg_mat2 = ((THPTensor*)(__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_mat2))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(spaddmm)(LIBRARY_STATE arg_result, AS_REAL(1), arg_self, arg_alpha, arg_mat1, arg_mat2);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif
#if !IS_DISTRIBUTED
          
    } else if (___out != NULL &&
          __argcount == 4 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_mat1) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat1)) == THSPTensorClass &&
          (__tuplecount > 2 || __kw_mat2) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THSTensor* arg_mat1 = ((THSPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat1))->cdata;
      THTensor* arg_mat2 = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat2))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(spaddmm)(LIBRARY_STATE arg_result, AS_REAL(1), arg_self, AS_REAL(1), arg_mat1, arg_mat2);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif

    } else if (___out == NULL &&
          __argcount == 3 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_mat1) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat1)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_mat2) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_mat1 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat1))->cdata;
      THTensor* arg_mat2 = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat2))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      
      if(THTensor_(nDimension)(LIBRARY_STATE arg_mat1) <= 0) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "mat1", 0 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_mat1));
      }
      int64_t arg_self_dim0_size = THTensor_(size)(LIBRARY_STATE arg_mat1, 0);
      if(THTensor_(nDimension)(LIBRARY_STATE arg_mat2) <= 1) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "mat2", 1 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_mat2));
      }
      int64_t arg_self_dim1_size = THTensor_(size)(LIBRARY_STATE arg_mat2, 1);
      THLongStoragePtr arg_self_storage(
          THLongStorage_newWithSize2(arg_self_dim0_size, arg_self_dim1_size));
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_self_storage->data, arg_self_storage->size);
      if (try_expand) {
        arg_self_guard =
        THTensor_(new)(LIBRARY_STATE_NOARGS);
        
        expand(LIBRARY_STATE arg_self_guard.get(), arg_self, arg_self_storage);
        arg_self = arg_self_guard.get();
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addmm)(LIBRARY_STATE arg_result, AS_REAL(1), arg_self, AS_REAL(1), arg_mat1, arg_mat2);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      
      
    #if !IS_DISTRIBUTED
          
    } else if (___out == NULL &&
          __argcount == 3 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_mat1) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat1)) == THSPTensorClass &&
          (__tuplecount > 2 || __kw_mat2) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THSTensor* arg_mat1 = ((THSPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat1))->cdata;
      THTensor* arg_mat2 = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat2))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(spaddmm)(LIBRARY_STATE arg_result, AS_REAL(1), arg_self, AS_REAL(1), arg_mat1, arg_mat2);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif

    }

    THPUtils_invalidArguments(args, kwargs, "torch.addmm", 8, "(" THPTensorStr " source, " THPTensorStr " mat1, " THPTensorStr " mat2, #" THPTensorStr " out)", "(" THPTensorStr " source, " THSPTensorStr " mat1, " THPTensorStr " mat2, #" THPTensorStr " out)", "(" RealStr " beta, " THPTensorStr " source, " THPTensorStr " mat1, " THPTensorStr " mat2, #" THPTensorStr " out)", "(" THPTensorStr " source, " RealStr " alpha, " THPTensorStr " mat1, " THPTensorStr " mat2, #" THPTensorStr " out)", "(" RealStr " beta, " THPTensorStr " source, " THSPTensorStr " mat1, " THPTensorStr " mat2, #" THPTensorStr " out)", "(" THPTensorStr " source, " RealStr " alpha, " THSPTensorStr " mat1, " THPTensorStr " mat2, #" THPTensorStr " out)", "(" RealStr " beta, " THPTensorStr " source, " RealStr " alpha, " THPTensorStr " mat1, " THPTensorStr " mat2, #" THPTensorStr " out)", "(" RealStr " beta, " THPTensorStr " source, " RealStr " alpha, " THSPTensorStr " mat1, " THPTensorStr " mat2, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_(addmm_)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_beta = NULL;
    PyObject *__kw_alpha = NULL;
    PyObject *__kw_mat1 = NULL;
    PyObject *__kw_mat2 = NULL;
    if (kwargs) {
      __kw_beta = PyDict_GetItemString(kwargs, "beta");
      __kw_alpha = PyDict_GetItemString(kwargs, "alpha");
      __kw_mat1 = PyDict_GetItemString(kwargs, "mat1");
      __kw_mat2 = PyDict_GetItemString(kwargs, "mat2");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 4 &&
          (__tuplecount > 0 || __kw_beta) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta)) &&
          (__tuplecount > 1 || __kw_alpha) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_alpha)) &&
          (__tuplecount > 2 || __kw_mat1) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat1)) == THPTensorClass &&
          (__tuplecount > 3 || __kw_mat2) && (PyObject*)Py_TYPE((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_mat2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_beta = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta));
      real arg_alpha = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_alpha));
      THTensor* arg_mat1 = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat1))->cdata;
      THTensor* arg_mat2 = ((THPTensor*)(__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_mat2))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addmm)(LIBRARY_STATE arg_self, arg_beta, arg_self, arg_alpha, arg_mat1, arg_mat2);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    #if !IS_DISTRIBUTED
          
    } else if (__argcount == 4 &&
          (__tuplecount > 0 || __kw_beta) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta)) &&
          (__tuplecount > 1 || __kw_alpha) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_alpha)) &&
          (__tuplecount > 2 || __kw_mat1) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat1)) == THSPTensorClass &&
          (__tuplecount > 3 || __kw_mat2) && (PyObject*)Py_TYPE((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_mat2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_beta = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta));
      real arg_alpha = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_alpha));
      THSTensor* arg_mat1 = ((THSPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat1))->cdata;
      THTensor* arg_mat2 = ((THPTensor*)(__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_mat2))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(spaddmm)(LIBRARY_STATE arg_self, arg_beta, arg_self, arg_alpha, arg_mat1, arg_mat2);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif

    } else if (__argcount == 3 &&
          (__tuplecount > 0 || __kw_beta) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta)) &&
          (__tuplecount > 1 || __kw_mat1) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat1)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_mat2) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_beta = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta));
      THTensor* arg_mat1 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat1))->cdata;
      THTensor* arg_mat2 = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat2))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addmm)(LIBRARY_STATE arg_self, arg_beta, arg_self, AS_REAL(1), arg_mat1, arg_mat2);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 3 &&
          (__tuplecount > 0 || __kw_alpha) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_alpha)) &&
          (__tuplecount > 1 || __kw_mat1) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat1)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_mat2) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_alpha = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_alpha));
      THTensor* arg_mat1 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat1))->cdata;
      THTensor* arg_mat2 = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat2))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addmm)(LIBRARY_STATE arg_self, AS_REAL(1), arg_self, arg_alpha, arg_mat1, arg_mat2);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    #if !IS_DISTRIBUTED
          
    } else if (__argcount == 3 &&
          (__tuplecount > 0 || __kw_beta) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta)) &&
          (__tuplecount > 1 || __kw_mat1) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat1)) == THSPTensorClass &&
          (__tuplecount > 2 || __kw_mat2) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_beta = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta));
      THSTensor* arg_mat1 = ((THSPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat1))->cdata;
      THTensor* arg_mat2 = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat2))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(spaddmm)(LIBRARY_STATE arg_self, arg_beta, arg_self, AS_REAL(1), arg_mat1, arg_mat2);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif
#if !IS_DISTRIBUTED
          
    } else if (__argcount == 3 &&
          (__tuplecount > 0 || __kw_alpha) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_alpha)) &&
          (__tuplecount > 1 || __kw_mat1) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat1)) == THSPTensorClass &&
          (__tuplecount > 2 || __kw_mat2) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_alpha = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_alpha));
      THSTensor* arg_mat1 = ((THSPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat1))->cdata;
      THTensor* arg_mat2 = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat2))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(spaddmm)(LIBRARY_STATE arg_self, AS_REAL(1), arg_self, arg_alpha, arg_mat1, arg_mat2);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif

    } else if (__argcount == 2 &&
          (__tuplecount > 0 || __kw_mat1) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mat1)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_mat2) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_mat1 = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mat1))->cdata;
      THTensor* arg_mat2 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat2))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addmm)(LIBRARY_STATE arg_self, AS_REAL(1), arg_self, AS_REAL(1), arg_mat1, arg_mat2);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    #if !IS_DISTRIBUTED
          
    } else if (__argcount == 2 &&
          (__tuplecount > 0 || __kw_mat1) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mat1)) == THSPTensorClass &&
          (__tuplecount > 1 || __kw_mat2) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THSTensor* arg_mat1 = ((THSPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mat1))->cdata;
      THTensor* arg_mat2 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat2))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(spaddmm)(LIBRARY_STATE arg_self, AS_REAL(1), arg_self, AS_REAL(1), arg_mat1, arg_mat2);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif

    }

    THPUtils_invalidArguments(args, kwargs, "addmm_", 8, "(" THPTensorStr " mat1, " THPTensorStr " mat2)", "(" THSPTensorStr " mat1, " THPTensorStr " mat2)", "(" RealStr " beta, " THPTensorStr " mat1, " THPTensorStr " mat2)", "(" RealStr " alpha, " THPTensorStr " mat1, " THPTensorStr " mat2)", "(" RealStr " beta, " THSPTensorStr " mat1, " THPTensorStr " mat2)", "(" RealStr " alpha, " THSPTensorStr " mat1, " THPTensorStr " mat2)", "(" RealStr " beta, " RealStr " alpha, " THPTensorStr " mat1, " THPTensorStr " mat2)", "(" RealStr " beta, " RealStr " alpha, " THSPTensorStr " mat1, " THPTensorStr " mat2)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_(addmv)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_beta = NULL;
    PyObject *__kw_alpha = NULL;
    PyObject *__kw_mat = NULL;
    PyObject *__kw_vec = NULL;
    if (kwargs) {
      __kw_beta = PyDict_GetItemString(kwargs, "beta");
      __kw_alpha = PyDict_GetItemString(kwargs, "alpha");
      __kw_mat = PyDict_GetItemString(kwargs, "mat");
      __kw_vec = PyDict_GetItemString(kwargs, "vec");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 5 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_beta) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta)) &&
          (__tuplecount > 1 || __kw_alpha) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_alpha)) &&
          (__tuplecount > 2 || __kw_mat) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat)) == THPTensorClass &&
          (__tuplecount > 3 || __kw_vec) && (PyObject*)Py_TYPE((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_vec)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      real arg_beta = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta));
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_alpha = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_alpha));
      THTensor* arg_mat = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat))->cdata;
      THTensor* arg_vec = ((THPTensor*)(__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_vec))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      
      if(THTensor_(nDimension)(LIBRARY_STATE arg_mat) <= 0) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "mat", 0 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_mat));
      }
      int64_t arg_self_dim0_size = THTensor_(size)(LIBRARY_STATE arg_mat, 0);
      THLongStoragePtr arg_self_storage(THLongStorage_newWithSize1(arg_self_dim0_size));
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_self_storage->data, arg_self_storage->size);
      if (try_expand) {
        arg_self_guard =
        THTensor_(new)(LIBRARY_STATE_NOARGS);
        
        expand(LIBRARY_STATE arg_self_guard.get(), arg_self, arg_self_storage);
        arg_self = arg_self_guard.get();
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addmv)(LIBRARY_STATE arg_result, arg_beta, arg_self, arg_alpha, arg_mat, arg_vec);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      
      
    
    } else if (___out == NULL &&
          __argcount == 4 &&
          (__tuplecount > 0 || __kw_beta) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta)) &&
          (__tuplecount > 1 || __kw_alpha) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_alpha)) &&
          (__tuplecount > 2 || __kw_mat) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat)) == THPTensorClass &&
          (__tuplecount > 3 || __kw_vec) && (PyObject*)Py_TYPE((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_vec)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      real arg_beta = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta));
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_alpha = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_alpha));
      THTensor* arg_mat = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat))->cdata;
      THTensor* arg_vec = ((THPTensor*)(__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_vec))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      
      if(THTensor_(nDimension)(LIBRARY_STATE arg_mat) <= 0) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "mat", 0 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_mat));
      }
      int64_t arg_self_dim0_size = THTensor_(size)(LIBRARY_STATE arg_mat, 0);
      THLongStoragePtr arg_self_storage(THLongStorage_newWithSize1(arg_self_dim0_size));
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_self_storage->data, arg_self_storage->size);
      if (try_expand) {
        arg_self_guard =
        THTensor_(new)(LIBRARY_STATE_NOARGS);
        
        expand(LIBRARY_STATE arg_self_guard.get(), arg_self, arg_self_storage);
        arg_self = arg_self_guard.get();
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addmv)(LIBRARY_STATE arg_result, arg_beta, arg_self, arg_alpha, arg_mat, arg_vec);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      
      
    
    } else if (___out != NULL &&
          __argcount == 4 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_beta) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta)) &&
          (__tuplecount > 1 || __kw_mat) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_vec) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_vec)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      real arg_beta = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta));
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_mat = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat))->cdata;
      THTensor* arg_vec = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_vec))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      
      if(THTensor_(nDimension)(LIBRARY_STATE arg_mat) <= 0) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "mat", 0 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_mat));
      }
      int64_t arg_self_dim0_size = THTensor_(size)(LIBRARY_STATE arg_mat, 0);
      THLongStoragePtr arg_self_storage(THLongStorage_newWithSize1(arg_self_dim0_size));
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_self_storage->data, arg_self_storage->size);
      if (try_expand) {
        arg_self_guard =
        THTensor_(new)(LIBRARY_STATE_NOARGS);
        
        expand(LIBRARY_STATE arg_self_guard.get(), arg_self, arg_self_storage);
        arg_self = arg_self_guard.get();
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addmv)(LIBRARY_STATE arg_result, arg_beta, arg_self, AS_REAL(1), arg_mat, arg_vec);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      
      
    
    } else if (___out != NULL &&
          __argcount == 4 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_alpha) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_alpha)) &&
          (__tuplecount > 1 || __kw_mat) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_vec) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_vec)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_alpha = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_alpha));
      THTensor* arg_mat = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat))->cdata;
      THTensor* arg_vec = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_vec))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      
      if(THTensor_(nDimension)(LIBRARY_STATE arg_mat) <= 0) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "mat", 0 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_mat));
      }
      int64_t arg_self_dim0_size = THTensor_(size)(LIBRARY_STATE arg_mat, 0);
      THLongStoragePtr arg_self_storage(THLongStorage_newWithSize1(arg_self_dim0_size));
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_self_storage->data, arg_self_storage->size);
      if (try_expand) {
        arg_self_guard =
        THTensor_(new)(LIBRARY_STATE_NOARGS);
        
        expand(LIBRARY_STATE arg_self_guard.get(), arg_self, arg_self_storage);
        arg_self = arg_self_guard.get();
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addmv)(LIBRARY_STATE arg_result, AS_REAL(1), arg_self, arg_alpha, arg_mat, arg_vec);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      
      
    
    } else if (___out == NULL &&
          __argcount == 3 &&
          (__tuplecount > 0 || __kw_beta) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta)) &&
          (__tuplecount > 1 || __kw_mat) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_vec) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_vec)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      real arg_beta = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta));
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_mat = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat))->cdata;
      THTensor* arg_vec = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_vec))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      
      if(THTensor_(nDimension)(LIBRARY_STATE arg_mat) <= 0) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "mat", 0 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_mat));
      }
      int64_t arg_self_dim0_size = THTensor_(size)(LIBRARY_STATE arg_mat, 0);
      THLongStoragePtr arg_self_storage(THLongStorage_newWithSize1(arg_self_dim0_size));
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_self_storage->data, arg_self_storage->size);
      if (try_expand) {
        arg_self_guard =
        THTensor_(new)(LIBRARY_STATE_NOARGS);
        
        expand(LIBRARY_STATE arg_self_guard.get(), arg_self, arg_self_storage);
        arg_self = arg_self_guard.get();
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addmv)(LIBRARY_STATE arg_result, arg_beta, arg_self, AS_REAL(1), arg_mat, arg_vec);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      
      
    
    } else if (___out == NULL &&
          __argcount == 3 &&
          (__tuplecount > 0 || __kw_alpha) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_alpha)) &&
          (__tuplecount > 1 || __kw_mat) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_vec) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_vec)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_alpha = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_alpha));
      THTensor* arg_mat = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat))->cdata;
      THTensor* arg_vec = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_vec))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      
      if(THTensor_(nDimension)(LIBRARY_STATE arg_mat) <= 0) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "mat", 0 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_mat));
      }
      int64_t arg_self_dim0_size = THTensor_(size)(LIBRARY_STATE arg_mat, 0);
      THLongStoragePtr arg_self_storage(THLongStorage_newWithSize1(arg_self_dim0_size));
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_self_storage->data, arg_self_storage->size);
      if (try_expand) {
        arg_self_guard =
        THTensor_(new)(LIBRARY_STATE_NOARGS);
        
        expand(LIBRARY_STATE arg_self_guard.get(), arg_self, arg_self_storage);
        arg_self = arg_self_guard.get();
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addmv)(LIBRARY_STATE arg_result, AS_REAL(1), arg_self, arg_alpha, arg_mat, arg_vec);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      
      
    
    } else if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_mat) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mat)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_vec) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_vec)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_mat = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mat))->cdata;
      THTensor* arg_vec = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_vec))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      
      if(THTensor_(nDimension)(LIBRARY_STATE arg_mat) <= 0) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "mat", 0 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_mat));
      }
      int64_t arg_self_dim0_size = THTensor_(size)(LIBRARY_STATE arg_mat, 0);
      THLongStoragePtr arg_self_storage(THLongStorage_newWithSize1(arg_self_dim0_size));
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_self_storage->data, arg_self_storage->size);
      if (try_expand) {
        arg_self_guard =
        THTensor_(new)(LIBRARY_STATE_NOARGS);
        
        expand(LIBRARY_STATE arg_self_guard.get(), arg_self, arg_self_storage);
        arg_self = arg_self_guard.get();
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addmv)(LIBRARY_STATE arg_result, AS_REAL(1), arg_self, AS_REAL(1), arg_mat, arg_vec);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      
      
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_mat) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mat)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_vec) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_vec)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_mat = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mat))->cdata;
      THTensor* arg_vec = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_vec))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      
      if(THTensor_(nDimension)(LIBRARY_STATE arg_mat) <= 0) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "mat", 0 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_mat));
      }
      int64_t arg_self_dim0_size = THTensor_(size)(LIBRARY_STATE arg_mat, 0);
      THLongStoragePtr arg_self_storage(THLongStorage_newWithSize1(arg_self_dim0_size));
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_self_storage->data, arg_self_storage->size);
      if (try_expand) {
        arg_self_guard =
        THTensor_(new)(LIBRARY_STATE_NOARGS);
        
        expand(LIBRARY_STATE arg_self_guard.get(), arg_self, arg_self_storage);
        arg_self = arg_self_guard.get();
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addmv)(LIBRARY_STATE arg_result, AS_REAL(1), arg_self, AS_REAL(1), arg_mat, arg_vec);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      
      
    
    }

    THPUtils_invalidArguments(args, kwargs, "addmv", 4, "(" THPTensorStr " mat, " THPTensorStr " vec, #" THPTensorStr " out)", "(" RealStr " beta, " THPTensorStr " mat, " THPTensorStr " vec, #" THPTensorStr " out)", "(" RealStr " alpha, " THPTensorStr " mat, " THPTensorStr " vec, #" THPTensorStr " out)", "(" RealStr " beta, " RealStr " alpha, " THPTensorStr " mat, " THPTensorStr " vec, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_stateless_(addmv)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_beta = NULL;
    PyObject *__kw_source = NULL;
    PyObject *__kw_alpha = NULL;
    PyObject *__kw_mat = NULL;
    PyObject *__kw_vec = NULL;
    if (kwargs) {
      __kw_beta = PyDict_GetItemString(kwargs, "beta");
      __kw_source = PyDict_GetItemString(kwargs, "source");
      __kw_alpha = PyDict_GetItemString(kwargs, "alpha");
      __kw_mat = PyDict_GetItemString(kwargs, "mat");
      __kw_vec = PyDict_GetItemString(kwargs, "vec");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 6 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_beta) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta)) &&
          (__tuplecount > 1 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_alpha) && THPUtils_(checkReal)((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_alpha)) &&
          (__tuplecount > 3 || __kw_mat) && (PyObject*)Py_TYPE((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_mat)) == THPTensorClass &&
          (__tuplecount > 4 || __kw_vec) && (PyObject*)Py_TYPE((__tuplecount > 4 ? PyTuple_GET_ITEM(args, 4) : __kw_vec)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      real arg_beta = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta));
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_source))->cdata;
      real arg_alpha = THPUtils_(unpackReal)((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_alpha));
      THTensor* arg_mat = ((THPTensor*)(__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_mat))->cdata;
      THTensor* arg_vec = ((THPTensor*)(__tuplecount > 4 ? PyTuple_GET_ITEM(args, 4) : __kw_vec))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      
      if(THTensor_(nDimension)(LIBRARY_STATE arg_mat) <= 0) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "mat", 0 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_mat));
      }
      int64_t arg_self_dim0_size = THTensor_(size)(LIBRARY_STATE arg_mat, 0);
      THLongStoragePtr arg_self_storage(THLongStorage_newWithSize1(arg_self_dim0_size));
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_self_storage->data, arg_self_storage->size);
      if (try_expand) {
        arg_self_guard =
        THTensor_(new)(LIBRARY_STATE_NOARGS);
        
        expand(LIBRARY_STATE arg_self_guard.get(), arg_self, arg_self_storage);
        arg_self = arg_self_guard.get();
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addmv)(LIBRARY_STATE arg_result, arg_beta, arg_self, arg_alpha, arg_mat, arg_vec);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      
      
    
    } else if (___out == NULL &&
          __argcount == 5 &&
          (__tuplecount > 0 || __kw_beta) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta)) &&
          (__tuplecount > 1 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_alpha) && THPUtils_(checkReal)((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_alpha)) &&
          (__tuplecount > 3 || __kw_mat) && (PyObject*)Py_TYPE((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_mat)) == THPTensorClass &&
          (__tuplecount > 4 || __kw_vec) && (PyObject*)Py_TYPE((__tuplecount > 4 ? PyTuple_GET_ITEM(args, 4) : __kw_vec)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      real arg_beta = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta));
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_source))->cdata;
      real arg_alpha = THPUtils_(unpackReal)((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_alpha));
      THTensor* arg_mat = ((THPTensor*)(__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_mat))->cdata;
      THTensor* arg_vec = ((THPTensor*)(__tuplecount > 4 ? PyTuple_GET_ITEM(args, 4) : __kw_vec))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      
      if(THTensor_(nDimension)(LIBRARY_STATE arg_mat) <= 0) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "mat", 0 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_mat));
      }
      int64_t arg_self_dim0_size = THTensor_(size)(LIBRARY_STATE arg_mat, 0);
      THLongStoragePtr arg_self_storage(THLongStorage_newWithSize1(arg_self_dim0_size));
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_self_storage->data, arg_self_storage->size);
      if (try_expand) {
        arg_self_guard =
        THTensor_(new)(LIBRARY_STATE_NOARGS);
        
        expand(LIBRARY_STATE arg_self_guard.get(), arg_self, arg_self_storage);
        arg_self = arg_self_guard.get();
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addmv)(LIBRARY_STATE arg_result, arg_beta, arg_self, arg_alpha, arg_mat, arg_vec);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      
      
    
    } else if (___out != NULL &&
          __argcount == 5 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_beta) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta)) &&
          (__tuplecount > 1 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_mat) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat)) == THPTensorClass &&
          (__tuplecount > 3 || __kw_vec) && (PyObject*)Py_TYPE((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_vec)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      real arg_beta = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta));
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_source))->cdata;
      THTensor* arg_mat = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat))->cdata;
      THTensor* arg_vec = ((THPTensor*)(__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_vec))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      
      if(THTensor_(nDimension)(LIBRARY_STATE arg_mat) <= 0) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "mat", 0 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_mat));
      }
      int64_t arg_self_dim0_size = THTensor_(size)(LIBRARY_STATE arg_mat, 0);
      THLongStoragePtr arg_self_storage(THLongStorage_newWithSize1(arg_self_dim0_size));
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_self_storage->data, arg_self_storage->size);
      if (try_expand) {
        arg_self_guard =
        THTensor_(new)(LIBRARY_STATE_NOARGS);
        
        expand(LIBRARY_STATE arg_self_guard.get(), arg_self, arg_self_storage);
        arg_self = arg_self_guard.get();
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addmv)(LIBRARY_STATE arg_result, arg_beta, arg_self, AS_REAL(1), arg_mat, arg_vec);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      
      
    
    } else if (___out != NULL &&
          __argcount == 5 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_alpha) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_alpha)) &&
          (__tuplecount > 2 || __kw_mat) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat)) == THPTensorClass &&
          (__tuplecount > 3 || __kw_vec) && (PyObject*)Py_TYPE((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_vec)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      real arg_alpha = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_alpha));
      THTensor* arg_mat = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat))->cdata;
      THTensor* arg_vec = ((THPTensor*)(__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_vec))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      
      if(THTensor_(nDimension)(LIBRARY_STATE arg_mat) <= 0) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "mat", 0 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_mat));
      }
      int64_t arg_self_dim0_size = THTensor_(size)(LIBRARY_STATE arg_mat, 0);
      THLongStoragePtr arg_self_storage(THLongStorage_newWithSize1(arg_self_dim0_size));
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_self_storage->data, arg_self_storage->size);
      if (try_expand) {
        arg_self_guard =
        THTensor_(new)(LIBRARY_STATE_NOARGS);
        
        expand(LIBRARY_STATE arg_self_guard.get(), arg_self, arg_self_storage);
        arg_self = arg_self_guard.get();
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addmv)(LIBRARY_STATE arg_result, AS_REAL(1), arg_self, arg_alpha, arg_mat, arg_vec);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      
      
    
    } else if (___out == NULL &&
          __argcount == 4 &&
          (__tuplecount > 0 || __kw_beta) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta)) &&
          (__tuplecount > 1 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_mat) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat)) == THPTensorClass &&
          (__tuplecount > 3 || __kw_vec) && (PyObject*)Py_TYPE((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_vec)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      real arg_beta = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta));
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_source))->cdata;
      THTensor* arg_mat = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat))->cdata;
      THTensor* arg_vec = ((THPTensor*)(__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_vec))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      
      if(THTensor_(nDimension)(LIBRARY_STATE arg_mat) <= 0) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "mat", 0 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_mat));
      }
      int64_t arg_self_dim0_size = THTensor_(size)(LIBRARY_STATE arg_mat, 0);
      THLongStoragePtr arg_self_storage(THLongStorage_newWithSize1(arg_self_dim0_size));
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_self_storage->data, arg_self_storage->size);
      if (try_expand) {
        arg_self_guard =
        THTensor_(new)(LIBRARY_STATE_NOARGS);
        
        expand(LIBRARY_STATE arg_self_guard.get(), arg_self, arg_self_storage);
        arg_self = arg_self_guard.get();
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addmv)(LIBRARY_STATE arg_result, arg_beta, arg_self, AS_REAL(1), arg_mat, arg_vec);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      
      
    
    } else if (___out == NULL &&
          __argcount == 4 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_alpha) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_alpha)) &&
          (__tuplecount > 2 || __kw_mat) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat)) == THPTensorClass &&
          (__tuplecount > 3 || __kw_vec) && (PyObject*)Py_TYPE((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_vec)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      real arg_alpha = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_alpha));
      THTensor* arg_mat = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat))->cdata;
      THTensor* arg_vec = ((THPTensor*)(__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_vec))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      
      if(THTensor_(nDimension)(LIBRARY_STATE arg_mat) <= 0) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "mat", 0 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_mat));
      }
      int64_t arg_self_dim0_size = THTensor_(size)(LIBRARY_STATE arg_mat, 0);
      THLongStoragePtr arg_self_storage(THLongStorage_newWithSize1(arg_self_dim0_size));
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_self_storage->data, arg_self_storage->size);
      if (try_expand) {
        arg_self_guard =
        THTensor_(new)(LIBRARY_STATE_NOARGS);
        
        expand(LIBRARY_STATE arg_self_guard.get(), arg_self, arg_self_storage);
        arg_self = arg_self_guard.get();
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addmv)(LIBRARY_STATE arg_result, AS_REAL(1), arg_self, arg_alpha, arg_mat, arg_vec);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      
      
    
    } else if (___out != NULL &&
          __argcount == 4 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_mat) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_vec) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_vec)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_mat = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat))->cdata;
      THTensor* arg_vec = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_vec))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      
      if(THTensor_(nDimension)(LIBRARY_STATE arg_mat) <= 0) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "mat", 0 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_mat));
      }
      int64_t arg_self_dim0_size = THTensor_(size)(LIBRARY_STATE arg_mat, 0);
      THLongStoragePtr arg_self_storage(THLongStorage_newWithSize1(arg_self_dim0_size));
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_self_storage->data, arg_self_storage->size);
      if (try_expand) {
        arg_self_guard =
        THTensor_(new)(LIBRARY_STATE_NOARGS);
        
        expand(LIBRARY_STATE arg_self_guard.get(), arg_self, arg_self_storage);
        arg_self = arg_self_guard.get();
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addmv)(LIBRARY_STATE arg_result, AS_REAL(1), arg_self, AS_REAL(1), arg_mat, arg_vec);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      
      
    
    } else if (___out == NULL &&
          __argcount == 3 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_mat) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_vec) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_vec)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_mat = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat))->cdata;
      THTensor* arg_vec = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_vec))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      
      if(THTensor_(nDimension)(LIBRARY_STATE arg_mat) <= 0) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "mat", 0 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_mat));
      }
      int64_t arg_self_dim0_size = THTensor_(size)(LIBRARY_STATE arg_mat, 0);
      THLongStoragePtr arg_self_storage(THLongStorage_newWithSize1(arg_self_dim0_size));
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_self_storage->data, arg_self_storage->size);
      if (try_expand) {
        arg_self_guard =
        THTensor_(new)(LIBRARY_STATE_NOARGS);
        
        expand(LIBRARY_STATE arg_self_guard.get(), arg_self, arg_self_storage);
        arg_self = arg_self_guard.get();
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addmv)(LIBRARY_STATE arg_result, AS_REAL(1), arg_self, AS_REAL(1), arg_mat, arg_vec);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      
      
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.addmv", 4, "(" THPTensorStr " source, " THPTensorStr " mat, " THPTensorStr " vec, #" THPTensorStr " out)", "(" RealStr " beta, " THPTensorStr " source, " THPTensorStr " mat, " THPTensorStr " vec, #" THPTensorStr " out)", "(" THPTensorStr " source, " RealStr " alpha, " THPTensorStr " mat, " THPTensorStr " vec, #" THPTensorStr " out)", "(" RealStr " beta, " THPTensorStr " source, " RealStr " alpha, " THPTensorStr " mat, " THPTensorStr " vec, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_(addmv_)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_beta = NULL;
    PyObject *__kw_alpha = NULL;
    PyObject *__kw_mat = NULL;
    PyObject *__kw_vec = NULL;
    if (kwargs) {
      __kw_beta = PyDict_GetItemString(kwargs, "beta");
      __kw_alpha = PyDict_GetItemString(kwargs, "alpha");
      __kw_mat = PyDict_GetItemString(kwargs, "mat");
      __kw_vec = PyDict_GetItemString(kwargs, "vec");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 4 &&
          (__tuplecount > 0 || __kw_beta) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta)) &&
          (__tuplecount > 1 || __kw_alpha) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_alpha)) &&
          (__tuplecount > 2 || __kw_mat) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat)) == THPTensorClass &&
          (__tuplecount > 3 || __kw_vec) && (PyObject*)Py_TYPE((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_vec)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_beta = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta));
      real arg_alpha = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_alpha));
      THTensor* arg_mat = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat))->cdata;
      THTensor* arg_vec = ((THPTensor*)(__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_vec))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addmv)(LIBRARY_STATE arg_self, arg_beta, arg_self, arg_alpha, arg_mat, arg_vec);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 3 &&
          (__tuplecount > 0 || __kw_beta) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta)) &&
          (__tuplecount > 1 || __kw_mat) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_vec) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_vec)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_beta = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta));
      THTensor* arg_mat = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat))->cdata;
      THTensor* arg_vec = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_vec))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addmv)(LIBRARY_STATE arg_self, arg_beta, arg_self, AS_REAL(1), arg_mat, arg_vec);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 3 &&
          (__tuplecount > 0 || __kw_alpha) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_alpha)) &&
          (__tuplecount > 1 || __kw_mat) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_vec) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_vec)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_alpha = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_alpha));
      THTensor* arg_mat = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat))->cdata;
      THTensor* arg_vec = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_vec))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addmv)(LIBRARY_STATE arg_self, AS_REAL(1), arg_self, arg_alpha, arg_mat, arg_vec);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 2 &&
          (__tuplecount > 0 || __kw_mat) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mat)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_vec) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_vec)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_mat = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mat))->cdata;
      THTensor* arg_vec = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_vec))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addmv)(LIBRARY_STATE arg_self, AS_REAL(1), arg_self, AS_REAL(1), arg_mat, arg_vec);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "addmv_", 4, "(" THPTensorStr " mat, " THPTensorStr " vec)", "(" RealStr " beta, " THPTensorStr " mat, " THPTensorStr " vec)", "(" RealStr " alpha, " THPTensorStr " mat, " THPTensorStr " vec)", "(" RealStr " beta, " RealStr " alpha, " THPTensorStr " mat, " THPTensorStr " vec)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_(addr)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_beta = NULL;
    PyObject *__kw_alpha = NULL;
    PyObject *__kw_vec1 = NULL;
    PyObject *__kw_vec2 = NULL;
    if (kwargs) {
      __kw_beta = PyDict_GetItemString(kwargs, "beta");
      __kw_alpha = PyDict_GetItemString(kwargs, "alpha");
      __kw_vec1 = PyDict_GetItemString(kwargs, "vec1");
      __kw_vec2 = PyDict_GetItemString(kwargs, "vec2");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 5 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_beta) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta)) &&
          (__tuplecount > 1 || __kw_alpha) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_alpha)) &&
          (__tuplecount > 2 || __kw_vec1) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_vec1)) == THPTensorClass &&
          (__tuplecount > 3 || __kw_vec2) && (PyObject*)Py_TYPE((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_vec2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      real arg_beta = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta));
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_alpha = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_alpha));
      THTensor* arg_vec1 = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_vec1))->cdata;
      THTensor* arg_vec2 = ((THPTensor*)(__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_vec2))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      
      if(THTensor_(nDimension)(LIBRARY_STATE arg_vec1) <= 0) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "vec1", 0 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_vec1));
      }
      int64_t arg_self_dim0_size = THTensor_(size)(LIBRARY_STATE arg_vec1, 0);
      if(THTensor_(nDimension)(LIBRARY_STATE arg_vec2) <= 0) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "vec2", 0 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_vec2));
      }
      int64_t arg_self_dim1_size = THTensor_(size)(LIBRARY_STATE arg_vec2, 0);
      THLongStoragePtr arg_self_storage(
          THLongStorage_newWithSize2(arg_self_dim0_size, arg_self_dim1_size));
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_self_storage->data, arg_self_storage->size);
      if (try_expand) {
        arg_self_guard =
        THTensor_(new)(LIBRARY_STATE_NOARGS);
        
        expand(LIBRARY_STATE arg_self_guard.get(), arg_self, arg_self_storage);
        arg_self = arg_self_guard.get();
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addr)(LIBRARY_STATE arg_result, arg_beta, arg_self, arg_alpha, arg_vec1, arg_vec2);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      
      
    
    } else if (___out == NULL &&
          __argcount == 4 &&
          (__tuplecount > 0 || __kw_beta) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta)) &&
          (__tuplecount > 1 || __kw_alpha) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_alpha)) &&
          (__tuplecount > 2 || __kw_vec1) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_vec1)) == THPTensorClass &&
          (__tuplecount > 3 || __kw_vec2) && (PyObject*)Py_TYPE((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_vec2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      real arg_beta = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta));
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_alpha = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_alpha));
      THTensor* arg_vec1 = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_vec1))->cdata;
      THTensor* arg_vec2 = ((THPTensor*)(__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_vec2))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      
      if(THTensor_(nDimension)(LIBRARY_STATE arg_vec1) <= 0) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "vec1", 0 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_vec1));
      }
      int64_t arg_self_dim0_size = THTensor_(size)(LIBRARY_STATE arg_vec1, 0);
      if(THTensor_(nDimension)(LIBRARY_STATE arg_vec2) <= 0) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "vec2", 0 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_vec2));
      }
      int64_t arg_self_dim1_size = THTensor_(size)(LIBRARY_STATE arg_vec2, 0);
      THLongStoragePtr arg_self_storage(
          THLongStorage_newWithSize2(arg_self_dim0_size, arg_self_dim1_size));
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_self_storage->data, arg_self_storage->size);
      if (try_expand) {
        arg_self_guard =
        THTensor_(new)(LIBRARY_STATE_NOARGS);
        
        expand(LIBRARY_STATE arg_self_guard.get(), arg_self, arg_self_storage);
        arg_self = arg_self_guard.get();
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addr)(LIBRARY_STATE arg_result, arg_beta, arg_self, arg_alpha, arg_vec1, arg_vec2);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      
      
    
    } else if (___out != NULL &&
          __argcount == 4 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_beta) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta)) &&
          (__tuplecount > 1 || __kw_vec1) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_vec1)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_vec2) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_vec2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      real arg_beta = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta));
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_vec1 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_vec1))->cdata;
      THTensor* arg_vec2 = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_vec2))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      
      if(THTensor_(nDimension)(LIBRARY_STATE arg_vec1) <= 0) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "vec1", 0 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_vec1));
      }
      int64_t arg_self_dim0_size = THTensor_(size)(LIBRARY_STATE arg_vec1, 0);
      if(THTensor_(nDimension)(LIBRARY_STATE arg_vec2) <= 0) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "vec2", 0 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_vec2));
      }
      int64_t arg_self_dim1_size = THTensor_(size)(LIBRARY_STATE arg_vec2, 0);
      THLongStoragePtr arg_self_storage(
          THLongStorage_newWithSize2(arg_self_dim0_size, arg_self_dim1_size));
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_self_storage->data, arg_self_storage->size);
      if (try_expand) {
        arg_self_guard =
        THTensor_(new)(LIBRARY_STATE_NOARGS);
        
        expand(LIBRARY_STATE arg_self_guard.get(), arg_self, arg_self_storage);
        arg_self = arg_self_guard.get();
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addr)(LIBRARY_STATE arg_result, arg_beta, arg_self, AS_REAL(1), arg_vec1, arg_vec2);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      
      
    
    } else if (___out != NULL &&
          __argcount == 4 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_alpha) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_alpha)) &&
          (__tuplecount > 1 || __kw_vec1) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_vec1)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_vec2) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_vec2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_alpha = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_alpha));
      THTensor* arg_vec1 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_vec1))->cdata;
      THTensor* arg_vec2 = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_vec2))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      
      if(THTensor_(nDimension)(LIBRARY_STATE arg_vec1) <= 0) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "vec1", 0 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_vec1));
      }
      int64_t arg_self_dim0_size = THTensor_(size)(LIBRARY_STATE arg_vec1, 0);
      if(THTensor_(nDimension)(LIBRARY_STATE arg_vec2) <= 0) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "vec2", 0 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_vec2));
      }
      int64_t arg_self_dim1_size = THTensor_(size)(LIBRARY_STATE arg_vec2, 0);
      THLongStoragePtr arg_self_storage(
          THLongStorage_newWithSize2(arg_self_dim0_size, arg_self_dim1_size));
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_self_storage->data, arg_self_storage->size);
      if (try_expand) {
        arg_self_guard =
        THTensor_(new)(LIBRARY_STATE_NOARGS);
        
        expand(LIBRARY_STATE arg_self_guard.get(), arg_self, arg_self_storage);
        arg_self = arg_self_guard.get();
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addr)(LIBRARY_STATE arg_result, AS_REAL(1), arg_self, arg_alpha, arg_vec1, arg_vec2);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      
      
    
    } else if (___out == NULL &&
          __argcount == 3 &&
          (__tuplecount > 0 || __kw_beta) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta)) &&
          (__tuplecount > 1 || __kw_vec1) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_vec1)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_vec2) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_vec2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      real arg_beta = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta));
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_vec1 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_vec1))->cdata;
      THTensor* arg_vec2 = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_vec2))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      
      if(THTensor_(nDimension)(LIBRARY_STATE arg_vec1) <= 0) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "vec1", 0 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_vec1));
      }
      int64_t arg_self_dim0_size = THTensor_(size)(LIBRARY_STATE arg_vec1, 0);
      if(THTensor_(nDimension)(LIBRARY_STATE arg_vec2) <= 0) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "vec2", 0 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_vec2));
      }
      int64_t arg_self_dim1_size = THTensor_(size)(LIBRARY_STATE arg_vec2, 0);
      THLongStoragePtr arg_self_storage(
          THLongStorage_newWithSize2(arg_self_dim0_size, arg_self_dim1_size));
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_self_storage->data, arg_self_storage->size);
      if (try_expand) {
        arg_self_guard =
        THTensor_(new)(LIBRARY_STATE_NOARGS);
        
        expand(LIBRARY_STATE arg_self_guard.get(), arg_self, arg_self_storage);
        arg_self = arg_self_guard.get();
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addr)(LIBRARY_STATE arg_result, arg_beta, arg_self, AS_REAL(1), arg_vec1, arg_vec2);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      
      
    
    } else if (___out == NULL &&
          __argcount == 3 &&
          (__tuplecount > 0 || __kw_alpha) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_alpha)) &&
          (__tuplecount > 1 || __kw_vec1) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_vec1)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_vec2) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_vec2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_alpha = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_alpha));
      THTensor* arg_vec1 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_vec1))->cdata;
      THTensor* arg_vec2 = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_vec2))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      
      if(THTensor_(nDimension)(LIBRARY_STATE arg_vec1) <= 0) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "vec1", 0 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_vec1));
      }
      int64_t arg_self_dim0_size = THTensor_(size)(LIBRARY_STATE arg_vec1, 0);
      if(THTensor_(nDimension)(LIBRARY_STATE arg_vec2) <= 0) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "vec2", 0 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_vec2));
      }
      int64_t arg_self_dim1_size = THTensor_(size)(LIBRARY_STATE arg_vec2, 0);
      THLongStoragePtr arg_self_storage(
          THLongStorage_newWithSize2(arg_self_dim0_size, arg_self_dim1_size));
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_self_storage->data, arg_self_storage->size);
      if (try_expand) {
        arg_self_guard =
        THTensor_(new)(LIBRARY_STATE_NOARGS);
        
        expand(LIBRARY_STATE arg_self_guard.get(), arg_self, arg_self_storage);
        arg_self = arg_self_guard.get();
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addr)(LIBRARY_STATE arg_result, AS_REAL(1), arg_self, arg_alpha, arg_vec1, arg_vec2);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      
      
    
    } else if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_vec1) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_vec1)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_vec2) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_vec2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_vec1 = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_vec1))->cdata;
      THTensor* arg_vec2 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_vec2))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      
      if(THTensor_(nDimension)(LIBRARY_STATE arg_vec1) <= 0) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "vec1", 0 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_vec1));
      }
      int64_t arg_self_dim0_size = THTensor_(size)(LIBRARY_STATE arg_vec1, 0);
      if(THTensor_(nDimension)(LIBRARY_STATE arg_vec2) <= 0) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "vec2", 0 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_vec2));
      }
      int64_t arg_self_dim1_size = THTensor_(size)(LIBRARY_STATE arg_vec2, 0);
      THLongStoragePtr arg_self_storage(
          THLongStorage_newWithSize2(arg_self_dim0_size, arg_self_dim1_size));
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_self_storage->data, arg_self_storage->size);
      if (try_expand) {
        arg_self_guard =
        THTensor_(new)(LIBRARY_STATE_NOARGS);
        
        expand(LIBRARY_STATE arg_self_guard.get(), arg_self, arg_self_storage);
        arg_self = arg_self_guard.get();
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addr)(LIBRARY_STATE arg_result, AS_REAL(1), arg_self, AS_REAL(1), arg_vec1, arg_vec2);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      
      
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_vec1) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_vec1)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_vec2) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_vec2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_vec1 = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_vec1))->cdata;
      THTensor* arg_vec2 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_vec2))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      
      if(THTensor_(nDimension)(LIBRARY_STATE arg_vec1) <= 0) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "vec1", 0 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_vec1));
      }
      int64_t arg_self_dim0_size = THTensor_(size)(LIBRARY_STATE arg_vec1, 0);
      if(THTensor_(nDimension)(LIBRARY_STATE arg_vec2) <= 0) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "vec2", 0 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_vec2));
      }
      int64_t arg_self_dim1_size = THTensor_(size)(LIBRARY_STATE arg_vec2, 0);
      THLongStoragePtr arg_self_storage(
          THLongStorage_newWithSize2(arg_self_dim0_size, arg_self_dim1_size));
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_self_storage->data, arg_self_storage->size);
      if (try_expand) {
        arg_self_guard =
        THTensor_(new)(LIBRARY_STATE_NOARGS);
        
        expand(LIBRARY_STATE arg_self_guard.get(), arg_self, arg_self_storage);
        arg_self = arg_self_guard.get();
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addr)(LIBRARY_STATE arg_result, AS_REAL(1), arg_self, AS_REAL(1), arg_vec1, arg_vec2);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      
      
    
    }

    THPUtils_invalidArguments(args, kwargs, "addr", 4, "(" THPTensorStr " vec1, " THPTensorStr " vec2, #" THPTensorStr " out)", "(" RealStr " beta, " THPTensorStr " vec1, " THPTensorStr " vec2, #" THPTensorStr " out)", "(" RealStr " alpha, " THPTensorStr " vec1, " THPTensorStr " vec2, #" THPTensorStr " out)", "(" RealStr " beta, " RealStr " alpha, " THPTensorStr " vec1, " THPTensorStr " vec2, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_stateless_(addr)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_beta = NULL;
    PyObject *__kw_source = NULL;
    PyObject *__kw_alpha = NULL;
    PyObject *__kw_vec1 = NULL;
    PyObject *__kw_vec2 = NULL;
    if (kwargs) {
      __kw_beta = PyDict_GetItemString(kwargs, "beta");
      __kw_source = PyDict_GetItemString(kwargs, "source");
      __kw_alpha = PyDict_GetItemString(kwargs, "alpha");
      __kw_vec1 = PyDict_GetItemString(kwargs, "vec1");
      __kw_vec2 = PyDict_GetItemString(kwargs, "vec2");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 6 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_beta) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta)) &&
          (__tuplecount > 1 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_alpha) && THPUtils_(checkReal)((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_alpha)) &&
          (__tuplecount > 3 || __kw_vec1) && (PyObject*)Py_TYPE((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_vec1)) == THPTensorClass &&
          (__tuplecount > 4 || __kw_vec2) && (PyObject*)Py_TYPE((__tuplecount > 4 ? PyTuple_GET_ITEM(args, 4) : __kw_vec2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      real arg_beta = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta));
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_source))->cdata;
      real arg_alpha = THPUtils_(unpackReal)((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_alpha));
      THTensor* arg_vec1 = ((THPTensor*)(__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_vec1))->cdata;
      THTensor* arg_vec2 = ((THPTensor*)(__tuplecount > 4 ? PyTuple_GET_ITEM(args, 4) : __kw_vec2))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      
      if(THTensor_(nDimension)(LIBRARY_STATE arg_vec1) <= 0) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "vec1", 0 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_vec1));
      }
      int64_t arg_self_dim0_size = THTensor_(size)(LIBRARY_STATE arg_vec1, 0);
      if(THTensor_(nDimension)(LIBRARY_STATE arg_vec2) <= 0) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "vec2", 0 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_vec2));
      }
      int64_t arg_self_dim1_size = THTensor_(size)(LIBRARY_STATE arg_vec2, 0);
      THLongStoragePtr arg_self_storage(
          THLongStorage_newWithSize2(arg_self_dim0_size, arg_self_dim1_size));
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_self_storage->data, arg_self_storage->size);
      if (try_expand) {
        arg_self_guard =
        THTensor_(new)(LIBRARY_STATE_NOARGS);
        
        expand(LIBRARY_STATE arg_self_guard.get(), arg_self, arg_self_storage);
        arg_self = arg_self_guard.get();
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addr)(LIBRARY_STATE arg_result, arg_beta, arg_self, arg_alpha, arg_vec1, arg_vec2);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      
      
    
    } else if (___out == NULL &&
          __argcount == 5 &&
          (__tuplecount > 0 || __kw_beta) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta)) &&
          (__tuplecount > 1 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_alpha) && THPUtils_(checkReal)((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_alpha)) &&
          (__tuplecount > 3 || __kw_vec1) && (PyObject*)Py_TYPE((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_vec1)) == THPTensorClass &&
          (__tuplecount > 4 || __kw_vec2) && (PyObject*)Py_TYPE((__tuplecount > 4 ? PyTuple_GET_ITEM(args, 4) : __kw_vec2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      real arg_beta = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta));
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_source))->cdata;
      real arg_alpha = THPUtils_(unpackReal)((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_alpha));
      THTensor* arg_vec1 = ((THPTensor*)(__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_vec1))->cdata;
      THTensor* arg_vec2 = ((THPTensor*)(__tuplecount > 4 ? PyTuple_GET_ITEM(args, 4) : __kw_vec2))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      
      if(THTensor_(nDimension)(LIBRARY_STATE arg_vec1) <= 0) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "vec1", 0 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_vec1));
      }
      int64_t arg_self_dim0_size = THTensor_(size)(LIBRARY_STATE arg_vec1, 0);
      if(THTensor_(nDimension)(LIBRARY_STATE arg_vec2) <= 0) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "vec2", 0 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_vec2));
      }
      int64_t arg_self_dim1_size = THTensor_(size)(LIBRARY_STATE arg_vec2, 0);
      THLongStoragePtr arg_self_storage(
          THLongStorage_newWithSize2(arg_self_dim0_size, arg_self_dim1_size));
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_self_storage->data, arg_self_storage->size);
      if (try_expand) {
        arg_self_guard =
        THTensor_(new)(LIBRARY_STATE_NOARGS);
        
        expand(LIBRARY_STATE arg_self_guard.get(), arg_self, arg_self_storage);
        arg_self = arg_self_guard.get();
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addr)(LIBRARY_STATE arg_result, arg_beta, arg_self, arg_alpha, arg_vec1, arg_vec2);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      
      
    
    } else if (___out != NULL &&
          __argcount == 5 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_beta) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta)) &&
          (__tuplecount > 1 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_vec1) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_vec1)) == THPTensorClass &&
          (__tuplecount > 3 || __kw_vec2) && (PyObject*)Py_TYPE((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_vec2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      real arg_beta = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta));
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_source))->cdata;
      THTensor* arg_vec1 = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_vec1))->cdata;
      THTensor* arg_vec2 = ((THPTensor*)(__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_vec2))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      
      if(THTensor_(nDimension)(LIBRARY_STATE arg_vec1) <= 0) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "vec1", 0 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_vec1));
      }
      int64_t arg_self_dim0_size = THTensor_(size)(LIBRARY_STATE arg_vec1, 0);
      if(THTensor_(nDimension)(LIBRARY_STATE arg_vec2) <= 0) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "vec2", 0 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_vec2));
      }
      int64_t arg_self_dim1_size = THTensor_(size)(LIBRARY_STATE arg_vec2, 0);
      THLongStoragePtr arg_self_storage(
          THLongStorage_newWithSize2(arg_self_dim0_size, arg_self_dim1_size));
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_self_storage->data, arg_self_storage->size);
      if (try_expand) {
        arg_self_guard =
        THTensor_(new)(LIBRARY_STATE_NOARGS);
        
        expand(LIBRARY_STATE arg_self_guard.get(), arg_self, arg_self_storage);
        arg_self = arg_self_guard.get();
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addr)(LIBRARY_STATE arg_result, arg_beta, arg_self, AS_REAL(1), arg_vec1, arg_vec2);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      
      
    
    } else if (___out != NULL &&
          __argcount == 5 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_alpha) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_alpha)) &&
          (__tuplecount > 2 || __kw_vec1) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_vec1)) == THPTensorClass &&
          (__tuplecount > 3 || __kw_vec2) && (PyObject*)Py_TYPE((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_vec2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      real arg_alpha = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_alpha));
      THTensor* arg_vec1 = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_vec1))->cdata;
      THTensor* arg_vec2 = ((THPTensor*)(__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_vec2))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      
      if(THTensor_(nDimension)(LIBRARY_STATE arg_vec1) <= 0) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "vec1", 0 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_vec1));
      }
      int64_t arg_self_dim0_size = THTensor_(size)(LIBRARY_STATE arg_vec1, 0);
      if(THTensor_(nDimension)(LIBRARY_STATE arg_vec2) <= 0) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "vec2", 0 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_vec2));
      }
      int64_t arg_self_dim1_size = THTensor_(size)(LIBRARY_STATE arg_vec2, 0);
      THLongStoragePtr arg_self_storage(
          THLongStorage_newWithSize2(arg_self_dim0_size, arg_self_dim1_size));
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_self_storage->data, arg_self_storage->size);
      if (try_expand) {
        arg_self_guard =
        THTensor_(new)(LIBRARY_STATE_NOARGS);
        
        expand(LIBRARY_STATE arg_self_guard.get(), arg_self, arg_self_storage);
        arg_self = arg_self_guard.get();
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addr)(LIBRARY_STATE arg_result, AS_REAL(1), arg_self, arg_alpha, arg_vec1, arg_vec2);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      
      
    
    } else if (___out == NULL &&
          __argcount == 4 &&
          (__tuplecount > 0 || __kw_beta) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta)) &&
          (__tuplecount > 1 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_vec1) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_vec1)) == THPTensorClass &&
          (__tuplecount > 3 || __kw_vec2) && (PyObject*)Py_TYPE((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_vec2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      real arg_beta = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta));
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_source))->cdata;
      THTensor* arg_vec1 = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_vec1))->cdata;
      THTensor* arg_vec2 = ((THPTensor*)(__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_vec2))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      
      if(THTensor_(nDimension)(LIBRARY_STATE arg_vec1) <= 0) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "vec1", 0 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_vec1));
      }
      int64_t arg_self_dim0_size = THTensor_(size)(LIBRARY_STATE arg_vec1, 0);
      if(THTensor_(nDimension)(LIBRARY_STATE arg_vec2) <= 0) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "vec2", 0 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_vec2));
      }
      int64_t arg_self_dim1_size = THTensor_(size)(LIBRARY_STATE arg_vec2, 0);
      THLongStoragePtr arg_self_storage(
          THLongStorage_newWithSize2(arg_self_dim0_size, arg_self_dim1_size));
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_self_storage->data, arg_self_storage->size);
      if (try_expand) {
        arg_self_guard =
        THTensor_(new)(LIBRARY_STATE_NOARGS);
        
        expand(LIBRARY_STATE arg_self_guard.get(), arg_self, arg_self_storage);
        arg_self = arg_self_guard.get();
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addr)(LIBRARY_STATE arg_result, arg_beta, arg_self, AS_REAL(1), arg_vec1, arg_vec2);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      
      
    
    } else if (___out == NULL &&
          __argcount == 4 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_alpha) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_alpha)) &&
          (__tuplecount > 2 || __kw_vec1) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_vec1)) == THPTensorClass &&
          (__tuplecount > 3 || __kw_vec2) && (PyObject*)Py_TYPE((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_vec2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      real arg_alpha = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_alpha));
      THTensor* arg_vec1 = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_vec1))->cdata;
      THTensor* arg_vec2 = ((THPTensor*)(__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_vec2))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      
      if(THTensor_(nDimension)(LIBRARY_STATE arg_vec1) <= 0) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "vec1", 0 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_vec1));
      }
      int64_t arg_self_dim0_size = THTensor_(size)(LIBRARY_STATE arg_vec1, 0);
      if(THTensor_(nDimension)(LIBRARY_STATE arg_vec2) <= 0) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "vec2", 0 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_vec2));
      }
      int64_t arg_self_dim1_size = THTensor_(size)(LIBRARY_STATE arg_vec2, 0);
      THLongStoragePtr arg_self_storage(
          THLongStorage_newWithSize2(arg_self_dim0_size, arg_self_dim1_size));
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_self_storage->data, arg_self_storage->size);
      if (try_expand) {
        arg_self_guard =
        THTensor_(new)(LIBRARY_STATE_NOARGS);
        
        expand(LIBRARY_STATE arg_self_guard.get(), arg_self, arg_self_storage);
        arg_self = arg_self_guard.get();
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addr)(LIBRARY_STATE arg_result, AS_REAL(1), arg_self, arg_alpha, arg_vec1, arg_vec2);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      
      
    
    } else if (___out != NULL &&
          __argcount == 4 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_vec1) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_vec1)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_vec2) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_vec2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_vec1 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_vec1))->cdata;
      THTensor* arg_vec2 = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_vec2))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      
      if(THTensor_(nDimension)(LIBRARY_STATE arg_vec1) <= 0) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "vec1", 0 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_vec1));
      }
      int64_t arg_self_dim0_size = THTensor_(size)(LIBRARY_STATE arg_vec1, 0);
      if(THTensor_(nDimension)(LIBRARY_STATE arg_vec2) <= 0) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "vec2", 0 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_vec2));
      }
      int64_t arg_self_dim1_size = THTensor_(size)(LIBRARY_STATE arg_vec2, 0);
      THLongStoragePtr arg_self_storage(
          THLongStorage_newWithSize2(arg_self_dim0_size, arg_self_dim1_size));
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_self_storage->data, arg_self_storage->size);
      if (try_expand) {
        arg_self_guard =
        THTensor_(new)(LIBRARY_STATE_NOARGS);
        
        expand(LIBRARY_STATE arg_self_guard.get(), arg_self, arg_self_storage);
        arg_self = arg_self_guard.get();
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addr)(LIBRARY_STATE arg_result, AS_REAL(1), arg_self, AS_REAL(1), arg_vec1, arg_vec2);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      
      
    
    } else if (___out == NULL &&
          __argcount == 3 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_vec1) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_vec1)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_vec2) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_vec2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_vec1 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_vec1))->cdata;
      THTensor* arg_vec2 = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_vec2))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      
      if(THTensor_(nDimension)(LIBRARY_STATE arg_vec1) <= 0) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "vec1", 0 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_vec1));
      }
      int64_t arg_self_dim0_size = THTensor_(size)(LIBRARY_STATE arg_vec1, 0);
      if(THTensor_(nDimension)(LIBRARY_STATE arg_vec2) <= 0) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "vec2", 0 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_vec2));
      }
      int64_t arg_self_dim1_size = THTensor_(size)(LIBRARY_STATE arg_vec2, 0);
      THLongStoragePtr arg_self_storage(
          THLongStorage_newWithSize2(arg_self_dim0_size, arg_self_dim1_size));
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_self_storage->data, arg_self_storage->size);
      if (try_expand) {
        arg_self_guard =
        THTensor_(new)(LIBRARY_STATE_NOARGS);
        
        expand(LIBRARY_STATE arg_self_guard.get(), arg_self, arg_self_storage);
        arg_self = arg_self_guard.get();
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addr)(LIBRARY_STATE arg_result, AS_REAL(1), arg_self, AS_REAL(1), arg_vec1, arg_vec2);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      
      
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.addr", 4, "(" THPTensorStr " source, " THPTensorStr " vec1, " THPTensorStr " vec2, #" THPTensorStr " out)", "(" RealStr " beta, " THPTensorStr " source, " THPTensorStr " vec1, " THPTensorStr " vec2, #" THPTensorStr " out)", "(" THPTensorStr " source, " RealStr " alpha, " THPTensorStr " vec1, " THPTensorStr " vec2, #" THPTensorStr " out)", "(" RealStr " beta, " THPTensorStr " source, " RealStr " alpha, " THPTensorStr " vec1, " THPTensorStr " vec2, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_(addr_)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_beta = NULL;
    PyObject *__kw_alpha = NULL;
    PyObject *__kw_vec1 = NULL;
    PyObject *__kw_vec2 = NULL;
    if (kwargs) {
      __kw_beta = PyDict_GetItemString(kwargs, "beta");
      __kw_alpha = PyDict_GetItemString(kwargs, "alpha");
      __kw_vec1 = PyDict_GetItemString(kwargs, "vec1");
      __kw_vec2 = PyDict_GetItemString(kwargs, "vec2");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 4 &&
          (__tuplecount > 0 || __kw_beta) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta)) &&
          (__tuplecount > 1 || __kw_alpha) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_alpha)) &&
          (__tuplecount > 2 || __kw_vec1) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_vec1)) == THPTensorClass &&
          (__tuplecount > 3 || __kw_vec2) && (PyObject*)Py_TYPE((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_vec2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_beta = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta));
      real arg_alpha = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_alpha));
      THTensor* arg_vec1 = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_vec1))->cdata;
      THTensor* arg_vec2 = ((THPTensor*)(__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_vec2))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addr)(LIBRARY_STATE arg_self, arg_beta, arg_self, arg_alpha, arg_vec1, arg_vec2);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 3 &&
          (__tuplecount > 0 || __kw_beta) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta)) &&
          (__tuplecount > 1 || __kw_vec1) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_vec1)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_vec2) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_vec2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_beta = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta));
      THTensor* arg_vec1 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_vec1))->cdata;
      THTensor* arg_vec2 = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_vec2))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addr)(LIBRARY_STATE arg_self, arg_beta, arg_self, AS_REAL(1), arg_vec1, arg_vec2);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 3 &&
          (__tuplecount > 0 || __kw_alpha) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_alpha)) &&
          (__tuplecount > 1 || __kw_vec1) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_vec1)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_vec2) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_vec2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_alpha = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_alpha));
      THTensor* arg_vec1 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_vec1))->cdata;
      THTensor* arg_vec2 = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_vec2))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addr)(LIBRARY_STATE arg_self, AS_REAL(1), arg_self, arg_alpha, arg_vec1, arg_vec2);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 2 &&
          (__tuplecount > 0 || __kw_vec1) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_vec1)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_vec2) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_vec2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_vec1 = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_vec1))->cdata;
      THTensor* arg_vec2 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_vec2))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addr)(LIBRARY_STATE arg_self, AS_REAL(1), arg_self, AS_REAL(1), arg_vec1, arg_vec2);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "addr_", 4, "(" THPTensorStr " vec1, " THPTensorStr " vec2)", "(" RealStr " beta, " THPTensorStr " vec1, " THPTensorStr " vec2)", "(" RealStr " alpha, " THPTensorStr " vec1, " THPTensorStr " vec2)", "(" RealStr " beta, " RealStr " alpha, " THPTensorStr " vec1, " THPTensorStr " vec2)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_(ger)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_vec2 = NULL;
    if (kwargs) {
      __kw_vec2 = PyDict_GetItemString(kwargs, "vec2");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_vec2) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_vec2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_0 = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_vec2 = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_vec2))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        int64_t s1 = THTensor_(size)(LIBRARY_STATE ((THPTensor*)self)->cdata, 0);
        int64_t s2 = THTensor_(size)(LIBRARY_STATE ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_vec2))->cdata, 0);
        THTensor_(resize2d)(LIBRARY_STATE ((THPTensor*)___out)->cdata, s1, s2);
        
        Py_UNBLOCK_THREADS;
        THTensor_(addr)(LIBRARY_STATE arg_result, AS_REAL(0), arg_0, AS_REAL(1), arg_self, arg_vec2);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_vec2) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_vec2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_0 = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_vec2 = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_vec2))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        int64_t s1 = THTensor_(size)(LIBRARY_STATE ((THPTensor*)self)->cdata, 0);
        int64_t s2 = THTensor_(size)(LIBRARY_STATE ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_vec2))->cdata, 0);
        THTensor_(resize2d)(LIBRARY_STATE ((THPTensor*)result)->cdata, s1, s2);
        
        Py_UNBLOCK_THREADS;
        THTensor_(addr)(LIBRARY_STATE arg_result, AS_REAL(0), arg_0, AS_REAL(1), arg_self, arg_vec2);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "ger", 1, "(" THPTensorStr " vec2, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_stateless_(ger)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    PyObject *__kw_vec2 = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
      __kw_vec2 = PyDict_GetItemString(kwargs, "vec2");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_vec2) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_vec2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_0 = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_vec2 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_vec2))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        int64_t s1 = THTensor_(size)(LIBRARY_STATE ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata, 0);
        int64_t s2 = THTensor_(size)(LIBRARY_STATE ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_vec2))->cdata, 0);
        THTensor_(resize2d)(LIBRARY_STATE ((THPTensor*)___out)->cdata, s1, s2);
        
        Py_UNBLOCK_THREADS;
        THTensor_(addr)(LIBRARY_STATE arg_result, AS_REAL(0), arg_0, AS_REAL(1), arg_self, arg_vec2);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_vec2) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_vec2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_0 = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_vec2 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_vec2))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        int64_t s1 = THTensor_(size)(LIBRARY_STATE ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata, 0);
        int64_t s2 = THTensor_(size)(LIBRARY_STATE ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_vec2))->cdata, 0);
        THTensor_(resize2d)(LIBRARY_STATE ((THPTensor*)result)->cdata, s1, s2);
        
        Py_UNBLOCK_THREADS;
        THTensor_(addr)(LIBRARY_STATE arg_result, AS_REAL(0), arg_0, AS_REAL(1), arg_self, arg_vec2);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.ger", 1, "(" THPTensorStr " source, " THPTensorStr " vec2, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_(mv)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_vec = NULL;
    if (kwargs) {
      __kw_vec = PyDict_GetItemString(kwargs, "vec");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_vec) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_vec)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_0 = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_vec = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_vec))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        int64_t s = THTensor_(size)(LIBRARY_STATE ((THPTensor*)self)->cdata, 0);
        THTensor_(resize1d)(LIBRARY_STATE ((THPTensor*)___out)->cdata, s);
        #if !IS_CUDA
        THTensor_(zero)(LIBRARY_STATE ((THPTensor*)___out)->cdata);
        #endif
        
        Py_UNBLOCK_THREADS;
        THTensor_(addmv)(LIBRARY_STATE arg_result, AS_REAL(0), arg_0, AS_REAL(1), arg_self, arg_vec);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_vec) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_vec)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_0 = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_vec = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_vec))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        int64_t s = THTensor_(size)(LIBRARY_STATE ((THPTensor*)self)->cdata, 0);
        THTensor_(resize1d)(LIBRARY_STATE ((THPTensor*)result)->cdata, s);
        #if !IS_CUDA
        THTensor_(zero)(LIBRARY_STATE ((THPTensor*)result)->cdata);
        #endif
        
        Py_UNBLOCK_THREADS;
        THTensor_(addmv)(LIBRARY_STATE arg_result, AS_REAL(0), arg_0, AS_REAL(1), arg_self, arg_vec);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "mv", 1, "(" THPTensorStr " vec, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_stateless_(mv)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    PyObject *__kw_vec = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
      __kw_vec = PyDict_GetItemString(kwargs, "vec");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_vec) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_vec)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_0 = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_vec = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_vec))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        int64_t s = THTensor_(size)(LIBRARY_STATE ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata, 0);
        THTensor_(resize1d)(LIBRARY_STATE ((THPTensor*)___out)->cdata, s);
        #if !IS_CUDA
        THTensor_(zero)(LIBRARY_STATE ((THPTensor*)___out)->cdata);
        #endif
        
        Py_UNBLOCK_THREADS;
        THTensor_(addmv)(LIBRARY_STATE arg_result, AS_REAL(0), arg_0, AS_REAL(1), arg_self, arg_vec);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_vec) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_vec)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_0 = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_vec = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_vec))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        int64_t s = THTensor_(size)(LIBRARY_STATE ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata, 0);
        THTensor_(resize1d)(LIBRARY_STATE ((THPTensor*)result)->cdata, s);
        #if !IS_CUDA
        THTensor_(zero)(LIBRARY_STATE ((THPTensor*)result)->cdata);
        #endif
        
        Py_UNBLOCK_THREADS;
        THTensor_(addmv)(LIBRARY_STATE arg_result, AS_REAL(0), arg_0, AS_REAL(1), arg_self, arg_vec);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.mv", 1, "(" THPTensorStr " source, " THPTensorStr " vec, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_(mm)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_mat2 = NULL;
    if (kwargs) {
      __kw_mat2 = PyDict_GetItemString(kwargs, "mat2");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_mat2) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mat2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_0 = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_mat2 = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mat2))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        int64_t s1 = THTensor_(size)(LIBRARY_STATE ((THPTensor*)self)->cdata, 0);
        int64_t s2 = THTensor_(size)(LIBRARY_STATE ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mat2))->cdata, 1);
        THTensor_(resize2d)(LIBRARY_STATE ((THPTensor*)___out)->cdata, s1, s2);
        #if !IS_CUDA
        THTensor_(zero)(LIBRARY_STATE ((THPTensor*)___out)->cdata);
        #endif
        
        Py_UNBLOCK_THREADS;
        THTensor_(addmm)(LIBRARY_STATE arg_result, AS_REAL(0), arg_0, AS_REAL(1), arg_self, arg_mat2);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    #if !IS_DISTRIBUTED
          
    } else if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_mat2) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mat2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_0 = ((THPTensor*)___out)->cdata;
      THSTensor* arg_self = ((THSPTensor*)self)->cdata;
      THTensor* arg_mat2 = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mat2))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(spaddmm)(LIBRARY_STATE arg_result, AS_REAL(0), arg_0, AS_REAL(1), arg_self, arg_mat2);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif

    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_mat2) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mat2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_0 = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_mat2 = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mat2))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        int64_t s1 = THTensor_(size)(LIBRARY_STATE ((THPTensor*)self)->cdata, 0);
        int64_t s2 = THTensor_(size)(LIBRARY_STATE ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mat2))->cdata, 1);
        THTensor_(resize2d)(LIBRARY_STATE ((THPTensor*)result)->cdata, s1, s2);
        #if !IS_CUDA
        THTensor_(zero)(LIBRARY_STATE ((THPTensor*)result)->cdata);
        #endif
        
        Py_UNBLOCK_THREADS;
        THTensor_(addmm)(LIBRARY_STATE arg_result, AS_REAL(0), arg_0, AS_REAL(1), arg_self, arg_mat2);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    #if !IS_DISTRIBUTED
          
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_mat2) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mat2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_0 = ((THPTensor*)result)->cdata;
      THSTensor* arg_self = ((THSPTensor*)self)->cdata;
      THTensor* arg_mat2 = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mat2))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(spaddmm)(LIBRARY_STATE arg_result, AS_REAL(0), arg_0, AS_REAL(1), arg_self, arg_mat2);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif

    }

    THPUtils_invalidArguments(args, kwargs, "mm", 1, "(" THPTensorStr " mat2, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_stateless_(mm)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    PyObject *__kw_mat2 = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
      __kw_mat2 = PyDict_GetItemString(kwargs, "mat2");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_mat2) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_0 = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_mat2 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat2))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        int64_t s1 = THTensor_(size)(LIBRARY_STATE ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata, 0);
        int64_t s2 = THTensor_(size)(LIBRARY_STATE ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat2))->cdata, 1);
        THTensor_(resize2d)(LIBRARY_STATE ((THPTensor*)___out)->cdata, s1, s2);
        #if !IS_CUDA
        THTensor_(zero)(LIBRARY_STATE ((THPTensor*)___out)->cdata);
        #endif
        
        Py_UNBLOCK_THREADS;
        THTensor_(addmm)(LIBRARY_STATE arg_result, AS_REAL(0), arg_0, AS_REAL(1), arg_self, arg_mat2);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    #if !IS_DISTRIBUTED
          
    } else if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THSPTensorClass &&
          (__tuplecount > 1 || __kw_mat2) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_0 = ((THPTensor*)___out)->cdata;
      THSTensor* arg_self = ((THSPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_mat2 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat2))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(spaddmm)(LIBRARY_STATE arg_result, AS_REAL(0), arg_0, AS_REAL(1), arg_self, arg_mat2);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif

    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_mat2) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_0 = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_mat2 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat2))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        int64_t s1 = THTensor_(size)(LIBRARY_STATE ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata, 0);
        int64_t s2 = THTensor_(size)(LIBRARY_STATE ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat2))->cdata, 1);
        THTensor_(resize2d)(LIBRARY_STATE ((THPTensor*)result)->cdata, s1, s2);
        #if !IS_CUDA
        THTensor_(zero)(LIBRARY_STATE ((THPTensor*)result)->cdata);
        #endif
        
        Py_UNBLOCK_THREADS;
        THTensor_(addmm)(LIBRARY_STATE arg_result, AS_REAL(0), arg_0, AS_REAL(1), arg_self, arg_mat2);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    #if !IS_DISTRIBUTED
          
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THSPTensorClass &&
          (__tuplecount > 1 || __kw_mat2) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_0 = ((THPTensor*)result)->cdata;
      THSTensor* arg_self = ((THSPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_mat2 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat2))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(spaddmm)(LIBRARY_STATE arg_result, AS_REAL(0), arg_0, AS_REAL(1), arg_self, arg_mat2);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif

    }

    THPUtils_invalidArguments(args, kwargs, "torch.mm", 2, "(" THPTensorStr " source, " THPTensorStr " mat2, #" THPTensorStr " out)", "(" THSPTensorStr " source, " THPTensorStr " mat2, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_(bmm)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_mat2 = NULL;
    if (kwargs) {
      __kw_mat2 = PyDict_GetItemString(kwargs, "mat2");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_mat2) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mat2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_0 = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_mat2 = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mat2))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        int64_t s1 = THTensor_(size)(LIBRARY_STATE ((THPTensor*)self)->cdata, 0);
        int64_t s2 = THTensor_(size)(LIBRARY_STATE ((THPTensor*)self)->cdata, 1);
        int64_t s3 = THTensor_(size)(LIBRARY_STATE ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mat2))->cdata, 2);
        THTensor_(resize3d)(LIBRARY_STATE ((THPTensor*)___out)->cdata, s1, s2, s3);
        #if !IS_CUDA
        THTensor_(zero)(LIBRARY_STATE ((THPTensor*)___out)->cdata);
        #endif
        
        Py_UNBLOCK_THREADS;
        THTensor_(baddbmm)(LIBRARY_STATE arg_result, AS_REAL(0), arg_0, AS_REAL(1), arg_self, arg_mat2);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_mat2) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mat2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_0 = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_mat2 = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mat2))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        int64_t s1 = THTensor_(size)(LIBRARY_STATE ((THPTensor*)self)->cdata, 0);
        int64_t s2 = THTensor_(size)(LIBRARY_STATE ((THPTensor*)self)->cdata, 1);
        int64_t s3 = THTensor_(size)(LIBRARY_STATE ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mat2))->cdata, 2);
        THTensor_(resize3d)(LIBRARY_STATE ((THPTensor*)result)->cdata, s1, s2, s3);
        #if !IS_CUDA
        THTensor_(zero)(LIBRARY_STATE ((THPTensor*)result)->cdata);
        #endif
        
        Py_UNBLOCK_THREADS;
        THTensor_(baddbmm)(LIBRARY_STATE arg_result, AS_REAL(0), arg_0, AS_REAL(1), arg_self, arg_mat2);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "bmm", 1, "(" THPTensorStr " mat2, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_stateless_(bmm)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    PyObject *__kw_mat2 = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
      __kw_mat2 = PyDict_GetItemString(kwargs, "mat2");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_mat2) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_0 = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_mat2 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat2))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        int64_t s1 = THTensor_(size)(LIBRARY_STATE ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata, 0);
        int64_t s2 = THTensor_(size)(LIBRARY_STATE ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata, 1);
        int64_t s3 = THTensor_(size)(LIBRARY_STATE ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat2))->cdata, 2);
        THTensor_(resize3d)(LIBRARY_STATE ((THPTensor*)___out)->cdata, s1, s2, s3);
        #if !IS_CUDA
        THTensor_(zero)(LIBRARY_STATE ((THPTensor*)___out)->cdata);
        #endif
        
        Py_UNBLOCK_THREADS;
        THTensor_(baddbmm)(LIBRARY_STATE arg_result, AS_REAL(0), arg_0, AS_REAL(1), arg_self, arg_mat2);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_mat2) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_0 = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_mat2 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat2))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        int64_t s1 = THTensor_(size)(LIBRARY_STATE ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata, 0);
        int64_t s2 = THTensor_(size)(LIBRARY_STATE ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata, 1);
        int64_t s3 = THTensor_(size)(LIBRARY_STATE ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat2))->cdata, 2);
        THTensor_(resize3d)(LIBRARY_STATE ((THPTensor*)result)->cdata, s1, s2, s3);
        #if !IS_CUDA
        THTensor_(zero)(LIBRARY_STATE ((THPTensor*)result)->cdata);
        #endif
        
        Py_UNBLOCK_THREADS;
        THTensor_(baddbmm)(LIBRARY_STATE arg_result, AS_REAL(0), arg_0, AS_REAL(1), arg_self, arg_mat2);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.bmm", 1, "(" THPTensorStr " source, " THPTensorStr " mat2, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_(addbmm)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_beta = NULL;
    PyObject *__kw_alpha = NULL;
    PyObject *__kw_batch1 = NULL;
    PyObject *__kw_batch2 = NULL;
    if (kwargs) {
      __kw_beta = PyDict_GetItemString(kwargs, "beta");
      __kw_alpha = PyDict_GetItemString(kwargs, "alpha");
      __kw_batch1 = PyDict_GetItemString(kwargs, "batch1");
      __kw_batch2 = PyDict_GetItemString(kwargs, "batch2");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 5 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_beta) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta)) &&
          (__tuplecount > 1 || __kw_alpha) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_alpha)) &&
          (__tuplecount > 2 || __kw_batch1) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_batch1)) == THPTensorClass &&
          (__tuplecount > 3 || __kw_batch2) && (PyObject*)Py_TYPE((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_batch2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      real arg_beta = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta));
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_alpha = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_alpha));
      THTensor* arg_batch1 = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_batch1))->cdata;
      THTensor* arg_batch2 = ((THPTensor*)(__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_batch2))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      
      if(THTensor_(nDimension)(LIBRARY_STATE arg_batch1) <= 1) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "batch1", 1 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_batch1));
      }
      int64_t arg_self_dim0_size = THTensor_(size)(LIBRARY_STATE arg_batch1, 1);
      if(THTensor_(nDimension)(LIBRARY_STATE arg_batch2) <= 2) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "batch2", 2 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_batch2));
      }
      int64_t arg_self_dim1_size = THTensor_(size)(LIBRARY_STATE arg_batch2, 2);
      THLongStoragePtr arg_self_storage(
          THLongStorage_newWithSize2(arg_self_dim0_size, arg_self_dim1_size));
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_self_storage->data, arg_self_storage->size);
      if (try_expand) {
        arg_self_guard =
        THTensor_(new)(LIBRARY_STATE_NOARGS);
        
        expand(LIBRARY_STATE arg_self_guard.get(), arg_self, arg_self_storage);
        arg_self = arg_self_guard.get();
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addbmm)(LIBRARY_STATE arg_result, arg_beta, arg_self, arg_alpha, arg_batch1, arg_batch2);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      
      
    
    } else if (___out == NULL &&
          __argcount == 4 &&
          (__tuplecount > 0 || __kw_beta) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta)) &&
          (__tuplecount > 1 || __kw_alpha) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_alpha)) &&
          (__tuplecount > 2 || __kw_batch1) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_batch1)) == THPTensorClass &&
          (__tuplecount > 3 || __kw_batch2) && (PyObject*)Py_TYPE((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_batch2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      real arg_beta = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta));
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_alpha = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_alpha));
      THTensor* arg_batch1 = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_batch1))->cdata;
      THTensor* arg_batch2 = ((THPTensor*)(__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_batch2))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      
      if(THTensor_(nDimension)(LIBRARY_STATE arg_batch1) <= 1) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "batch1", 1 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_batch1));
      }
      int64_t arg_self_dim0_size = THTensor_(size)(LIBRARY_STATE arg_batch1, 1);
      if(THTensor_(nDimension)(LIBRARY_STATE arg_batch2) <= 2) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "batch2", 2 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_batch2));
      }
      int64_t arg_self_dim1_size = THTensor_(size)(LIBRARY_STATE arg_batch2, 2);
      THLongStoragePtr arg_self_storage(
          THLongStorage_newWithSize2(arg_self_dim0_size, arg_self_dim1_size));
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_self_storage->data, arg_self_storage->size);
      if (try_expand) {
        arg_self_guard =
        THTensor_(new)(LIBRARY_STATE_NOARGS);
        
        expand(LIBRARY_STATE arg_self_guard.get(), arg_self, arg_self_storage);
        arg_self = arg_self_guard.get();
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addbmm)(LIBRARY_STATE arg_result, arg_beta, arg_self, arg_alpha, arg_batch1, arg_batch2);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      
      
    
    } else if (___out != NULL &&
          __argcount == 4 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_beta) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta)) &&
          (__tuplecount > 1 || __kw_batch1) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_batch1)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_batch2) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_batch2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      real arg_beta = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta));
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_batch1 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_batch1))->cdata;
      THTensor* arg_batch2 = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_batch2))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      
      if(THTensor_(nDimension)(LIBRARY_STATE arg_batch1) <= 1) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "batch1", 1 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_batch1));
      }
      int64_t arg_self_dim0_size = THTensor_(size)(LIBRARY_STATE arg_batch1, 1);
      if(THTensor_(nDimension)(LIBRARY_STATE arg_batch2) <= 2) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "batch2", 2 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_batch2));
      }
      int64_t arg_self_dim1_size = THTensor_(size)(LIBRARY_STATE arg_batch2, 2);
      THLongStoragePtr arg_self_storage(
          THLongStorage_newWithSize2(arg_self_dim0_size, arg_self_dim1_size));
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_self_storage->data, arg_self_storage->size);
      if (try_expand) {
        arg_self_guard =
        THTensor_(new)(LIBRARY_STATE_NOARGS);
        
        expand(LIBRARY_STATE arg_self_guard.get(), arg_self, arg_self_storage);
        arg_self = arg_self_guard.get();
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addbmm)(LIBRARY_STATE arg_result, arg_beta, arg_self, AS_REAL(1), arg_batch1, arg_batch2);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      
      
    
    } else if (___out != NULL &&
          __argcount == 4 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_alpha) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_alpha)) &&
          (__tuplecount > 1 || __kw_batch1) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_batch1)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_batch2) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_batch2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_alpha = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_alpha));
      THTensor* arg_batch1 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_batch1))->cdata;
      THTensor* arg_batch2 = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_batch2))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      
      if(THTensor_(nDimension)(LIBRARY_STATE arg_batch1) <= 1) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "batch1", 1 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_batch1));
      }
      int64_t arg_self_dim0_size = THTensor_(size)(LIBRARY_STATE arg_batch1, 1);
      if(THTensor_(nDimension)(LIBRARY_STATE arg_batch2) <= 2) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "batch2", 2 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_batch2));
      }
      int64_t arg_self_dim1_size = THTensor_(size)(LIBRARY_STATE arg_batch2, 2);
      THLongStoragePtr arg_self_storage(
          THLongStorage_newWithSize2(arg_self_dim0_size, arg_self_dim1_size));
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_self_storage->data, arg_self_storage->size);
      if (try_expand) {
        arg_self_guard =
        THTensor_(new)(LIBRARY_STATE_NOARGS);
        
        expand(LIBRARY_STATE arg_self_guard.get(), arg_self, arg_self_storage);
        arg_self = arg_self_guard.get();
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addbmm)(LIBRARY_STATE arg_result, AS_REAL(1), arg_self, arg_alpha, arg_batch1, arg_batch2);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      
      
    
    } else if (___out == NULL &&
          __argcount == 3 &&
          (__tuplecount > 0 || __kw_beta) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta)) &&
          (__tuplecount > 1 || __kw_batch1) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_batch1)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_batch2) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_batch2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      real arg_beta = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta));
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_batch1 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_batch1))->cdata;
      THTensor* arg_batch2 = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_batch2))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      
      if(THTensor_(nDimension)(LIBRARY_STATE arg_batch1) <= 1) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "batch1", 1 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_batch1));
      }
      int64_t arg_self_dim0_size = THTensor_(size)(LIBRARY_STATE arg_batch1, 1);
      if(THTensor_(nDimension)(LIBRARY_STATE arg_batch2) <= 2) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "batch2", 2 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_batch2));
      }
      int64_t arg_self_dim1_size = THTensor_(size)(LIBRARY_STATE arg_batch2, 2);
      THLongStoragePtr arg_self_storage(
          THLongStorage_newWithSize2(arg_self_dim0_size, arg_self_dim1_size));
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_self_storage->data, arg_self_storage->size);
      if (try_expand) {
        arg_self_guard =
        THTensor_(new)(LIBRARY_STATE_NOARGS);
        
        expand(LIBRARY_STATE arg_self_guard.get(), arg_self, arg_self_storage);
        arg_self = arg_self_guard.get();
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addbmm)(LIBRARY_STATE arg_result, arg_beta, arg_self, AS_REAL(1), arg_batch1, arg_batch2);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      
      
    
    } else if (___out == NULL &&
          __argcount == 3 &&
          (__tuplecount > 0 || __kw_alpha) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_alpha)) &&
          (__tuplecount > 1 || __kw_batch1) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_batch1)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_batch2) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_batch2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_alpha = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_alpha));
      THTensor* arg_batch1 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_batch1))->cdata;
      THTensor* arg_batch2 = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_batch2))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      
      if(THTensor_(nDimension)(LIBRARY_STATE arg_batch1) <= 1) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "batch1", 1 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_batch1));
      }
      int64_t arg_self_dim0_size = THTensor_(size)(LIBRARY_STATE arg_batch1, 1);
      if(THTensor_(nDimension)(LIBRARY_STATE arg_batch2) <= 2) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "batch2", 2 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_batch2));
      }
      int64_t arg_self_dim1_size = THTensor_(size)(LIBRARY_STATE arg_batch2, 2);
      THLongStoragePtr arg_self_storage(
          THLongStorage_newWithSize2(arg_self_dim0_size, arg_self_dim1_size));
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_self_storage->data, arg_self_storage->size);
      if (try_expand) {
        arg_self_guard =
        THTensor_(new)(LIBRARY_STATE_NOARGS);
        
        expand(LIBRARY_STATE arg_self_guard.get(), arg_self, arg_self_storage);
        arg_self = arg_self_guard.get();
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addbmm)(LIBRARY_STATE arg_result, AS_REAL(1), arg_self, arg_alpha, arg_batch1, arg_batch2);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      
      
    
    } else if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_batch1) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_batch1)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_batch2) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_batch2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_batch1 = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_batch1))->cdata;
      THTensor* arg_batch2 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_batch2))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      
      if(THTensor_(nDimension)(LIBRARY_STATE arg_batch1) <= 1) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "batch1", 1 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_batch1));
      }
      int64_t arg_self_dim0_size = THTensor_(size)(LIBRARY_STATE arg_batch1, 1);
      if(THTensor_(nDimension)(LIBRARY_STATE arg_batch2) <= 2) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "batch2", 2 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_batch2));
      }
      int64_t arg_self_dim1_size = THTensor_(size)(LIBRARY_STATE arg_batch2, 2);
      THLongStoragePtr arg_self_storage(
          THLongStorage_newWithSize2(arg_self_dim0_size, arg_self_dim1_size));
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_self_storage->data, arg_self_storage->size);
      if (try_expand) {
        arg_self_guard =
        THTensor_(new)(LIBRARY_STATE_NOARGS);
        
        expand(LIBRARY_STATE arg_self_guard.get(), arg_self, arg_self_storage);
        arg_self = arg_self_guard.get();
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addbmm)(LIBRARY_STATE arg_result, AS_REAL(1), arg_self, AS_REAL(1), arg_batch1, arg_batch2);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      
      
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_batch1) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_batch1)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_batch2) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_batch2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_batch1 = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_batch1))->cdata;
      THTensor* arg_batch2 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_batch2))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      
      if(THTensor_(nDimension)(LIBRARY_STATE arg_batch1) <= 1) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "batch1", 1 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_batch1));
      }
      int64_t arg_self_dim0_size = THTensor_(size)(LIBRARY_STATE arg_batch1, 1);
      if(THTensor_(nDimension)(LIBRARY_STATE arg_batch2) <= 2) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "batch2", 2 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_batch2));
      }
      int64_t arg_self_dim1_size = THTensor_(size)(LIBRARY_STATE arg_batch2, 2);
      THLongStoragePtr arg_self_storage(
          THLongStorage_newWithSize2(arg_self_dim0_size, arg_self_dim1_size));
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_self_storage->data, arg_self_storage->size);
      if (try_expand) {
        arg_self_guard =
        THTensor_(new)(LIBRARY_STATE_NOARGS);
        
        expand(LIBRARY_STATE arg_self_guard.get(), arg_self, arg_self_storage);
        arg_self = arg_self_guard.get();
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addbmm)(LIBRARY_STATE arg_result, AS_REAL(1), arg_self, AS_REAL(1), arg_batch1, arg_batch2);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      
      
    
    }

    THPUtils_invalidArguments(args, kwargs, "addbmm", 4, "(" THPTensorStr " batch1, " THPTensorStr " batch2, #" THPTensorStr " out)", "(" RealStr " beta, " THPTensorStr " batch1, " THPTensorStr " batch2, #" THPTensorStr " out)", "(" RealStr " alpha, " THPTensorStr " batch1, " THPTensorStr " batch2, #" THPTensorStr " out)", "(" RealStr " beta, " RealStr " alpha, " THPTensorStr " batch1, " THPTensorStr " batch2, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_stateless_(addbmm)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_beta = NULL;
    PyObject *__kw_source = NULL;
    PyObject *__kw_alpha = NULL;
    PyObject *__kw_batch1 = NULL;
    PyObject *__kw_batch2 = NULL;
    if (kwargs) {
      __kw_beta = PyDict_GetItemString(kwargs, "beta");
      __kw_source = PyDict_GetItemString(kwargs, "source");
      __kw_alpha = PyDict_GetItemString(kwargs, "alpha");
      __kw_batch1 = PyDict_GetItemString(kwargs, "batch1");
      __kw_batch2 = PyDict_GetItemString(kwargs, "batch2");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 6 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_beta) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta)) &&
          (__tuplecount > 1 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_alpha) && THPUtils_(checkReal)((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_alpha)) &&
          (__tuplecount > 3 || __kw_batch1) && (PyObject*)Py_TYPE((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_batch1)) == THPTensorClass &&
          (__tuplecount > 4 || __kw_batch2) && (PyObject*)Py_TYPE((__tuplecount > 4 ? PyTuple_GET_ITEM(args, 4) : __kw_batch2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      real arg_beta = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta));
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_source))->cdata;
      real arg_alpha = THPUtils_(unpackReal)((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_alpha));
      THTensor* arg_batch1 = ((THPTensor*)(__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_batch1))->cdata;
      THTensor* arg_batch2 = ((THPTensor*)(__tuplecount > 4 ? PyTuple_GET_ITEM(args, 4) : __kw_batch2))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      
      if(THTensor_(nDimension)(LIBRARY_STATE arg_batch1) <= 1) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "batch1", 1 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_batch1));
      }
      int64_t arg_self_dim0_size = THTensor_(size)(LIBRARY_STATE arg_batch1, 1);
      if(THTensor_(nDimension)(LIBRARY_STATE arg_batch2) <= 2) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "batch2", 2 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_batch2));
      }
      int64_t arg_self_dim1_size = THTensor_(size)(LIBRARY_STATE arg_batch2, 2);
      THLongStoragePtr arg_self_storage(
          THLongStorage_newWithSize2(arg_self_dim0_size, arg_self_dim1_size));
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_self_storage->data, arg_self_storage->size);
      if (try_expand) {
        arg_self_guard =
        THTensor_(new)(LIBRARY_STATE_NOARGS);
        
        expand(LIBRARY_STATE arg_self_guard.get(), arg_self, arg_self_storage);
        arg_self = arg_self_guard.get();
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addbmm)(LIBRARY_STATE arg_result, arg_beta, arg_self, arg_alpha, arg_batch1, arg_batch2);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      
      
    
    } else if (___out == NULL &&
          __argcount == 5 &&
          (__tuplecount > 0 || __kw_beta) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta)) &&
          (__tuplecount > 1 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_alpha) && THPUtils_(checkReal)((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_alpha)) &&
          (__tuplecount > 3 || __kw_batch1) && (PyObject*)Py_TYPE((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_batch1)) == THPTensorClass &&
          (__tuplecount > 4 || __kw_batch2) && (PyObject*)Py_TYPE((__tuplecount > 4 ? PyTuple_GET_ITEM(args, 4) : __kw_batch2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      real arg_beta = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta));
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_source))->cdata;
      real arg_alpha = THPUtils_(unpackReal)((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_alpha));
      THTensor* arg_batch1 = ((THPTensor*)(__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_batch1))->cdata;
      THTensor* arg_batch2 = ((THPTensor*)(__tuplecount > 4 ? PyTuple_GET_ITEM(args, 4) : __kw_batch2))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      
      if(THTensor_(nDimension)(LIBRARY_STATE arg_batch1) <= 1) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "batch1", 1 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_batch1));
      }
      int64_t arg_self_dim0_size = THTensor_(size)(LIBRARY_STATE arg_batch1, 1);
      if(THTensor_(nDimension)(LIBRARY_STATE arg_batch2) <= 2) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "batch2", 2 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_batch2));
      }
      int64_t arg_self_dim1_size = THTensor_(size)(LIBRARY_STATE arg_batch2, 2);
      THLongStoragePtr arg_self_storage(
          THLongStorage_newWithSize2(arg_self_dim0_size, arg_self_dim1_size));
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_self_storage->data, arg_self_storage->size);
      if (try_expand) {
        arg_self_guard =
        THTensor_(new)(LIBRARY_STATE_NOARGS);
        
        expand(LIBRARY_STATE arg_self_guard.get(), arg_self, arg_self_storage);
        arg_self = arg_self_guard.get();
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addbmm)(LIBRARY_STATE arg_result, arg_beta, arg_self, arg_alpha, arg_batch1, arg_batch2);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      
      
    
    } else if (___out != NULL &&
          __argcount == 5 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_beta) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta)) &&
          (__tuplecount > 1 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_batch1) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_batch1)) == THPTensorClass &&
          (__tuplecount > 3 || __kw_batch2) && (PyObject*)Py_TYPE((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_batch2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      real arg_beta = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta));
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_source))->cdata;
      THTensor* arg_batch1 = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_batch1))->cdata;
      THTensor* arg_batch2 = ((THPTensor*)(__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_batch2))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      
      if(THTensor_(nDimension)(LIBRARY_STATE arg_batch1) <= 1) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "batch1", 1 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_batch1));
      }
      int64_t arg_self_dim0_size = THTensor_(size)(LIBRARY_STATE arg_batch1, 1);
      if(THTensor_(nDimension)(LIBRARY_STATE arg_batch2) <= 2) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "batch2", 2 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_batch2));
      }
      int64_t arg_self_dim1_size = THTensor_(size)(LIBRARY_STATE arg_batch2, 2);
      THLongStoragePtr arg_self_storage(
          THLongStorage_newWithSize2(arg_self_dim0_size, arg_self_dim1_size));
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_self_storage->data, arg_self_storage->size);
      if (try_expand) {
        arg_self_guard =
        THTensor_(new)(LIBRARY_STATE_NOARGS);
        
        expand(LIBRARY_STATE arg_self_guard.get(), arg_self, arg_self_storage);
        arg_self = arg_self_guard.get();
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addbmm)(LIBRARY_STATE arg_result, arg_beta, arg_self, AS_REAL(1), arg_batch1, arg_batch2);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      
      
    
    } else if (___out != NULL &&
          __argcount == 5 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_alpha) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_alpha)) &&
          (__tuplecount > 2 || __kw_batch1) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_batch1)) == THPTensorClass &&
          (__tuplecount > 3 || __kw_batch2) && (PyObject*)Py_TYPE((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_batch2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      real arg_alpha = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_alpha));
      THTensor* arg_batch1 = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_batch1))->cdata;
      THTensor* arg_batch2 = ((THPTensor*)(__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_batch2))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      
      if(THTensor_(nDimension)(LIBRARY_STATE arg_batch1) <= 1) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "batch1", 1 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_batch1));
      }
      int64_t arg_self_dim0_size = THTensor_(size)(LIBRARY_STATE arg_batch1, 1);
      if(THTensor_(nDimension)(LIBRARY_STATE arg_batch2) <= 2) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "batch2", 2 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_batch2));
      }
      int64_t arg_self_dim1_size = THTensor_(size)(LIBRARY_STATE arg_batch2, 2);
      THLongStoragePtr arg_self_storage(
          THLongStorage_newWithSize2(arg_self_dim0_size, arg_self_dim1_size));
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_self_storage->data, arg_self_storage->size);
      if (try_expand) {
        arg_self_guard =
        THTensor_(new)(LIBRARY_STATE_NOARGS);
        
        expand(LIBRARY_STATE arg_self_guard.get(), arg_self, arg_self_storage);
        arg_self = arg_self_guard.get();
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addbmm)(LIBRARY_STATE arg_result, AS_REAL(1), arg_self, arg_alpha, arg_batch1, arg_batch2);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      
      
    
    } else if (___out == NULL &&
          __argcount == 4 &&
          (__tuplecount > 0 || __kw_beta) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta)) &&
          (__tuplecount > 1 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_batch1) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_batch1)) == THPTensorClass &&
          (__tuplecount > 3 || __kw_batch2) && (PyObject*)Py_TYPE((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_batch2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      real arg_beta = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta));
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_source))->cdata;
      THTensor* arg_batch1 = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_batch1))->cdata;
      THTensor* arg_batch2 = ((THPTensor*)(__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_batch2))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      
      if(THTensor_(nDimension)(LIBRARY_STATE arg_batch1) <= 1) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "batch1", 1 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_batch1));
      }
      int64_t arg_self_dim0_size = THTensor_(size)(LIBRARY_STATE arg_batch1, 1);
      if(THTensor_(nDimension)(LIBRARY_STATE arg_batch2) <= 2) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "batch2", 2 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_batch2));
      }
      int64_t arg_self_dim1_size = THTensor_(size)(LIBRARY_STATE arg_batch2, 2);
      THLongStoragePtr arg_self_storage(
          THLongStorage_newWithSize2(arg_self_dim0_size, arg_self_dim1_size));
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_self_storage->data, arg_self_storage->size);
      if (try_expand) {
        arg_self_guard =
        THTensor_(new)(LIBRARY_STATE_NOARGS);
        
        expand(LIBRARY_STATE arg_self_guard.get(), arg_self, arg_self_storage);
        arg_self = arg_self_guard.get();
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addbmm)(LIBRARY_STATE arg_result, arg_beta, arg_self, AS_REAL(1), arg_batch1, arg_batch2);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      
      
    
    } else if (___out == NULL &&
          __argcount == 4 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_alpha) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_alpha)) &&
          (__tuplecount > 2 || __kw_batch1) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_batch1)) == THPTensorClass &&
          (__tuplecount > 3 || __kw_batch2) && (PyObject*)Py_TYPE((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_batch2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      real arg_alpha = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_alpha));
      THTensor* arg_batch1 = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_batch1))->cdata;
      THTensor* arg_batch2 = ((THPTensor*)(__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_batch2))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      
      if(THTensor_(nDimension)(LIBRARY_STATE arg_batch1) <= 1) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "batch1", 1 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_batch1));
      }
      int64_t arg_self_dim0_size = THTensor_(size)(LIBRARY_STATE arg_batch1, 1);
      if(THTensor_(nDimension)(LIBRARY_STATE arg_batch2) <= 2) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "batch2", 2 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_batch2));
      }
      int64_t arg_self_dim1_size = THTensor_(size)(LIBRARY_STATE arg_batch2, 2);
      THLongStoragePtr arg_self_storage(
          THLongStorage_newWithSize2(arg_self_dim0_size, arg_self_dim1_size));
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_self_storage->data, arg_self_storage->size);
      if (try_expand) {
        arg_self_guard =
        THTensor_(new)(LIBRARY_STATE_NOARGS);
        
        expand(LIBRARY_STATE arg_self_guard.get(), arg_self, arg_self_storage);
        arg_self = arg_self_guard.get();
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addbmm)(LIBRARY_STATE arg_result, AS_REAL(1), arg_self, arg_alpha, arg_batch1, arg_batch2);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      
      
    
    } else if (___out != NULL &&
          __argcount == 4 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_batch1) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_batch1)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_batch2) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_batch2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_batch1 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_batch1))->cdata;
      THTensor* arg_batch2 = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_batch2))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      
      if(THTensor_(nDimension)(LIBRARY_STATE arg_batch1) <= 1) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "batch1", 1 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_batch1));
      }
      int64_t arg_self_dim0_size = THTensor_(size)(LIBRARY_STATE arg_batch1, 1);
      if(THTensor_(nDimension)(LIBRARY_STATE arg_batch2) <= 2) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "batch2", 2 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_batch2));
      }
      int64_t arg_self_dim1_size = THTensor_(size)(LIBRARY_STATE arg_batch2, 2);
      THLongStoragePtr arg_self_storage(
          THLongStorage_newWithSize2(arg_self_dim0_size, arg_self_dim1_size));
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_self_storage->data, arg_self_storage->size);
      if (try_expand) {
        arg_self_guard =
        THTensor_(new)(LIBRARY_STATE_NOARGS);
        
        expand(LIBRARY_STATE arg_self_guard.get(), arg_self, arg_self_storage);
        arg_self = arg_self_guard.get();
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addbmm)(LIBRARY_STATE arg_result, AS_REAL(1), arg_self, AS_REAL(1), arg_batch1, arg_batch2);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      
      
    
    } else if (___out == NULL &&
          __argcount == 3 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_batch1) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_batch1)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_batch2) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_batch2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_batch1 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_batch1))->cdata;
      THTensor* arg_batch2 = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_batch2))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      
      if(THTensor_(nDimension)(LIBRARY_STATE arg_batch1) <= 1) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "batch1", 1 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_batch1));
      }
      int64_t arg_self_dim0_size = THTensor_(size)(LIBRARY_STATE arg_batch1, 1);
      if(THTensor_(nDimension)(LIBRARY_STATE arg_batch2) <= 2) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "batch2", 2 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_batch2));
      }
      int64_t arg_self_dim1_size = THTensor_(size)(LIBRARY_STATE arg_batch2, 2);
      THLongStoragePtr arg_self_storage(
          THLongStorage_newWithSize2(arg_self_dim0_size, arg_self_dim1_size));
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_self_storage->data, arg_self_storage->size);
      if (try_expand) {
        arg_self_guard =
        THTensor_(new)(LIBRARY_STATE_NOARGS);
        
        expand(LIBRARY_STATE arg_self_guard.get(), arg_self, arg_self_storage);
        arg_self = arg_self_guard.get();
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addbmm)(LIBRARY_STATE arg_result, AS_REAL(1), arg_self, AS_REAL(1), arg_batch1, arg_batch2);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      
      
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.addbmm", 4, "(" THPTensorStr " source, " THPTensorStr " batch1, " THPTensorStr " batch2, #" THPTensorStr " out)", "(" RealStr " beta, " THPTensorStr " source, " THPTensorStr " batch1, " THPTensorStr " batch2, #" THPTensorStr " out)", "(" THPTensorStr " source, " RealStr " alpha, " THPTensorStr " batch1, " THPTensorStr " batch2, #" THPTensorStr " out)", "(" RealStr " beta, " THPTensorStr " source, " RealStr " alpha, " THPTensorStr " batch1, " THPTensorStr " batch2, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_(addbmm_)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_beta = NULL;
    PyObject *__kw_alpha = NULL;
    PyObject *__kw_batch1 = NULL;
    PyObject *__kw_batch2 = NULL;
    if (kwargs) {
      __kw_beta = PyDict_GetItemString(kwargs, "beta");
      __kw_alpha = PyDict_GetItemString(kwargs, "alpha");
      __kw_batch1 = PyDict_GetItemString(kwargs, "batch1");
      __kw_batch2 = PyDict_GetItemString(kwargs, "batch2");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 4 &&
          (__tuplecount > 0 || __kw_beta) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta)) &&
          (__tuplecount > 1 || __kw_alpha) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_alpha)) &&
          (__tuplecount > 2 || __kw_batch1) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_batch1)) == THPTensorClass &&
          (__tuplecount > 3 || __kw_batch2) && (PyObject*)Py_TYPE((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_batch2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_beta = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta));
      real arg_alpha = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_alpha));
      THTensor* arg_batch1 = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_batch1))->cdata;
      THTensor* arg_batch2 = ((THPTensor*)(__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_batch2))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addbmm)(LIBRARY_STATE arg_self, arg_beta, arg_self, arg_alpha, arg_batch1, arg_batch2);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 3 &&
          (__tuplecount > 0 || __kw_beta) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta)) &&
          (__tuplecount > 1 || __kw_batch1) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_batch1)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_batch2) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_batch2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_beta = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta));
      THTensor* arg_batch1 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_batch1))->cdata;
      THTensor* arg_batch2 = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_batch2))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addbmm)(LIBRARY_STATE arg_self, arg_beta, arg_self, AS_REAL(1), arg_batch1, arg_batch2);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 3 &&
          (__tuplecount > 0 || __kw_alpha) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_alpha)) &&
          (__tuplecount > 1 || __kw_batch1) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_batch1)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_batch2) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_batch2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_alpha = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_alpha));
      THTensor* arg_batch1 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_batch1))->cdata;
      THTensor* arg_batch2 = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_batch2))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addbmm)(LIBRARY_STATE arg_self, AS_REAL(1), arg_self, arg_alpha, arg_batch1, arg_batch2);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 2 &&
          (__tuplecount > 0 || __kw_batch1) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_batch1)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_batch2) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_batch2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_batch1 = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_batch1))->cdata;
      THTensor* arg_batch2 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_batch2))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addbmm)(LIBRARY_STATE arg_self, AS_REAL(1), arg_self, AS_REAL(1), arg_batch1, arg_batch2);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "addbmm_", 4, "(" THPTensorStr " batch1, " THPTensorStr " batch2)", "(" RealStr " beta, " THPTensorStr " batch1, " THPTensorStr " batch2)", "(" RealStr " alpha, " THPTensorStr " batch1, " THPTensorStr " batch2)", "(" RealStr " beta, " RealStr " alpha, " THPTensorStr " batch1, " THPTensorStr " batch2)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_(baddbmm)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_beta = NULL;
    PyObject *__kw_alpha = NULL;
    PyObject *__kw_batch1 = NULL;
    PyObject *__kw_batch2 = NULL;
    if (kwargs) {
      __kw_beta = PyDict_GetItemString(kwargs, "beta");
      __kw_alpha = PyDict_GetItemString(kwargs, "alpha");
      __kw_batch1 = PyDict_GetItemString(kwargs, "batch1");
      __kw_batch2 = PyDict_GetItemString(kwargs, "batch2");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 5 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_beta) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta)) &&
          (__tuplecount > 1 || __kw_alpha) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_alpha)) &&
          (__tuplecount > 2 || __kw_batch1) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_batch1)) == THPTensorClass &&
          (__tuplecount > 3 || __kw_batch2) && (PyObject*)Py_TYPE((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_batch2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      real arg_beta = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta));
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_alpha = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_alpha));
      THTensor* arg_batch1 = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_batch1))->cdata;
      THTensor* arg_batch2 = ((THPTensor*)(__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_batch2))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      
      if(THTensor_(nDimension)(LIBRARY_STATE arg_batch1) <= 0) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "batch1", 0 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_batch1));
      }
      int64_t arg_self_dim0_size = THTensor_(size)(LIBRARY_STATE arg_batch1, 0);
      if(THTensor_(nDimension)(LIBRARY_STATE arg_batch1) <= 1) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "batch1", 1 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_batch1));
      }
      int64_t arg_self_dim1_size = THTensor_(size)(LIBRARY_STATE arg_batch1, 1);
      if(THTensor_(nDimension)(LIBRARY_STATE arg_batch2) <= 2) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "batch2", 2 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_batch2));
      }
      int64_t arg_self_dim2_size = THTensor_(size)(LIBRARY_STATE arg_batch2, 2);
      THLongStoragePtr arg_self_storage(
          THLongStorage_newWithSize3(arg_self_dim0_size, arg_self_dim1_size, arg_self_dim2_size));
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_self_storage->data, arg_self_storage->size);
      if (try_expand) {
        arg_self_guard =
        THTensor_(new)(LIBRARY_STATE_NOARGS);
        
        expand(LIBRARY_STATE arg_self_guard.get(), arg_self, arg_self_storage);
        arg_self = arg_self_guard.get();
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(baddbmm)(LIBRARY_STATE arg_result, arg_beta, arg_self, arg_alpha, arg_batch1, arg_batch2);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      
      
    
    } else if (___out == NULL &&
          __argcount == 4 &&
          (__tuplecount > 0 || __kw_beta) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta)) &&
          (__tuplecount > 1 || __kw_alpha) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_alpha)) &&
          (__tuplecount > 2 || __kw_batch1) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_batch1)) == THPTensorClass &&
          (__tuplecount > 3 || __kw_batch2) && (PyObject*)Py_TYPE((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_batch2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      real arg_beta = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta));
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_alpha = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_alpha));
      THTensor* arg_batch1 = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_batch1))->cdata;
      THTensor* arg_batch2 = ((THPTensor*)(__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_batch2))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      
      if(THTensor_(nDimension)(LIBRARY_STATE arg_batch1) <= 0) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "batch1", 0 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_batch1));
      }
      int64_t arg_self_dim0_size = THTensor_(size)(LIBRARY_STATE arg_batch1, 0);
      if(THTensor_(nDimension)(LIBRARY_STATE arg_batch1) <= 1) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "batch1", 1 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_batch1));
      }
      int64_t arg_self_dim1_size = THTensor_(size)(LIBRARY_STATE arg_batch1, 1);
      if(THTensor_(nDimension)(LIBRARY_STATE arg_batch2) <= 2) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "batch2", 2 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_batch2));
      }
      int64_t arg_self_dim2_size = THTensor_(size)(LIBRARY_STATE arg_batch2, 2);
      THLongStoragePtr arg_self_storage(
          THLongStorage_newWithSize3(arg_self_dim0_size, arg_self_dim1_size, arg_self_dim2_size));
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_self_storage->data, arg_self_storage->size);
      if (try_expand) {
        arg_self_guard =
        THTensor_(new)(LIBRARY_STATE_NOARGS);
        
        expand(LIBRARY_STATE arg_self_guard.get(), arg_self, arg_self_storage);
        arg_self = arg_self_guard.get();
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(baddbmm)(LIBRARY_STATE arg_result, arg_beta, arg_self, arg_alpha, arg_batch1, arg_batch2);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      
      
    
    } else if (___out != NULL &&
          __argcount == 4 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_beta) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta)) &&
          (__tuplecount > 1 || __kw_batch1) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_batch1)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_batch2) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_batch2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      real arg_beta = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta));
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_batch1 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_batch1))->cdata;
      THTensor* arg_batch2 = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_batch2))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      
      if(THTensor_(nDimension)(LIBRARY_STATE arg_batch1) <= 0) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "batch1", 0 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_batch1));
      }
      int64_t arg_self_dim0_size = THTensor_(size)(LIBRARY_STATE arg_batch1, 0);
      if(THTensor_(nDimension)(LIBRARY_STATE arg_batch1) <= 1) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "batch1", 1 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_batch1));
      }
      int64_t arg_self_dim1_size = THTensor_(size)(LIBRARY_STATE arg_batch1, 1);
      if(THTensor_(nDimension)(LIBRARY_STATE arg_batch2) <= 2) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "batch2", 2 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_batch2));
      }
      int64_t arg_self_dim2_size = THTensor_(size)(LIBRARY_STATE arg_batch2, 2);
      THLongStoragePtr arg_self_storage(
          THLongStorage_newWithSize3(arg_self_dim0_size, arg_self_dim1_size, arg_self_dim2_size));
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_self_storage->data, arg_self_storage->size);
      if (try_expand) {
        arg_self_guard =
        THTensor_(new)(LIBRARY_STATE_NOARGS);
        
        expand(LIBRARY_STATE arg_self_guard.get(), arg_self, arg_self_storage);
        arg_self = arg_self_guard.get();
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(baddbmm)(LIBRARY_STATE arg_result, arg_beta, arg_self, AS_REAL(1), arg_batch1, arg_batch2);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      
      
    
    } else if (___out != NULL &&
          __argcount == 4 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_alpha) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_alpha)) &&
          (__tuplecount > 1 || __kw_batch1) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_batch1)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_batch2) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_batch2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_alpha = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_alpha));
      THTensor* arg_batch1 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_batch1))->cdata;
      THTensor* arg_batch2 = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_batch2))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      
      if(THTensor_(nDimension)(LIBRARY_STATE arg_batch1) <= 0) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "batch1", 0 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_batch1));
      }
      int64_t arg_self_dim0_size = THTensor_(size)(LIBRARY_STATE arg_batch1, 0);
      if(THTensor_(nDimension)(LIBRARY_STATE arg_batch1) <= 1) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "batch1", 1 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_batch1));
      }
      int64_t arg_self_dim1_size = THTensor_(size)(LIBRARY_STATE arg_batch1, 1);
      if(THTensor_(nDimension)(LIBRARY_STATE arg_batch2) <= 2) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "batch2", 2 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_batch2));
      }
      int64_t arg_self_dim2_size = THTensor_(size)(LIBRARY_STATE arg_batch2, 2);
      THLongStoragePtr arg_self_storage(
          THLongStorage_newWithSize3(arg_self_dim0_size, arg_self_dim1_size, arg_self_dim2_size));
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_self_storage->data, arg_self_storage->size);
      if (try_expand) {
        arg_self_guard =
        THTensor_(new)(LIBRARY_STATE_NOARGS);
        
        expand(LIBRARY_STATE arg_self_guard.get(), arg_self, arg_self_storage);
        arg_self = arg_self_guard.get();
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(baddbmm)(LIBRARY_STATE arg_result, AS_REAL(1), arg_self, arg_alpha, arg_batch1, arg_batch2);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      
      
    
    } else if (___out == NULL &&
          __argcount == 3 &&
          (__tuplecount > 0 || __kw_beta) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta)) &&
          (__tuplecount > 1 || __kw_batch1) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_batch1)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_batch2) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_batch2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      real arg_beta = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta));
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_batch1 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_batch1))->cdata;
      THTensor* arg_batch2 = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_batch2))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      
      if(THTensor_(nDimension)(LIBRARY_STATE arg_batch1) <= 0) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "batch1", 0 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_batch1));
      }
      int64_t arg_self_dim0_size = THTensor_(size)(LIBRARY_STATE arg_batch1, 0);
      if(THTensor_(nDimension)(LIBRARY_STATE arg_batch1) <= 1) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "batch1", 1 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_batch1));
      }
      int64_t arg_self_dim1_size = THTensor_(size)(LIBRARY_STATE arg_batch1, 1);
      if(THTensor_(nDimension)(LIBRARY_STATE arg_batch2) <= 2) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "batch2", 2 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_batch2));
      }
      int64_t arg_self_dim2_size = THTensor_(size)(LIBRARY_STATE arg_batch2, 2);
      THLongStoragePtr arg_self_storage(
          THLongStorage_newWithSize3(arg_self_dim0_size, arg_self_dim1_size, arg_self_dim2_size));
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_self_storage->data, arg_self_storage->size);
      if (try_expand) {
        arg_self_guard =
        THTensor_(new)(LIBRARY_STATE_NOARGS);
        
        expand(LIBRARY_STATE arg_self_guard.get(), arg_self, arg_self_storage);
        arg_self = arg_self_guard.get();
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(baddbmm)(LIBRARY_STATE arg_result, arg_beta, arg_self, AS_REAL(1), arg_batch1, arg_batch2);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      
      
    
    } else if (___out == NULL &&
          __argcount == 3 &&
          (__tuplecount > 0 || __kw_alpha) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_alpha)) &&
          (__tuplecount > 1 || __kw_batch1) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_batch1)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_batch2) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_batch2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_alpha = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_alpha));
      THTensor* arg_batch1 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_batch1))->cdata;
      THTensor* arg_batch2 = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_batch2))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      
      if(THTensor_(nDimension)(LIBRARY_STATE arg_batch1) <= 0) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "batch1", 0 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_batch1));
      }
      int64_t arg_self_dim0_size = THTensor_(size)(LIBRARY_STATE arg_batch1, 0);
      if(THTensor_(nDimension)(LIBRARY_STATE arg_batch1) <= 1) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "batch1", 1 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_batch1));
      }
      int64_t arg_self_dim1_size = THTensor_(size)(LIBRARY_STATE arg_batch1, 1);
      if(THTensor_(nDimension)(LIBRARY_STATE arg_batch2) <= 2) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "batch2", 2 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_batch2));
      }
      int64_t arg_self_dim2_size = THTensor_(size)(LIBRARY_STATE arg_batch2, 2);
      THLongStoragePtr arg_self_storage(
          THLongStorage_newWithSize3(arg_self_dim0_size, arg_self_dim1_size, arg_self_dim2_size));
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_self_storage->data, arg_self_storage->size);
      if (try_expand) {
        arg_self_guard =
        THTensor_(new)(LIBRARY_STATE_NOARGS);
        
        expand(LIBRARY_STATE arg_self_guard.get(), arg_self, arg_self_storage);
        arg_self = arg_self_guard.get();
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(baddbmm)(LIBRARY_STATE arg_result, AS_REAL(1), arg_self, arg_alpha, arg_batch1, arg_batch2);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      
      
    
    } else if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_batch1) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_batch1)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_batch2) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_batch2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_batch1 = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_batch1))->cdata;
      THTensor* arg_batch2 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_batch2))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      
      if(THTensor_(nDimension)(LIBRARY_STATE arg_batch1) <= 0) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "batch1", 0 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_batch1));
      }
      int64_t arg_self_dim0_size = THTensor_(size)(LIBRARY_STATE arg_batch1, 0);
      if(THTensor_(nDimension)(LIBRARY_STATE arg_batch1) <= 1) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "batch1", 1 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_batch1));
      }
      int64_t arg_self_dim1_size = THTensor_(size)(LIBRARY_STATE arg_batch1, 1);
      if(THTensor_(nDimension)(LIBRARY_STATE arg_batch2) <= 2) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "batch2", 2 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_batch2));
      }
      int64_t arg_self_dim2_size = THTensor_(size)(LIBRARY_STATE arg_batch2, 2);
      THLongStoragePtr arg_self_storage(
          THLongStorage_newWithSize3(arg_self_dim0_size, arg_self_dim1_size, arg_self_dim2_size));
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_self_storage->data, arg_self_storage->size);
      if (try_expand) {
        arg_self_guard =
        THTensor_(new)(LIBRARY_STATE_NOARGS);
        
        expand(LIBRARY_STATE arg_self_guard.get(), arg_self, arg_self_storage);
        arg_self = arg_self_guard.get();
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(baddbmm)(LIBRARY_STATE arg_result, AS_REAL(1), arg_self, AS_REAL(1), arg_batch1, arg_batch2);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      
      
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_batch1) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_batch1)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_batch2) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_batch2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_batch1 = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_batch1))->cdata;
      THTensor* arg_batch2 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_batch2))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      
      if(THTensor_(nDimension)(LIBRARY_STATE arg_batch1) <= 0) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "batch1", 0 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_batch1));
      }
      int64_t arg_self_dim0_size = THTensor_(size)(LIBRARY_STATE arg_batch1, 0);
      if(THTensor_(nDimension)(LIBRARY_STATE arg_batch1) <= 1) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "batch1", 1 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_batch1));
      }
      int64_t arg_self_dim1_size = THTensor_(size)(LIBRARY_STATE arg_batch1, 1);
      if(THTensor_(nDimension)(LIBRARY_STATE arg_batch2) <= 2) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "batch2", 2 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_batch2));
      }
      int64_t arg_self_dim2_size = THTensor_(size)(LIBRARY_STATE arg_batch2, 2);
      THLongStoragePtr arg_self_storage(
          THLongStorage_newWithSize3(arg_self_dim0_size, arg_self_dim1_size, arg_self_dim2_size));
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_self_storage->data, arg_self_storage->size);
      if (try_expand) {
        arg_self_guard =
        THTensor_(new)(LIBRARY_STATE_NOARGS);
        
        expand(LIBRARY_STATE arg_self_guard.get(), arg_self, arg_self_storage);
        arg_self = arg_self_guard.get();
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(baddbmm)(LIBRARY_STATE arg_result, AS_REAL(1), arg_self, AS_REAL(1), arg_batch1, arg_batch2);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      
      
    
    }

    THPUtils_invalidArguments(args, kwargs, "baddbmm", 4, "(" THPTensorStr " batch1, " THPTensorStr " batch2, #" THPTensorStr " out)", "(" RealStr " beta, " THPTensorStr " batch1, " THPTensorStr " batch2, #" THPTensorStr " out)", "(" RealStr " alpha, " THPTensorStr " batch1, " THPTensorStr " batch2, #" THPTensorStr " out)", "(" RealStr " beta, " RealStr " alpha, " THPTensorStr " batch1, " THPTensorStr " batch2, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_stateless_(baddbmm)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_beta = NULL;
    PyObject *__kw_source = NULL;
    PyObject *__kw_alpha = NULL;
    PyObject *__kw_batch1 = NULL;
    PyObject *__kw_batch2 = NULL;
    if (kwargs) {
      __kw_beta = PyDict_GetItemString(kwargs, "beta");
      __kw_source = PyDict_GetItemString(kwargs, "source");
      __kw_alpha = PyDict_GetItemString(kwargs, "alpha");
      __kw_batch1 = PyDict_GetItemString(kwargs, "batch1");
      __kw_batch2 = PyDict_GetItemString(kwargs, "batch2");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 6 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_beta) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta)) &&
          (__tuplecount > 1 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_alpha) && THPUtils_(checkReal)((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_alpha)) &&
          (__tuplecount > 3 || __kw_batch1) && (PyObject*)Py_TYPE((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_batch1)) == THPTensorClass &&
          (__tuplecount > 4 || __kw_batch2) && (PyObject*)Py_TYPE((__tuplecount > 4 ? PyTuple_GET_ITEM(args, 4) : __kw_batch2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      real arg_beta = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta));
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_source))->cdata;
      real arg_alpha = THPUtils_(unpackReal)((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_alpha));
      THTensor* arg_batch1 = ((THPTensor*)(__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_batch1))->cdata;
      THTensor* arg_batch2 = ((THPTensor*)(__tuplecount > 4 ? PyTuple_GET_ITEM(args, 4) : __kw_batch2))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      
      if(THTensor_(nDimension)(LIBRARY_STATE arg_batch1) <= 0) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "batch1", 0 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_batch1));
      }
      int64_t arg_self_dim0_size = THTensor_(size)(LIBRARY_STATE arg_batch1, 0);
      if(THTensor_(nDimension)(LIBRARY_STATE arg_batch1) <= 1) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "batch1", 1 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_batch1));
      }
      int64_t arg_self_dim1_size = THTensor_(size)(LIBRARY_STATE arg_batch1, 1);
      if(THTensor_(nDimension)(LIBRARY_STATE arg_batch2) <= 2) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "batch2", 2 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_batch2));
      }
      int64_t arg_self_dim2_size = THTensor_(size)(LIBRARY_STATE arg_batch2, 2);
      THLongStoragePtr arg_self_storage(
          THLongStorage_newWithSize3(arg_self_dim0_size, arg_self_dim1_size, arg_self_dim2_size));
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_self_storage->data, arg_self_storage->size);
      if (try_expand) {
        arg_self_guard =
        THTensor_(new)(LIBRARY_STATE_NOARGS);
        
        expand(LIBRARY_STATE arg_self_guard.get(), arg_self, arg_self_storage);
        arg_self = arg_self_guard.get();
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(baddbmm)(LIBRARY_STATE arg_result, arg_beta, arg_self, arg_alpha, arg_batch1, arg_batch2);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      
      
    
    } else if (___out == NULL &&
          __argcount == 5 &&
          (__tuplecount > 0 || __kw_beta) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta)) &&
          (__tuplecount > 1 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_alpha) && THPUtils_(checkReal)((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_alpha)) &&
          (__tuplecount > 3 || __kw_batch1) && (PyObject*)Py_TYPE((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_batch1)) == THPTensorClass &&
          (__tuplecount > 4 || __kw_batch2) && (PyObject*)Py_TYPE((__tuplecount > 4 ? PyTuple_GET_ITEM(args, 4) : __kw_batch2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      real arg_beta = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta));
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_source))->cdata;
      real arg_alpha = THPUtils_(unpackReal)((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_alpha));
      THTensor* arg_batch1 = ((THPTensor*)(__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_batch1))->cdata;
      THTensor* arg_batch2 = ((THPTensor*)(__tuplecount > 4 ? PyTuple_GET_ITEM(args, 4) : __kw_batch2))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      
      if(THTensor_(nDimension)(LIBRARY_STATE arg_batch1) <= 0) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "batch1", 0 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_batch1));
      }
      int64_t arg_self_dim0_size = THTensor_(size)(LIBRARY_STATE arg_batch1, 0);
      if(THTensor_(nDimension)(LIBRARY_STATE arg_batch1) <= 1) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "batch1", 1 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_batch1));
      }
      int64_t arg_self_dim1_size = THTensor_(size)(LIBRARY_STATE arg_batch1, 1);
      if(THTensor_(nDimension)(LIBRARY_STATE arg_batch2) <= 2) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "batch2", 2 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_batch2));
      }
      int64_t arg_self_dim2_size = THTensor_(size)(LIBRARY_STATE arg_batch2, 2);
      THLongStoragePtr arg_self_storage(
          THLongStorage_newWithSize3(arg_self_dim0_size, arg_self_dim1_size, arg_self_dim2_size));
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_self_storage->data, arg_self_storage->size);
      if (try_expand) {
        arg_self_guard =
        THTensor_(new)(LIBRARY_STATE_NOARGS);
        
        expand(LIBRARY_STATE arg_self_guard.get(), arg_self, arg_self_storage);
        arg_self = arg_self_guard.get();
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(baddbmm)(LIBRARY_STATE arg_result, arg_beta, arg_self, arg_alpha, arg_batch1, arg_batch2);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      
      
    
    } else if (___out != NULL &&
          __argcount == 5 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_beta) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta)) &&
          (__tuplecount > 1 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_batch1) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_batch1)) == THPTensorClass &&
          (__tuplecount > 3 || __kw_batch2) && (PyObject*)Py_TYPE((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_batch2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      real arg_beta = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta));
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_source))->cdata;
      THTensor* arg_batch1 = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_batch1))->cdata;
      THTensor* arg_batch2 = ((THPTensor*)(__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_batch2))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      
      if(THTensor_(nDimension)(LIBRARY_STATE arg_batch1) <= 0) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "batch1", 0 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_batch1));
      }
      int64_t arg_self_dim0_size = THTensor_(size)(LIBRARY_STATE arg_batch1, 0);
      if(THTensor_(nDimension)(LIBRARY_STATE arg_batch1) <= 1) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "batch1", 1 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_batch1));
      }
      int64_t arg_self_dim1_size = THTensor_(size)(LIBRARY_STATE arg_batch1, 1);
      if(THTensor_(nDimension)(LIBRARY_STATE arg_batch2) <= 2) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "batch2", 2 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_batch2));
      }
      int64_t arg_self_dim2_size = THTensor_(size)(LIBRARY_STATE arg_batch2, 2);
      THLongStoragePtr arg_self_storage(
          THLongStorage_newWithSize3(arg_self_dim0_size, arg_self_dim1_size, arg_self_dim2_size));
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_self_storage->data, arg_self_storage->size);
      if (try_expand) {
        arg_self_guard =
        THTensor_(new)(LIBRARY_STATE_NOARGS);
        
        expand(LIBRARY_STATE arg_self_guard.get(), arg_self, arg_self_storage);
        arg_self = arg_self_guard.get();
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(baddbmm)(LIBRARY_STATE arg_result, arg_beta, arg_self, AS_REAL(1), arg_batch1, arg_batch2);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      
      
    
    } else if (___out != NULL &&
          __argcount == 5 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_alpha) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_alpha)) &&
          (__tuplecount > 2 || __kw_batch1) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_batch1)) == THPTensorClass &&
          (__tuplecount > 3 || __kw_batch2) && (PyObject*)Py_TYPE((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_batch2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      real arg_alpha = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_alpha));
      THTensor* arg_batch1 = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_batch1))->cdata;
      THTensor* arg_batch2 = ((THPTensor*)(__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_batch2))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      
      if(THTensor_(nDimension)(LIBRARY_STATE arg_batch1) <= 0) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "batch1", 0 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_batch1));
      }
      int64_t arg_self_dim0_size = THTensor_(size)(LIBRARY_STATE arg_batch1, 0);
      if(THTensor_(nDimension)(LIBRARY_STATE arg_batch1) <= 1) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "batch1", 1 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_batch1));
      }
      int64_t arg_self_dim1_size = THTensor_(size)(LIBRARY_STATE arg_batch1, 1);
      if(THTensor_(nDimension)(LIBRARY_STATE arg_batch2) <= 2) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "batch2", 2 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_batch2));
      }
      int64_t arg_self_dim2_size = THTensor_(size)(LIBRARY_STATE arg_batch2, 2);
      THLongStoragePtr arg_self_storage(
          THLongStorage_newWithSize3(arg_self_dim0_size, arg_self_dim1_size, arg_self_dim2_size));
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_self_storage->data, arg_self_storage->size);
      if (try_expand) {
        arg_self_guard =
        THTensor_(new)(LIBRARY_STATE_NOARGS);
        
        expand(LIBRARY_STATE arg_self_guard.get(), arg_self, arg_self_storage);
        arg_self = arg_self_guard.get();
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(baddbmm)(LIBRARY_STATE arg_result, AS_REAL(1), arg_self, arg_alpha, arg_batch1, arg_batch2);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      
      
    
    } else if (___out == NULL &&
          __argcount == 4 &&
          (__tuplecount > 0 || __kw_beta) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta)) &&
          (__tuplecount > 1 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_batch1) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_batch1)) == THPTensorClass &&
          (__tuplecount > 3 || __kw_batch2) && (PyObject*)Py_TYPE((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_batch2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      real arg_beta = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta));
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_source))->cdata;
      THTensor* arg_batch1 = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_batch1))->cdata;
      THTensor* arg_batch2 = ((THPTensor*)(__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_batch2))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      
      if(THTensor_(nDimension)(LIBRARY_STATE arg_batch1) <= 0) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "batch1", 0 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_batch1));
      }
      int64_t arg_self_dim0_size = THTensor_(size)(LIBRARY_STATE arg_batch1, 0);
      if(THTensor_(nDimension)(LIBRARY_STATE arg_batch1) <= 1) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "batch1", 1 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_batch1));
      }
      int64_t arg_self_dim1_size = THTensor_(size)(LIBRARY_STATE arg_batch1, 1);
      if(THTensor_(nDimension)(LIBRARY_STATE arg_batch2) <= 2) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "batch2", 2 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_batch2));
      }
      int64_t arg_self_dim2_size = THTensor_(size)(LIBRARY_STATE arg_batch2, 2);
      THLongStoragePtr arg_self_storage(
          THLongStorage_newWithSize3(arg_self_dim0_size, arg_self_dim1_size, arg_self_dim2_size));
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_self_storage->data, arg_self_storage->size);
      if (try_expand) {
        arg_self_guard =
        THTensor_(new)(LIBRARY_STATE_NOARGS);
        
        expand(LIBRARY_STATE arg_self_guard.get(), arg_self, arg_self_storage);
        arg_self = arg_self_guard.get();
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(baddbmm)(LIBRARY_STATE arg_result, arg_beta, arg_self, AS_REAL(1), arg_batch1, arg_batch2);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      
      
    
    } else if (___out == NULL &&
          __argcount == 4 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_alpha) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_alpha)) &&
          (__tuplecount > 2 || __kw_batch1) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_batch1)) == THPTensorClass &&
          (__tuplecount > 3 || __kw_batch2) && (PyObject*)Py_TYPE((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_batch2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      real arg_alpha = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_alpha));
      THTensor* arg_batch1 = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_batch1))->cdata;
      THTensor* arg_batch2 = ((THPTensor*)(__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_batch2))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      
      if(THTensor_(nDimension)(LIBRARY_STATE arg_batch1) <= 0) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "batch1", 0 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_batch1));
      }
      int64_t arg_self_dim0_size = THTensor_(size)(LIBRARY_STATE arg_batch1, 0);
      if(THTensor_(nDimension)(LIBRARY_STATE arg_batch1) <= 1) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "batch1", 1 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_batch1));
      }
      int64_t arg_self_dim1_size = THTensor_(size)(LIBRARY_STATE arg_batch1, 1);
      if(THTensor_(nDimension)(LIBRARY_STATE arg_batch2) <= 2) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "batch2", 2 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_batch2));
      }
      int64_t arg_self_dim2_size = THTensor_(size)(LIBRARY_STATE arg_batch2, 2);
      THLongStoragePtr arg_self_storage(
          THLongStorage_newWithSize3(arg_self_dim0_size, arg_self_dim1_size, arg_self_dim2_size));
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_self_storage->data, arg_self_storage->size);
      if (try_expand) {
        arg_self_guard =
        THTensor_(new)(LIBRARY_STATE_NOARGS);
        
        expand(LIBRARY_STATE arg_self_guard.get(), arg_self, arg_self_storage);
        arg_self = arg_self_guard.get();
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(baddbmm)(LIBRARY_STATE arg_result, AS_REAL(1), arg_self, arg_alpha, arg_batch1, arg_batch2);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      
      
    
    } else if (___out != NULL &&
          __argcount == 4 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_batch1) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_batch1)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_batch2) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_batch2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_batch1 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_batch1))->cdata;
      THTensor* arg_batch2 = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_batch2))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      
      if(THTensor_(nDimension)(LIBRARY_STATE arg_batch1) <= 0) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "batch1", 0 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_batch1));
      }
      int64_t arg_self_dim0_size = THTensor_(size)(LIBRARY_STATE arg_batch1, 0);
      if(THTensor_(nDimension)(LIBRARY_STATE arg_batch1) <= 1) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "batch1", 1 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_batch1));
      }
      int64_t arg_self_dim1_size = THTensor_(size)(LIBRARY_STATE arg_batch1, 1);
      if(THTensor_(nDimension)(LIBRARY_STATE arg_batch2) <= 2) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "batch2", 2 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_batch2));
      }
      int64_t arg_self_dim2_size = THTensor_(size)(LIBRARY_STATE arg_batch2, 2);
      THLongStoragePtr arg_self_storage(
          THLongStorage_newWithSize3(arg_self_dim0_size, arg_self_dim1_size, arg_self_dim2_size));
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_self_storage->data, arg_self_storage->size);
      if (try_expand) {
        arg_self_guard =
        THTensor_(new)(LIBRARY_STATE_NOARGS);
        
        expand(LIBRARY_STATE arg_self_guard.get(), arg_self, arg_self_storage);
        arg_self = arg_self_guard.get();
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(baddbmm)(LIBRARY_STATE arg_result, AS_REAL(1), arg_self, AS_REAL(1), arg_batch1, arg_batch2);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      
      
    
    } else if (___out == NULL &&
          __argcount == 3 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_batch1) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_batch1)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_batch2) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_batch2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_batch1 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_batch1))->cdata;
      THTensor* arg_batch2 = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_batch2))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      
      if(THTensor_(nDimension)(LIBRARY_STATE arg_batch1) <= 0) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "batch1", 0 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_batch1));
      }
      int64_t arg_self_dim0_size = THTensor_(size)(LIBRARY_STATE arg_batch1, 0);
      if(THTensor_(nDimension)(LIBRARY_STATE arg_batch1) <= 1) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "batch1", 1 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_batch1));
      }
      int64_t arg_self_dim1_size = THTensor_(size)(LIBRARY_STATE arg_batch1, 1);
      if(THTensor_(nDimension)(LIBRARY_STATE arg_batch2) <= 2) {
        THError("Argument %s requires at least %d dimensions, but only has %d",
            "batch2", 2 + 1, THTensor_(nDimension)(LIBRARY_STATE arg_batch2));
      }
      int64_t arg_self_dim2_size = THTensor_(size)(LIBRARY_STATE arg_batch2, 2);
      THLongStoragePtr arg_self_storage(
          THLongStorage_newWithSize3(arg_self_dim0_size, arg_self_dim1_size, arg_self_dim2_size));
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_self_storage->data, arg_self_storage->size);
      if (try_expand) {
        arg_self_guard =
        THTensor_(new)(LIBRARY_STATE_NOARGS);
        
        expand(LIBRARY_STATE arg_self_guard.get(), arg_self, arg_self_storage);
        arg_self = arg_self_guard.get();
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(baddbmm)(LIBRARY_STATE arg_result, AS_REAL(1), arg_self, AS_REAL(1), arg_batch1, arg_batch2);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      
      
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.baddbmm", 4, "(" THPTensorStr " source, " THPTensorStr " batch1, " THPTensorStr " batch2, #" THPTensorStr " out)", "(" RealStr " beta, " THPTensorStr " source, " THPTensorStr " batch1, " THPTensorStr " batch2, #" THPTensorStr " out)", "(" THPTensorStr " source, " RealStr " alpha, " THPTensorStr " batch1, " THPTensorStr " batch2, #" THPTensorStr " out)", "(" RealStr " beta, " THPTensorStr " source, " RealStr " alpha, " THPTensorStr " batch1, " THPTensorStr " batch2, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_(baddbmm_)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_beta = NULL;
    PyObject *__kw_alpha = NULL;
    PyObject *__kw_batch1 = NULL;
    PyObject *__kw_batch2 = NULL;
    if (kwargs) {
      __kw_beta = PyDict_GetItemString(kwargs, "beta");
      __kw_alpha = PyDict_GetItemString(kwargs, "alpha");
      __kw_batch1 = PyDict_GetItemString(kwargs, "batch1");
      __kw_batch2 = PyDict_GetItemString(kwargs, "batch2");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 4 &&
          (__tuplecount > 0 || __kw_beta) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta)) &&
          (__tuplecount > 1 || __kw_alpha) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_alpha)) &&
          (__tuplecount > 2 || __kw_batch1) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_batch1)) == THPTensorClass &&
          (__tuplecount > 3 || __kw_batch2) && (PyObject*)Py_TYPE((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_batch2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_beta = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta));
      real arg_alpha = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_alpha));
      THTensor* arg_batch1 = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_batch1))->cdata;
      THTensor* arg_batch2 = ((THPTensor*)(__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_batch2))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(baddbmm)(LIBRARY_STATE arg_self, arg_beta, arg_self, arg_alpha, arg_batch1, arg_batch2);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)(self);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 3 &&
          (__tuplecount > 0 || __kw_beta) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta)) &&
          (__tuplecount > 1 || __kw_batch1) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_batch1)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_batch2) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_batch2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_beta = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta));
      THTensor* arg_batch1 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_batch1))->cdata;
      THTensor* arg_batch2 = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_batch2))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(baddbmm)(LIBRARY_STATE arg_self, arg_beta, arg_self, AS_REAL(1), arg_batch1, arg_batch2);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)(self);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 3 &&
          (__tuplecount > 0 || __kw_alpha) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_alpha)) &&
          (__tuplecount > 1 || __kw_batch1) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_batch1)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_batch2) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_batch2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_alpha = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_alpha));
      THTensor* arg_batch1 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_batch1))->cdata;
      THTensor* arg_batch2 = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_batch2))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(baddbmm)(LIBRARY_STATE arg_self, AS_REAL(1), arg_self, arg_alpha, arg_batch1, arg_batch2);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)(self);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 2 &&
          (__tuplecount > 0 || __kw_batch1) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_batch1)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_batch2) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_batch2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_batch1 = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_batch1))->cdata;
      THTensor* arg_batch2 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_batch2))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(baddbmm)(LIBRARY_STATE arg_self, AS_REAL(1), arg_self, AS_REAL(1), arg_batch1, arg_batch2);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)(self);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "baddbmm_", 4, "(" THPTensorStr " batch1, " THPTensorStr " batch2)", "(" RealStr " beta, " THPTensorStr " batch1, " THPTensorStr " batch2)", "(" RealStr " alpha, " THPTensorStr " batch1, " THPTensorStr " batch2)", "(" RealStr " beta, " RealStr " alpha, " THPTensorStr " batch1, " THPTensorStr " batch2)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_(addcmul)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_value = NULL;
    PyObject *__kw_tensor1 = NULL;
    PyObject *__kw_tensor2 = NULL;
    if (kwargs) {
      __kw_value = PyDict_GetItemString(kwargs, "value");
      __kw_tensor1 = PyDict_GetItemString(kwargs, "tensor1");
      __kw_tensor2 = PyDict_GetItemString(kwargs, "tensor2");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 4 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value)) &&
          (__tuplecount > 1 || __kw_tensor1) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_tensor1)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_tensor2) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_tensor2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value));
      THTensor* arg_tensor1 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_tensor1))->cdata;
      THTensor* arg_tensor2 = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_tensor2))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_tensor1_save = arg_tensor1;
      THTensorPtr arg_tensor1_guard(nullptr);
      THTensor *arg_tensor2_save = arg_tensor2;
      THTensorPtr arg_tensor2_guard(nullptr);
      
      bool try_expand = !(THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
              arg_tensor1->size, arg_tensor1->nDimension) &&
          THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
              arg_tensor2->size, arg_tensor2->nDimension));
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_tensor1_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_tensor2_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace3(LIBRARY_STATE arg_self_guard.get(),
              arg_tensor1_guard.get(), arg_tensor2_guard.get(),
              arg_self, arg_tensor1, arg_tensor2,
              "self", "tensor1", "tensor2", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_tensor1 = arg_tensor1_guard.get();
          arg_tensor2 = arg_tensor2_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addcmul)(LIBRARY_STATE arg_result, arg_self, arg_value, arg_tensor1, arg_tensor2);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_tensor1 = arg_tensor1_save;
      arg_tensor2 = arg_tensor2_save;
      
      
    
    } else if (___out == NULL &&
          __argcount == 3 &&
          (__tuplecount > 0 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value)) &&
          (__tuplecount > 1 || __kw_tensor1) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_tensor1)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_tensor2) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_tensor2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value));
      THTensor* arg_tensor1 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_tensor1))->cdata;
      THTensor* arg_tensor2 = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_tensor2))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_tensor1_save = arg_tensor1;
      THTensorPtr arg_tensor1_guard(nullptr);
      THTensor *arg_tensor2_save = arg_tensor2;
      THTensorPtr arg_tensor2_guard(nullptr);
      
      bool try_expand = !(THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
              arg_tensor1->size, arg_tensor1->nDimension) &&
          THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
              arg_tensor2->size, arg_tensor2->nDimension));
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_tensor1_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_tensor2_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace3(LIBRARY_STATE arg_self_guard.get(),
              arg_tensor1_guard.get(), arg_tensor2_guard.get(),
              arg_self, arg_tensor1, arg_tensor2,
              "self", "tensor1", "tensor2", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_tensor1 = arg_tensor1_guard.get();
          arg_tensor2 = arg_tensor2_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addcmul)(LIBRARY_STATE arg_result, arg_self, arg_value, arg_tensor1, arg_tensor2);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_tensor1 = arg_tensor1_save;
      arg_tensor2 = arg_tensor2_save;
      
      
    
    } else if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_tensor1) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor1)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_tensor2) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_tensor2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_tensor1 = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor1))->cdata;
      THTensor* arg_tensor2 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_tensor2))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_tensor1_save = arg_tensor1;
      THTensorPtr arg_tensor1_guard(nullptr);
      THTensor *arg_tensor2_save = arg_tensor2;
      THTensorPtr arg_tensor2_guard(nullptr);
      
      bool try_expand = !(THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
              arg_tensor1->size, arg_tensor1->nDimension) &&
          THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
              arg_tensor2->size, arg_tensor2->nDimension));
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_tensor1_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_tensor2_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace3(LIBRARY_STATE arg_self_guard.get(),
              arg_tensor1_guard.get(), arg_tensor2_guard.get(),
              arg_self, arg_tensor1, arg_tensor2,
              "self", "tensor1", "tensor2", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_tensor1 = arg_tensor1_guard.get();
          arg_tensor2 = arg_tensor2_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addcmul)(LIBRARY_STATE arg_result, arg_self, AS_REAL(1), arg_tensor1, arg_tensor2);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_tensor1 = arg_tensor1_save;
      arg_tensor2 = arg_tensor2_save;
      
      
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_tensor1) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor1)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_tensor2) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_tensor2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_tensor1 = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor1))->cdata;
      THTensor* arg_tensor2 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_tensor2))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_tensor1_save = arg_tensor1;
      THTensorPtr arg_tensor1_guard(nullptr);
      THTensor *arg_tensor2_save = arg_tensor2;
      THTensorPtr arg_tensor2_guard(nullptr);
      
      bool try_expand = !(THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
              arg_tensor1->size, arg_tensor1->nDimension) &&
          THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
              arg_tensor2->size, arg_tensor2->nDimension));
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_tensor1_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_tensor2_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace3(LIBRARY_STATE arg_self_guard.get(),
              arg_tensor1_guard.get(), arg_tensor2_guard.get(),
              arg_self, arg_tensor1, arg_tensor2,
              "self", "tensor1", "tensor2", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_tensor1 = arg_tensor1_guard.get();
          arg_tensor2 = arg_tensor2_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addcmul)(LIBRARY_STATE arg_result, arg_self, AS_REAL(1), arg_tensor1, arg_tensor2);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_tensor1 = arg_tensor1_save;
      arg_tensor2 = arg_tensor2_save;
      
      
    
    }

    THPUtils_invalidArguments(args, kwargs, "addcmul", 2, "(" THPTensorStr " tensor1, " THPTensorStr " tensor2, #" THPTensorStr " out)", "(" RealStr " value, " THPTensorStr " tensor1, " THPTensorStr " tensor2, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_stateless_(addcmul)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    PyObject *__kw_value = NULL;
    PyObject *__kw_tensor1 = NULL;
    PyObject *__kw_tensor2 = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
      __kw_value = PyDict_GetItemString(kwargs, "value");
      __kw_tensor1 = PyDict_GetItemString(kwargs, "tensor1");
      __kw_tensor2 = PyDict_GetItemString(kwargs, "tensor2");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 5 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value)) &&
          (__tuplecount > 2 || __kw_tensor1) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_tensor1)) == THPTensorClass &&
          (__tuplecount > 3 || __kw_tensor2) && (PyObject*)Py_TYPE((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_tensor2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value));
      THTensor* arg_tensor1 = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_tensor1))->cdata;
      THTensor* arg_tensor2 = ((THPTensor*)(__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_tensor2))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_tensor1_save = arg_tensor1;
      THTensorPtr arg_tensor1_guard(nullptr);
      THTensor *arg_tensor2_save = arg_tensor2;
      THTensorPtr arg_tensor2_guard(nullptr);
      
      bool try_expand = !(THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
              arg_tensor1->size, arg_tensor1->nDimension) &&
          THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
              arg_tensor2->size, arg_tensor2->nDimension));
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_tensor1_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_tensor2_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace3(LIBRARY_STATE arg_self_guard.get(),
              arg_tensor1_guard.get(), arg_tensor2_guard.get(),
              arg_self, arg_tensor1, arg_tensor2,
              "self", "tensor1", "tensor2", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_tensor1 = arg_tensor1_guard.get();
          arg_tensor2 = arg_tensor2_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addcmul)(LIBRARY_STATE arg_result, arg_self, arg_value, arg_tensor1, arg_tensor2);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_tensor1 = arg_tensor1_save;
      arg_tensor2 = arg_tensor2_save;
      
      
    
    } else if (___out == NULL &&
          __argcount == 4 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value)) &&
          (__tuplecount > 2 || __kw_tensor1) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_tensor1)) == THPTensorClass &&
          (__tuplecount > 3 || __kw_tensor2) && (PyObject*)Py_TYPE((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_tensor2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value));
      THTensor* arg_tensor1 = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_tensor1))->cdata;
      THTensor* arg_tensor2 = ((THPTensor*)(__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_tensor2))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_tensor1_save = arg_tensor1;
      THTensorPtr arg_tensor1_guard(nullptr);
      THTensor *arg_tensor2_save = arg_tensor2;
      THTensorPtr arg_tensor2_guard(nullptr);
      
      bool try_expand = !(THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
              arg_tensor1->size, arg_tensor1->nDimension) &&
          THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
              arg_tensor2->size, arg_tensor2->nDimension));
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_tensor1_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_tensor2_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace3(LIBRARY_STATE arg_self_guard.get(),
              arg_tensor1_guard.get(), arg_tensor2_guard.get(),
              arg_self, arg_tensor1, arg_tensor2,
              "self", "tensor1", "tensor2", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_tensor1 = arg_tensor1_guard.get();
          arg_tensor2 = arg_tensor2_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addcmul)(LIBRARY_STATE arg_result, arg_self, arg_value, arg_tensor1, arg_tensor2);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_tensor1 = arg_tensor1_save;
      arg_tensor2 = arg_tensor2_save;
      
      
    
    } else if (___out != NULL &&
          __argcount == 4 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_tensor1) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_tensor1)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_tensor2) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_tensor2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_tensor1 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_tensor1))->cdata;
      THTensor* arg_tensor2 = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_tensor2))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_tensor1_save = arg_tensor1;
      THTensorPtr arg_tensor1_guard(nullptr);
      THTensor *arg_tensor2_save = arg_tensor2;
      THTensorPtr arg_tensor2_guard(nullptr);
      
      bool try_expand = !(THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
              arg_tensor1->size, arg_tensor1->nDimension) &&
          THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
              arg_tensor2->size, arg_tensor2->nDimension));
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_tensor1_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_tensor2_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace3(LIBRARY_STATE arg_self_guard.get(),
              arg_tensor1_guard.get(), arg_tensor2_guard.get(),
              arg_self, arg_tensor1, arg_tensor2,
              "self", "tensor1", "tensor2", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_tensor1 = arg_tensor1_guard.get();
          arg_tensor2 = arg_tensor2_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addcmul)(LIBRARY_STATE arg_result, arg_self, AS_REAL(1), arg_tensor1, arg_tensor2);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_tensor1 = arg_tensor1_save;
      arg_tensor2 = arg_tensor2_save;
      
      
    
    } else if (___out == NULL &&
          __argcount == 3 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_tensor1) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_tensor1)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_tensor2) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_tensor2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_tensor1 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_tensor1))->cdata;
      THTensor* arg_tensor2 = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_tensor2))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_tensor1_save = arg_tensor1;
      THTensorPtr arg_tensor1_guard(nullptr);
      THTensor *arg_tensor2_save = arg_tensor2;
      THTensorPtr arg_tensor2_guard(nullptr);
      
      bool try_expand = !(THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
              arg_tensor1->size, arg_tensor1->nDimension) &&
          THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
              arg_tensor2->size, arg_tensor2->nDimension));
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_tensor1_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_tensor2_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace3(LIBRARY_STATE arg_self_guard.get(),
              arg_tensor1_guard.get(), arg_tensor2_guard.get(),
              arg_self, arg_tensor1, arg_tensor2,
              "self", "tensor1", "tensor2", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_tensor1 = arg_tensor1_guard.get();
          arg_tensor2 = arg_tensor2_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addcmul)(LIBRARY_STATE arg_result, arg_self, AS_REAL(1), arg_tensor1, arg_tensor2);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_tensor1 = arg_tensor1_save;
      arg_tensor2 = arg_tensor2_save;
      
      
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.addcmul", 2, "(" THPTensorStr " source, " THPTensorStr " tensor1, " THPTensorStr " tensor2, #" THPTensorStr " out)", "(" THPTensorStr " source, " RealStr " value, " THPTensorStr " tensor1, " THPTensorStr " tensor2, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_(addcmul_)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_value = NULL;
    PyObject *__kw_tensor1 = NULL;
    PyObject *__kw_tensor2 = NULL;
    if (kwargs) {
      __kw_value = PyDict_GetItemString(kwargs, "value");
      __kw_tensor1 = PyDict_GetItemString(kwargs, "tensor1");
      __kw_tensor2 = PyDict_GetItemString(kwargs, "tensor2");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 3 &&
          (__tuplecount > 0 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value)) &&
          (__tuplecount > 1 || __kw_tensor1) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_tensor1)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_tensor2) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_tensor2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value));
      THTensor* arg_tensor1 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_tensor1))->cdata;
      THTensor* arg_tensor2 = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_tensor2))->cdata;
      
      THTensor *arg_tensor1_save = arg_tensor1;
      THTensorPtr arg_tensor1_guard(nullptr);
      THTensor *arg_tensor2_save = arg_tensor2;
      THTensorPtr arg_tensor2_guard(nullptr);
      
      bool try_expand = !(THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
              arg_tensor1->size, arg_tensor1->nDimension) &&
          THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
              arg_tensor2->size, arg_tensor2->nDimension));
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_tensor1_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_tensor2_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_inplace2(LIBRARY_STATE arg_tensor1_guard.get(), arg_tensor2_guard.get(),
              arg_tensor1, arg_tensor2, arg_self,
              "tensor1", "tensor2", "self", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_tensor1 = arg_tensor1_guard.get();
          arg_tensor2 = arg_tensor2_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addcmul)(LIBRARY_STATE arg_self, arg_self, arg_value, arg_tensor1, arg_tensor2);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)(self);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_tensor1 = arg_tensor1_save;
      arg_tensor2 = arg_tensor2_save;
      
      
    #if !IS_DISTRIBUTED
          
    } else if (__argcount == 3 &&
          (__tuplecount > 0 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value)) &&
          (__tuplecount > 1 || __kw_tensor1) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_tensor1)) == THSPTensorClass &&
          (__tuplecount > 2 || __kw_tensor2) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_tensor2)) == THSPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value));
      THSTensor* arg_tensor1 = ((THSPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_tensor1))->cdata;
      THSTensor* arg_tensor2 = ((THSPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_tensor2))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(spaddcmul)(LIBRARY_STATE arg_self, arg_self, arg_value, arg_tensor1, arg_tensor2);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)(self);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif

    } else if (__argcount == 2 &&
          (__tuplecount > 0 || __kw_tensor1) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor1)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_tensor2) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_tensor2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_tensor1 = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor1))->cdata;
      THTensor* arg_tensor2 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_tensor2))->cdata;
      
      THTensor *arg_tensor1_save = arg_tensor1;
      THTensorPtr arg_tensor1_guard(nullptr);
      THTensor *arg_tensor2_save = arg_tensor2;
      THTensorPtr arg_tensor2_guard(nullptr);
      
      bool try_expand = !(THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
              arg_tensor1->size, arg_tensor1->nDimension) &&
          THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
              arg_tensor2->size, arg_tensor2->nDimension));
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_tensor1_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_tensor2_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_inplace2(LIBRARY_STATE arg_tensor1_guard.get(), arg_tensor2_guard.get(),
              arg_tensor1, arg_tensor2, arg_self,
              "tensor1", "tensor2", "self", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_tensor1 = arg_tensor1_guard.get();
          arg_tensor2 = arg_tensor2_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addcmul)(LIBRARY_STATE arg_self, arg_self, AS_REAL(1), arg_tensor1, arg_tensor2);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)(self);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_tensor1 = arg_tensor1_save;
      arg_tensor2 = arg_tensor2_save;
      
      
    #if !IS_DISTRIBUTED
          
    } else if (__argcount == 2 &&
          (__tuplecount > 0 || __kw_tensor1) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor1)) == THSPTensorClass &&
          (__tuplecount > 1 || __kw_tensor2) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_tensor2)) == THSPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THSTensor* arg_tensor1 = ((THSPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor1))->cdata;
      THSTensor* arg_tensor2 = ((THSPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_tensor2))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(spaddcmul)(LIBRARY_STATE arg_self, arg_self, AS_REAL(1), arg_tensor1, arg_tensor2);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)(self);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif

    }

    THPUtils_invalidArguments(args, kwargs, "addcmul_", 4, "(" THPTensorStr " tensor1, " THPTensorStr " tensor2)", "(" THSPTensorStr " tensor1, " THSPTensorStr " tensor2)", "(" RealStr " value, " THPTensorStr " tensor1, " THPTensorStr " tensor2)", "(" RealStr " value, " THSPTensorStr " tensor1, " THSPTensorStr " tensor2)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_(addcdiv)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_value = NULL;
    PyObject *__kw_tensor1 = NULL;
    PyObject *__kw_tensor2 = NULL;
    if (kwargs) {
      __kw_value = PyDict_GetItemString(kwargs, "value");
      __kw_tensor1 = PyDict_GetItemString(kwargs, "tensor1");
      __kw_tensor2 = PyDict_GetItemString(kwargs, "tensor2");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 4 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value)) &&
          (__tuplecount > 1 || __kw_tensor1) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_tensor1)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_tensor2) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_tensor2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value));
      THTensor* arg_tensor1 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_tensor1))->cdata;
      THTensor* arg_tensor2 = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_tensor2))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_tensor1_save = arg_tensor1;
      THTensorPtr arg_tensor1_guard(nullptr);
      THTensor *arg_tensor2_save = arg_tensor2;
      THTensorPtr arg_tensor2_guard(nullptr);
      
      bool try_expand = !(THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
              arg_tensor1->size, arg_tensor1->nDimension) &&
          THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
              arg_tensor2->size, arg_tensor2->nDimension));
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_tensor1_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_tensor2_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace3(LIBRARY_STATE arg_self_guard.get(),
              arg_tensor1_guard.get(), arg_tensor2_guard.get(),
              arg_self, arg_tensor1, arg_tensor2,
              "self", "tensor1", "tensor2", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_tensor1 = arg_tensor1_guard.get();
          arg_tensor2 = arg_tensor2_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addcdiv)(LIBRARY_STATE arg_result, arg_self, arg_value, arg_tensor1, arg_tensor2);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_tensor1 = arg_tensor1_save;
      arg_tensor2 = arg_tensor2_save;
      
      
    
    } else if (___out == NULL &&
          __argcount == 3 &&
          (__tuplecount > 0 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value)) &&
          (__tuplecount > 1 || __kw_tensor1) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_tensor1)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_tensor2) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_tensor2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value));
      THTensor* arg_tensor1 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_tensor1))->cdata;
      THTensor* arg_tensor2 = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_tensor2))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_tensor1_save = arg_tensor1;
      THTensorPtr arg_tensor1_guard(nullptr);
      THTensor *arg_tensor2_save = arg_tensor2;
      THTensorPtr arg_tensor2_guard(nullptr);
      
      bool try_expand = !(THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
              arg_tensor1->size, arg_tensor1->nDimension) &&
          THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
              arg_tensor2->size, arg_tensor2->nDimension));
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_tensor1_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_tensor2_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace3(LIBRARY_STATE arg_self_guard.get(),
              arg_tensor1_guard.get(), arg_tensor2_guard.get(),
              arg_self, arg_tensor1, arg_tensor2,
              "self", "tensor1", "tensor2", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_tensor1 = arg_tensor1_guard.get();
          arg_tensor2 = arg_tensor2_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addcdiv)(LIBRARY_STATE arg_result, arg_self, arg_value, arg_tensor1, arg_tensor2);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_tensor1 = arg_tensor1_save;
      arg_tensor2 = arg_tensor2_save;
      
      
    
    } else if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_tensor1) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor1)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_tensor2) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_tensor2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_tensor1 = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor1))->cdata;
      THTensor* arg_tensor2 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_tensor2))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_tensor1_save = arg_tensor1;
      THTensorPtr arg_tensor1_guard(nullptr);
      THTensor *arg_tensor2_save = arg_tensor2;
      THTensorPtr arg_tensor2_guard(nullptr);
      
      bool try_expand = !(THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
              arg_tensor1->size, arg_tensor1->nDimension) &&
          THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
              arg_tensor2->size, arg_tensor2->nDimension));
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_tensor1_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_tensor2_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace3(LIBRARY_STATE arg_self_guard.get(),
              arg_tensor1_guard.get(), arg_tensor2_guard.get(),
              arg_self, arg_tensor1, arg_tensor2,
              "self", "tensor1", "tensor2", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_tensor1 = arg_tensor1_guard.get();
          arg_tensor2 = arg_tensor2_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addcdiv)(LIBRARY_STATE arg_result, arg_self, AS_REAL(1), arg_tensor1, arg_tensor2);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_tensor1 = arg_tensor1_save;
      arg_tensor2 = arg_tensor2_save;
      
      
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_tensor1) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor1)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_tensor2) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_tensor2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_tensor1 = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor1))->cdata;
      THTensor* arg_tensor2 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_tensor2))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_tensor1_save = arg_tensor1;
      THTensorPtr arg_tensor1_guard(nullptr);
      THTensor *arg_tensor2_save = arg_tensor2;
      THTensorPtr arg_tensor2_guard(nullptr);
      
      bool try_expand = !(THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
              arg_tensor1->size, arg_tensor1->nDimension) &&
          THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
              arg_tensor2->size, arg_tensor2->nDimension));
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_tensor1_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_tensor2_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace3(LIBRARY_STATE arg_self_guard.get(),
              arg_tensor1_guard.get(), arg_tensor2_guard.get(),
              arg_self, arg_tensor1, arg_tensor2,
              "self", "tensor1", "tensor2", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_tensor1 = arg_tensor1_guard.get();
          arg_tensor2 = arg_tensor2_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addcdiv)(LIBRARY_STATE arg_result, arg_self, AS_REAL(1), arg_tensor1, arg_tensor2);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_tensor1 = arg_tensor1_save;
      arg_tensor2 = arg_tensor2_save;
      
      
    
    }

    THPUtils_invalidArguments(args, kwargs, "addcdiv", 2, "(" THPTensorStr " tensor1, " THPTensorStr " tensor2, #" THPTensorStr " out)", "(" RealStr " value, " THPTensorStr " tensor1, " THPTensorStr " tensor2, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_stateless_(addcdiv)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    PyObject *__kw_value = NULL;
    PyObject *__kw_tensor1 = NULL;
    PyObject *__kw_tensor2 = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
      __kw_value = PyDict_GetItemString(kwargs, "value");
      __kw_tensor1 = PyDict_GetItemString(kwargs, "tensor1");
      __kw_tensor2 = PyDict_GetItemString(kwargs, "tensor2");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 5 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value)) &&
          (__tuplecount > 2 || __kw_tensor1) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_tensor1)) == THPTensorClass &&
          (__tuplecount > 3 || __kw_tensor2) && (PyObject*)Py_TYPE((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_tensor2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value));
      THTensor* arg_tensor1 = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_tensor1))->cdata;
      THTensor* arg_tensor2 = ((THPTensor*)(__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_tensor2))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_tensor1_save = arg_tensor1;
      THTensorPtr arg_tensor1_guard(nullptr);
      THTensor *arg_tensor2_save = arg_tensor2;
      THTensorPtr arg_tensor2_guard(nullptr);
      
      bool try_expand = !(THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
              arg_tensor1->size, arg_tensor1->nDimension) &&
          THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
              arg_tensor2->size, arg_tensor2->nDimension));
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_tensor1_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_tensor2_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace3(LIBRARY_STATE arg_self_guard.get(),
              arg_tensor1_guard.get(), arg_tensor2_guard.get(),
              arg_self, arg_tensor1, arg_tensor2,
              "self", "tensor1", "tensor2", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_tensor1 = arg_tensor1_guard.get();
          arg_tensor2 = arg_tensor2_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addcdiv)(LIBRARY_STATE arg_result, arg_self, arg_value, arg_tensor1, arg_tensor2);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_tensor1 = arg_tensor1_save;
      arg_tensor2 = arg_tensor2_save;
      
      
    
    } else if (___out == NULL &&
          __argcount == 4 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value)) &&
          (__tuplecount > 2 || __kw_tensor1) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_tensor1)) == THPTensorClass &&
          (__tuplecount > 3 || __kw_tensor2) && (PyObject*)Py_TYPE((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_tensor2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value));
      THTensor* arg_tensor1 = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_tensor1))->cdata;
      THTensor* arg_tensor2 = ((THPTensor*)(__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_tensor2))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_tensor1_save = arg_tensor1;
      THTensorPtr arg_tensor1_guard(nullptr);
      THTensor *arg_tensor2_save = arg_tensor2;
      THTensorPtr arg_tensor2_guard(nullptr);
      
      bool try_expand = !(THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
              arg_tensor1->size, arg_tensor1->nDimension) &&
          THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
              arg_tensor2->size, arg_tensor2->nDimension));
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_tensor1_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_tensor2_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace3(LIBRARY_STATE arg_self_guard.get(),
              arg_tensor1_guard.get(), arg_tensor2_guard.get(),
              arg_self, arg_tensor1, arg_tensor2,
              "self", "tensor1", "tensor2", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_tensor1 = arg_tensor1_guard.get();
          arg_tensor2 = arg_tensor2_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addcdiv)(LIBRARY_STATE arg_result, arg_self, arg_value, arg_tensor1, arg_tensor2);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_tensor1 = arg_tensor1_save;
      arg_tensor2 = arg_tensor2_save;
      
      
    
    } else if (___out != NULL &&
          __argcount == 4 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_tensor1) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_tensor1)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_tensor2) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_tensor2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_tensor1 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_tensor1))->cdata;
      THTensor* arg_tensor2 = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_tensor2))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_tensor1_save = arg_tensor1;
      THTensorPtr arg_tensor1_guard(nullptr);
      THTensor *arg_tensor2_save = arg_tensor2;
      THTensorPtr arg_tensor2_guard(nullptr);
      
      bool try_expand = !(THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
              arg_tensor1->size, arg_tensor1->nDimension) &&
          THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
              arg_tensor2->size, arg_tensor2->nDimension));
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_tensor1_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_tensor2_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace3(LIBRARY_STATE arg_self_guard.get(),
              arg_tensor1_guard.get(), arg_tensor2_guard.get(),
              arg_self, arg_tensor1, arg_tensor2,
              "self", "tensor1", "tensor2", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_tensor1 = arg_tensor1_guard.get();
          arg_tensor2 = arg_tensor2_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addcdiv)(LIBRARY_STATE arg_result, arg_self, AS_REAL(1), arg_tensor1, arg_tensor2);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_tensor1 = arg_tensor1_save;
      arg_tensor2 = arg_tensor2_save;
      
      
    
    } else if (___out == NULL &&
          __argcount == 3 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_tensor1) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_tensor1)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_tensor2) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_tensor2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_tensor1 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_tensor1))->cdata;
      THTensor* arg_tensor2 = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_tensor2))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_tensor1_save = arg_tensor1;
      THTensorPtr arg_tensor1_guard(nullptr);
      THTensor *arg_tensor2_save = arg_tensor2;
      THTensorPtr arg_tensor2_guard(nullptr);
      
      bool try_expand = !(THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
              arg_tensor1->size, arg_tensor1->nDimension) &&
          THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
              arg_tensor2->size, arg_tensor2->nDimension));
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_tensor1_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_tensor2_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace3(LIBRARY_STATE arg_self_guard.get(),
              arg_tensor1_guard.get(), arg_tensor2_guard.get(),
              arg_self, arg_tensor1, arg_tensor2,
              "self", "tensor1", "tensor2", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_tensor1 = arg_tensor1_guard.get();
          arg_tensor2 = arg_tensor2_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addcdiv)(LIBRARY_STATE arg_result, arg_self, AS_REAL(1), arg_tensor1, arg_tensor2);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_tensor1 = arg_tensor1_save;
      arg_tensor2 = arg_tensor2_save;
      
      
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.addcdiv", 2, "(" THPTensorStr " source, " THPTensorStr " tensor1, " THPTensorStr " tensor2, #" THPTensorStr " out)", "(" THPTensorStr " source, " RealStr " value, " THPTensorStr " tensor1, " THPTensorStr " tensor2, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_(addcdiv_)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_value = NULL;
    PyObject *__kw_tensor1 = NULL;
    PyObject *__kw_tensor2 = NULL;
    if (kwargs) {
      __kw_value = PyDict_GetItemString(kwargs, "value");
      __kw_tensor1 = PyDict_GetItemString(kwargs, "tensor1");
      __kw_tensor2 = PyDict_GetItemString(kwargs, "tensor2");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 3 &&
          (__tuplecount > 0 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value)) &&
          (__tuplecount > 1 || __kw_tensor1) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_tensor1)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_tensor2) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_tensor2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value));
      THTensor* arg_tensor1 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_tensor1))->cdata;
      THTensor* arg_tensor2 = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_tensor2))->cdata;
      
      THTensor *arg_tensor1_save = arg_tensor1;
      THTensorPtr arg_tensor1_guard(nullptr);
      THTensor *arg_tensor2_save = arg_tensor2;
      THTensorPtr arg_tensor2_guard(nullptr);
      
      bool try_expand = !(THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
              arg_tensor1->size, arg_tensor1->nDimension) &&
          THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
              arg_tensor2->size, arg_tensor2->nDimension));
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_tensor1_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_tensor2_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_inplace2(LIBRARY_STATE arg_tensor1_guard.get(), arg_tensor2_guard.get(),
              arg_tensor1, arg_tensor2, arg_self,
              "tensor1", "tensor2", "self", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_tensor1 = arg_tensor1_guard.get();
          arg_tensor2 = arg_tensor2_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addcdiv)(LIBRARY_STATE arg_self, arg_self, arg_value, arg_tensor1, arg_tensor2);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)(self);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_tensor1 = arg_tensor1_save;
      arg_tensor2 = arg_tensor2_save;
      
      
    
    } else if (__argcount == 2 &&
          (__tuplecount > 0 || __kw_tensor1) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor1)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_tensor2) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_tensor2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_tensor1 = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor1))->cdata;
      THTensor* arg_tensor2 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_tensor2))->cdata;
      
      THTensor *arg_tensor1_save = arg_tensor1;
      THTensorPtr arg_tensor1_guard(nullptr);
      THTensor *arg_tensor2_save = arg_tensor2;
      THTensorPtr arg_tensor2_guard(nullptr);
      
      bool try_expand = !(THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
              arg_tensor1->size, arg_tensor1->nDimension) &&
          THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
              arg_tensor2->size, arg_tensor2->nDimension));
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_tensor1_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_tensor2_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_inplace2(LIBRARY_STATE arg_tensor1_guard.get(), arg_tensor2_guard.get(),
              arg_tensor1, arg_tensor2, arg_self,
              "tensor1", "tensor2", "self", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_tensor1 = arg_tensor1_guard.get();
          arg_tensor2 = arg_tensor2_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(addcdiv)(LIBRARY_STATE arg_self, arg_self, AS_REAL(1), arg_tensor1, arg_tensor2);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)(self);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_tensor1 = arg_tensor1_save;
      arg_tensor2 = arg_tensor2_save;
      
      
    
    }

    THPUtils_invalidArguments(args, kwargs, "addcdiv_", 2, "(" THPTensorStr " tensor1, " THPTensorStr " tensor2)", "(" RealStr " value, " THPTensorStr " tensor1, " THPTensorStr " tensor2)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if defined(TH_REAL_IS_FLOAT) || defined(TH_REAL_IS_DOUBLE) || CUDA_FLOAT || CUDA_DOUBLE
// We need to pass pointers to chars to tensor lapack functions...
static const char __U = 'U';
static const char __L = 'L';
static const char __N = 'N';
static const char __V = 'V';
static const char __A = 'A';
static const char __S = 'S';
#if !IS_CUDA
static const char __T = 'T';
static const char __R = 'R';
#endif
static const char *U = &__U;
static const char *L = &__L;
static const char *N = &__N;
static const char *V = &__V;
static const char *A = &__A;
static const char *S = &__S;
#if !IS_CUDA
static const char *T = &__T;
static const char *R = &__R;
#endif
#endif

#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_FLOAT) || defined(TH_REAL_IS_DOUBLE) || CUDA_FLOAT || CUDA_DOUBLE)
PyObject * THPTensor_(gesv)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_A = NULL;
    if (kwargs) {
      __kw_A = PyDict_GetItemString(kwargs, "A");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPTensorClass &&
          (__tuplecount > 0 || __kw_A) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_A)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_solution = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THTensor* arg_lu = ((THPTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_A = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_A))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(gesv)(LIBRARY_STATE arg_solution, arg_lu, arg_self, arg_A);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_A) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_A)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _solution_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_solution_guard.get()) return NULL;
      THPTensor* solution = _solution_guard.get();
      
      THPTensorPtr _lu_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_lu_guard.get()) return NULL;
      THPTensor* lu = _lu_guard.get();
      
      
      THTensor* arg_solution = ((THPTensor*)solution)->cdata;
      THTensor* arg_lu = ((THPTensor*)lu)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_A = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_A))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(gesv)(LIBRARY_STATE arg_solution, arg_lu, arg_self, arg_A);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, solution, lu);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "gesv", 1, "(" THPTensorStr " A, #tuple[" THPTensorStr ", " THPTensorStr "] out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_FLOAT) || defined(TH_REAL_IS_DOUBLE) || CUDA_FLOAT || CUDA_DOUBLE)
PyObject * THPTensor_stateless_(gesv)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    PyObject *__kw_A = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
      __kw_A = PyDict_GetItemString(kwargs, "A");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_A) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_A)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_solution = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THTensor* arg_lu = ((THPTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_A = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_A))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(gesv)(LIBRARY_STATE arg_solution, arg_lu, arg_self, arg_A);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_A) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_A)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _solution_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_solution_guard.get()) return NULL;
      THPTensor* solution = _solution_guard.get();
      
      THPTensorPtr _lu_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_lu_guard.get()) return NULL;
      THPTensor* lu = _lu_guard.get();
      
      
      THTensor* arg_solution = ((THPTensor*)solution)->cdata;
      THTensor* arg_lu = ((THPTensor*)lu)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_A = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_A))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(gesv)(LIBRARY_STATE arg_solution, arg_lu, arg_self, arg_A);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, solution, lu);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.gesv", 1, "(" THPTensorStr " source, " THPTensorStr " A, #tuple[" THPTensorStr ", " THPTensorStr "] out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_FLOAT) || defined(TH_REAL_IS_DOUBLE) || CUDA_FLOAT || CUDA_DOUBLE)
PyObject * THPTensor_(gels)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_A = NULL;
    if (kwargs) {
      __kw_A = PyDict_GetItemString(kwargs, "A");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPTensorClass &&
          (__tuplecount > 0 || __kw_A) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_A)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_res1 = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THTensor* arg_res2 = ((THPTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_A = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_A))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(gels)(LIBRARY_STATE arg_res1, arg_res2, arg_self, arg_A);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_A) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_A)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _res1_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_res1_guard.get()) return NULL;
      THPTensor* res1 = _res1_guard.get();
      
      THPTensorPtr _res2_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_res2_guard.get()) return NULL;
      THPTensor* res2 = _res2_guard.get();
      
      
      THTensor* arg_res1 = ((THPTensor*)res1)->cdata;
      THTensor* arg_res2 = ((THPTensor*)res2)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_A = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_A))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(gels)(LIBRARY_STATE arg_res1, arg_res2, arg_self, arg_A);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, res1, res2);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "gels", 1, "(" THPTensorStr " A, #tuple[" THPTensorStr ", " THPTensorStr "] out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_FLOAT) || defined(TH_REAL_IS_DOUBLE) || CUDA_FLOAT || CUDA_DOUBLE)
PyObject * THPTensor_stateless_(gels)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    PyObject *__kw_A = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
      __kw_A = PyDict_GetItemString(kwargs, "A");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_A) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_A)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_res1 = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THTensor* arg_res2 = ((THPTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_A = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_A))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(gels)(LIBRARY_STATE arg_res1, arg_res2, arg_self, arg_A);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_A) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_A)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _res1_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_res1_guard.get()) return NULL;
      THPTensor* res1 = _res1_guard.get();
      
      THPTensorPtr _res2_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_res2_guard.get()) return NULL;
      THPTensor* res2 = _res2_guard.get();
      
      
      THTensor* arg_res1 = ((THPTensor*)res1)->cdata;
      THTensor* arg_res2 = ((THPTensor*)res2)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_A = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_A))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(gels)(LIBRARY_STATE arg_res1, arg_res2, arg_self, arg_A);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, res1, res2);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.gels", 1, "(" THPTensorStr " source, " THPTensorStr " A, #tuple[" THPTensorStr ", " THPTensorStr "] out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_FLOAT) || defined(TH_REAL_IS_DOUBLE))
PyObject * THPTensor_(trtrs)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_A = NULL;
    PyObject *__kw_upper = NULL;
    PyObject *__kw_transpose = NULL;
    PyObject *__kw_unitriangular = NULL;
    if (kwargs) {
      __kw_A = PyDict_GetItemString(kwargs, "A");
      __kw_upper = PyDict_GetItemString(kwargs, "upper");
      __kw_transpose = PyDict_GetItemString(kwargs, "transpose");
      __kw_unitriangular = PyDict_GetItemString(kwargs, "unitriangular");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 5 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPTensorClass &&
          (__tuplecount > 0 || __kw_A) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_A)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_upper) && PyBool_Check((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_upper)) &&
          (__tuplecount > 2 || __kw_transpose) && PyBool_Check((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_transpose)) &&
          (__tuplecount > 3 || __kw_unitriangular) && PyBool_Check((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_unitriangular))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_res1 = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THTensor* arg_res2 = ((THPTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_A = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_A))->cdata;
      const char* arg_upper = (__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_upper) == Py_True ? U : L;
      const char* arg_transpose = (__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_transpose) == Py_True ? T : N;
      const char* arg_unitriangular = (__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_unitriangular) == Py_True ? U : N;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(trtrs)(LIBRARY_STATE arg_res1, arg_res2, arg_self, arg_A, arg_upper, arg_transpose, arg_unitriangular);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 4 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPTensorClass &&
          (__tuplecount > 0 || __kw_A) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_A)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_upper) && PyBool_Check((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_upper)) &&
          (__tuplecount > 2 || __kw_transpose) && PyBool_Check((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_transpose))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_res1 = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THTensor* arg_res2 = ((THPTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_A = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_A))->cdata;
      const char* arg_upper = (__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_upper) == Py_True ? U : L;
      const char* arg_transpose = (__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_transpose) == Py_True ? T : N;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(trtrs)(LIBRARY_STATE arg_res1, arg_res2, arg_self, arg_A, arg_upper, arg_transpose, N);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 4 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPTensorClass &&
          (__tuplecount > 0 || __kw_A) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_A)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_upper) && PyBool_Check((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_upper)) &&
          __kw_unitriangular && PyBool_Check(__kw_unitriangular)) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_res1 = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THTensor* arg_res2 = ((THPTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_A = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_A))->cdata;
      const char* arg_upper = (__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_upper) == Py_True ? U : L;
      const char* arg_unitriangular = __kw_unitriangular == Py_True ? U : N;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(trtrs)(LIBRARY_STATE arg_res1, arg_res2, arg_self, arg_A, arg_upper, N, arg_unitriangular);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 4 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPTensorClass &&
          (__tuplecount > 0 || __kw_A) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_A)) == THPTensorClass &&
          __kw_transpose && PyBool_Check(__kw_transpose) &&
          __kw_unitriangular && PyBool_Check(__kw_unitriangular)) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_res1 = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THTensor* arg_res2 = ((THPTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_A = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_A))->cdata;
      const char* arg_transpose = __kw_transpose == Py_True ? T : N;
      const char* arg_unitriangular = __kw_unitriangular == Py_True ? U : N;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(trtrs)(LIBRARY_STATE arg_res1, arg_res2, arg_self, arg_A, U, arg_transpose, arg_unitriangular);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 4 &&
          (__tuplecount > 0 || __kw_A) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_A)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_upper) && PyBool_Check((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_upper)) &&
          (__tuplecount > 2 || __kw_transpose) && PyBool_Check((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_transpose)) &&
          (__tuplecount > 3 || __kw_unitriangular) && PyBool_Check((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_unitriangular))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _res1_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_res1_guard.get()) return NULL;
      THPTensor* res1 = _res1_guard.get();
      
      THPTensorPtr _res2_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_res2_guard.get()) return NULL;
      THPTensor* res2 = _res2_guard.get();
      
      
      THTensor* arg_res1 = ((THPTensor*)res1)->cdata;
      THTensor* arg_res2 = ((THPTensor*)res2)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_A = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_A))->cdata;
      const char* arg_upper = (__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_upper) == Py_True ? U : L;
      const char* arg_transpose = (__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_transpose) == Py_True ? T : N;
      const char* arg_unitriangular = (__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_unitriangular) == Py_True ? U : N;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(trtrs)(LIBRARY_STATE arg_res1, arg_res2, arg_self, arg_A, arg_upper, arg_transpose, arg_unitriangular);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, res1, res2);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPTensorClass &&
          (__tuplecount > 0 || __kw_A) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_A)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_upper) && PyBool_Check((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_upper))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_res1 = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THTensor* arg_res2 = ((THPTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_A = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_A))->cdata;
      const char* arg_upper = (__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_upper) == Py_True ? U : L;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(trtrs)(LIBRARY_STATE arg_res1, arg_res2, arg_self, arg_A, arg_upper, N, N);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPTensorClass &&
          (__tuplecount > 0 || __kw_A) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_A)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_transpose) && PyBool_Check((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_transpose))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_res1 = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THTensor* arg_res2 = ((THPTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_A = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_A))->cdata;
      const char* arg_transpose = (__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_transpose) == Py_True ? T : N;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(trtrs)(LIBRARY_STATE arg_res1, arg_res2, arg_self, arg_A, U, arg_transpose, N);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPTensorClass &&
          (__tuplecount > 0 || __kw_A) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_A)) == THPTensorClass &&
          __kw_unitriangular && PyBool_Check(__kw_unitriangular)) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_res1 = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THTensor* arg_res2 = ((THPTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_A = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_A))->cdata;
      const char* arg_unitriangular = __kw_unitriangular == Py_True ? U : N;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(trtrs)(LIBRARY_STATE arg_res1, arg_res2, arg_self, arg_A, U, N, arg_unitriangular);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 3 &&
          (__tuplecount > 0 || __kw_A) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_A)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_upper) && PyBool_Check((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_upper)) &&
          (__tuplecount > 2 || __kw_transpose) && PyBool_Check((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_transpose))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _res1_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_res1_guard.get()) return NULL;
      THPTensor* res1 = _res1_guard.get();
      
      THPTensorPtr _res2_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_res2_guard.get()) return NULL;
      THPTensor* res2 = _res2_guard.get();
      
      
      THTensor* arg_res1 = ((THPTensor*)res1)->cdata;
      THTensor* arg_res2 = ((THPTensor*)res2)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_A = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_A))->cdata;
      const char* arg_upper = (__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_upper) == Py_True ? U : L;
      const char* arg_transpose = (__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_transpose) == Py_True ? T : N;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(trtrs)(LIBRARY_STATE arg_res1, arg_res2, arg_self, arg_A, arg_upper, arg_transpose, N);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, res1, res2);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 3 &&
          (__tuplecount > 0 || __kw_A) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_A)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_upper) && PyBool_Check((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_upper)) &&
          __kw_unitriangular && PyBool_Check(__kw_unitriangular)) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _res1_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_res1_guard.get()) return NULL;
      THPTensor* res1 = _res1_guard.get();
      
      THPTensorPtr _res2_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_res2_guard.get()) return NULL;
      THPTensor* res2 = _res2_guard.get();
      
      
      THTensor* arg_res1 = ((THPTensor*)res1)->cdata;
      THTensor* arg_res2 = ((THPTensor*)res2)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_A = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_A))->cdata;
      const char* arg_upper = (__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_upper) == Py_True ? U : L;
      const char* arg_unitriangular = __kw_unitriangular == Py_True ? U : N;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(trtrs)(LIBRARY_STATE arg_res1, arg_res2, arg_self, arg_A, arg_upper, N, arg_unitriangular);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, res1, res2);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 3 &&
          (__tuplecount > 0 || __kw_A) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_A)) == THPTensorClass &&
          __kw_transpose && PyBool_Check(__kw_transpose) &&
          __kw_unitriangular && PyBool_Check(__kw_unitriangular)) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _res1_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_res1_guard.get()) return NULL;
      THPTensor* res1 = _res1_guard.get();
      
      THPTensorPtr _res2_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_res2_guard.get()) return NULL;
      THPTensor* res2 = _res2_guard.get();
      
      
      THTensor* arg_res1 = ((THPTensor*)res1)->cdata;
      THTensor* arg_res2 = ((THPTensor*)res2)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_A = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_A))->cdata;
      const char* arg_transpose = __kw_transpose == Py_True ? T : N;
      const char* arg_unitriangular = __kw_unitriangular == Py_True ? U : N;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(trtrs)(LIBRARY_STATE arg_res1, arg_res2, arg_self, arg_A, U, arg_transpose, arg_unitriangular);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, res1, res2);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPTensorClass &&
          (__tuplecount > 0 || __kw_A) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_A)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_res1 = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THTensor* arg_res2 = ((THPTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_A = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_A))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(trtrs)(LIBRARY_STATE arg_res1, arg_res2, arg_self, arg_A, U, N, N);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_A) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_A)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_upper) && PyBool_Check((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_upper))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _res1_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_res1_guard.get()) return NULL;
      THPTensor* res1 = _res1_guard.get();
      
      THPTensorPtr _res2_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_res2_guard.get()) return NULL;
      THPTensor* res2 = _res2_guard.get();
      
      
      THTensor* arg_res1 = ((THPTensor*)res1)->cdata;
      THTensor* arg_res2 = ((THPTensor*)res2)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_A = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_A))->cdata;
      const char* arg_upper = (__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_upper) == Py_True ? U : L;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(trtrs)(LIBRARY_STATE arg_res1, arg_res2, arg_self, arg_A, arg_upper, N, N);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, res1, res2);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_A) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_A)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_transpose) && PyBool_Check((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_transpose))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _res1_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_res1_guard.get()) return NULL;
      THPTensor* res1 = _res1_guard.get();
      
      THPTensorPtr _res2_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_res2_guard.get()) return NULL;
      THPTensor* res2 = _res2_guard.get();
      
      
      THTensor* arg_res1 = ((THPTensor*)res1)->cdata;
      THTensor* arg_res2 = ((THPTensor*)res2)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_A = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_A))->cdata;
      const char* arg_transpose = (__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_transpose) == Py_True ? T : N;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(trtrs)(LIBRARY_STATE arg_res1, arg_res2, arg_self, arg_A, U, arg_transpose, N);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, res1, res2);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_A) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_A)) == THPTensorClass &&
          __kw_unitriangular && PyBool_Check(__kw_unitriangular)) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _res1_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_res1_guard.get()) return NULL;
      THPTensor* res1 = _res1_guard.get();
      
      THPTensorPtr _res2_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_res2_guard.get()) return NULL;
      THPTensor* res2 = _res2_guard.get();
      
      
      THTensor* arg_res1 = ((THPTensor*)res1)->cdata;
      THTensor* arg_res2 = ((THPTensor*)res2)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_A = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_A))->cdata;
      const char* arg_unitriangular = __kw_unitriangular == Py_True ? U : N;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(trtrs)(LIBRARY_STATE arg_res1, arg_res2, arg_self, arg_A, U, N, arg_unitriangular);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, res1, res2);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_A) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_A)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _res1_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_res1_guard.get()) return NULL;
      THPTensor* res1 = _res1_guard.get();
      
      THPTensorPtr _res2_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_res2_guard.get()) return NULL;
      THPTensor* res2 = _res2_guard.get();
      
      
      THTensor* arg_res1 = ((THPTensor*)res1)->cdata;
      THTensor* arg_res2 = ((THPTensor*)res2)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_A = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_A))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(trtrs)(LIBRARY_STATE arg_res1, arg_res2, arg_self, arg_A, U, N, N);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, res1, res2);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "trtrs", 8, "(" THPTensorStr " A, #tuple[" THPTensorStr ", " THPTensorStr "] out)", "(" THPTensorStr " A, bool upper, #tuple[" THPTensorStr ", " THPTensorStr "] out)", "(" THPTensorStr " A, bool transpose, #tuple[" THPTensorStr ", " THPTensorStr "] out)", "(" THPTensorStr " A, bool unitriangular, #tuple[" THPTensorStr ", " THPTensorStr "] out)", "(" THPTensorStr " A, bool upper, bool transpose, #tuple[" THPTensorStr ", " THPTensorStr "] out)", "(" THPTensorStr " A, bool upper, bool unitriangular, #tuple[" THPTensorStr ", " THPTensorStr "] out)", "(" THPTensorStr " A, bool transpose, bool unitriangular, #tuple[" THPTensorStr ", " THPTensorStr "] out)", "(" THPTensorStr " A, bool upper, bool transpose, bool unitriangular, #tuple[" THPTensorStr ", " THPTensorStr "] out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_FLOAT) || defined(TH_REAL_IS_DOUBLE))
PyObject * THPTensor_stateless_(trtrs)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    PyObject *__kw_A = NULL;
    PyObject *__kw_upper = NULL;
    PyObject *__kw_transpose = NULL;
    PyObject *__kw_unitriangular = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
      __kw_A = PyDict_GetItemString(kwargs, "A");
      __kw_upper = PyDict_GetItemString(kwargs, "upper");
      __kw_transpose = PyDict_GetItemString(kwargs, "transpose");
      __kw_unitriangular = PyDict_GetItemString(kwargs, "unitriangular");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 6 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_A) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_A)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_upper) && PyBool_Check((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_upper)) &&
          (__tuplecount > 3 || __kw_transpose) && PyBool_Check((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_transpose)) &&
          (__tuplecount > 4 || __kw_unitriangular) && PyBool_Check((__tuplecount > 4 ? PyTuple_GET_ITEM(args, 4) : __kw_unitriangular))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_res1 = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THTensor* arg_res2 = ((THPTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_A = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_A))->cdata;
      const char* arg_upper = (__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_upper) == Py_True ? U : L;
      const char* arg_transpose = (__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_transpose) == Py_True ? T : N;
      const char* arg_unitriangular = (__tuplecount > 4 ? PyTuple_GET_ITEM(args, 4) : __kw_unitriangular) == Py_True ? U : N;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(trtrs)(LIBRARY_STATE arg_res1, arg_res2, arg_self, arg_A, arg_upper, arg_transpose, arg_unitriangular);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 5 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_A) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_A)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_upper) && PyBool_Check((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_upper)) &&
          (__tuplecount > 3 || __kw_transpose) && PyBool_Check((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_transpose))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_res1 = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THTensor* arg_res2 = ((THPTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_A = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_A))->cdata;
      const char* arg_upper = (__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_upper) == Py_True ? U : L;
      const char* arg_transpose = (__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_transpose) == Py_True ? T : N;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(trtrs)(LIBRARY_STATE arg_res1, arg_res2, arg_self, arg_A, arg_upper, arg_transpose, N);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 5 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_A) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_A)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_upper) && PyBool_Check((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_upper)) &&
          __kw_unitriangular && PyBool_Check(__kw_unitriangular)) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_res1 = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THTensor* arg_res2 = ((THPTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_A = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_A))->cdata;
      const char* arg_upper = (__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_upper) == Py_True ? U : L;
      const char* arg_unitriangular = __kw_unitriangular == Py_True ? U : N;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(trtrs)(LIBRARY_STATE arg_res1, arg_res2, arg_self, arg_A, arg_upper, N, arg_unitriangular);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 5 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_A) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_A)) == THPTensorClass &&
          __kw_transpose && PyBool_Check(__kw_transpose) &&
          __kw_unitriangular && PyBool_Check(__kw_unitriangular)) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_res1 = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THTensor* arg_res2 = ((THPTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_A = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_A))->cdata;
      const char* arg_transpose = __kw_transpose == Py_True ? T : N;
      const char* arg_unitriangular = __kw_unitriangular == Py_True ? U : N;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(trtrs)(LIBRARY_STATE arg_res1, arg_res2, arg_self, arg_A, U, arg_transpose, arg_unitriangular);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 5 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_A) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_A)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_upper) && PyBool_Check((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_upper)) &&
          (__tuplecount > 3 || __kw_transpose) && PyBool_Check((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_transpose)) &&
          (__tuplecount > 4 || __kw_unitriangular) && PyBool_Check((__tuplecount > 4 ? PyTuple_GET_ITEM(args, 4) : __kw_unitriangular))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _res1_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_res1_guard.get()) return NULL;
      THPTensor* res1 = _res1_guard.get();
      
      THPTensorPtr _res2_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_res2_guard.get()) return NULL;
      THPTensor* res2 = _res2_guard.get();
      
      
      THTensor* arg_res1 = ((THPTensor*)res1)->cdata;
      THTensor* arg_res2 = ((THPTensor*)res2)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_A = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_A))->cdata;
      const char* arg_upper = (__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_upper) == Py_True ? U : L;
      const char* arg_transpose = (__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_transpose) == Py_True ? T : N;
      const char* arg_unitriangular = (__tuplecount > 4 ? PyTuple_GET_ITEM(args, 4) : __kw_unitriangular) == Py_True ? U : N;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(trtrs)(LIBRARY_STATE arg_res1, arg_res2, arg_self, arg_A, arg_upper, arg_transpose, arg_unitriangular);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, res1, res2);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 4 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_A) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_A)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_upper) && PyBool_Check((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_upper))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_res1 = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THTensor* arg_res2 = ((THPTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_A = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_A))->cdata;
      const char* arg_upper = (__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_upper) == Py_True ? U : L;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(trtrs)(LIBRARY_STATE arg_res1, arg_res2, arg_self, arg_A, arg_upper, N, N);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 4 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_A) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_A)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_transpose) && PyBool_Check((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_transpose))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_res1 = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THTensor* arg_res2 = ((THPTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_A = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_A))->cdata;
      const char* arg_transpose = (__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_transpose) == Py_True ? T : N;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(trtrs)(LIBRARY_STATE arg_res1, arg_res2, arg_self, arg_A, U, arg_transpose, N);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 4 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_A) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_A)) == THPTensorClass &&
          __kw_unitriangular && PyBool_Check(__kw_unitriangular)) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_res1 = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THTensor* arg_res2 = ((THPTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_A = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_A))->cdata;
      const char* arg_unitriangular = __kw_unitriangular == Py_True ? U : N;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(trtrs)(LIBRARY_STATE arg_res1, arg_res2, arg_self, arg_A, U, N, arg_unitriangular);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 4 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_A) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_A)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_upper) && PyBool_Check((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_upper)) &&
          (__tuplecount > 3 || __kw_transpose) && PyBool_Check((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_transpose))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _res1_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_res1_guard.get()) return NULL;
      THPTensor* res1 = _res1_guard.get();
      
      THPTensorPtr _res2_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_res2_guard.get()) return NULL;
      THPTensor* res2 = _res2_guard.get();
      
      
      THTensor* arg_res1 = ((THPTensor*)res1)->cdata;
      THTensor* arg_res2 = ((THPTensor*)res2)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_A = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_A))->cdata;
      const char* arg_upper = (__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_upper) == Py_True ? U : L;
      const char* arg_transpose = (__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_transpose) == Py_True ? T : N;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(trtrs)(LIBRARY_STATE arg_res1, arg_res2, arg_self, arg_A, arg_upper, arg_transpose, N);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, res1, res2);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 4 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_A) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_A)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_upper) && PyBool_Check((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_upper)) &&
          __kw_unitriangular && PyBool_Check(__kw_unitriangular)) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _res1_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_res1_guard.get()) return NULL;
      THPTensor* res1 = _res1_guard.get();
      
      THPTensorPtr _res2_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_res2_guard.get()) return NULL;
      THPTensor* res2 = _res2_guard.get();
      
      
      THTensor* arg_res1 = ((THPTensor*)res1)->cdata;
      THTensor* arg_res2 = ((THPTensor*)res2)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_A = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_A))->cdata;
      const char* arg_upper = (__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_upper) == Py_True ? U : L;
      const char* arg_unitriangular = __kw_unitriangular == Py_True ? U : N;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(trtrs)(LIBRARY_STATE arg_res1, arg_res2, arg_self, arg_A, arg_upper, N, arg_unitriangular);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, res1, res2);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 4 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_A) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_A)) == THPTensorClass &&
          __kw_transpose && PyBool_Check(__kw_transpose) &&
          __kw_unitriangular && PyBool_Check(__kw_unitriangular)) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _res1_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_res1_guard.get()) return NULL;
      THPTensor* res1 = _res1_guard.get();
      
      THPTensorPtr _res2_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_res2_guard.get()) return NULL;
      THPTensor* res2 = _res2_guard.get();
      
      
      THTensor* arg_res1 = ((THPTensor*)res1)->cdata;
      THTensor* arg_res2 = ((THPTensor*)res2)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_A = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_A))->cdata;
      const char* arg_transpose = __kw_transpose == Py_True ? T : N;
      const char* arg_unitriangular = __kw_unitriangular == Py_True ? U : N;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(trtrs)(LIBRARY_STATE arg_res1, arg_res2, arg_self, arg_A, U, arg_transpose, arg_unitriangular);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, res1, res2);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_A) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_A)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_res1 = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THTensor* arg_res2 = ((THPTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_A = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_A))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(trtrs)(LIBRARY_STATE arg_res1, arg_res2, arg_self, arg_A, U, N, N);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 3 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_A) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_A)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_upper) && PyBool_Check((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_upper))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _res1_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_res1_guard.get()) return NULL;
      THPTensor* res1 = _res1_guard.get();
      
      THPTensorPtr _res2_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_res2_guard.get()) return NULL;
      THPTensor* res2 = _res2_guard.get();
      
      
      THTensor* arg_res1 = ((THPTensor*)res1)->cdata;
      THTensor* arg_res2 = ((THPTensor*)res2)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_A = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_A))->cdata;
      const char* arg_upper = (__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_upper) == Py_True ? U : L;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(trtrs)(LIBRARY_STATE arg_res1, arg_res2, arg_self, arg_A, arg_upper, N, N);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, res1, res2);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 3 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_A) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_A)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_transpose) && PyBool_Check((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_transpose))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _res1_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_res1_guard.get()) return NULL;
      THPTensor* res1 = _res1_guard.get();
      
      THPTensorPtr _res2_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_res2_guard.get()) return NULL;
      THPTensor* res2 = _res2_guard.get();
      
      
      THTensor* arg_res1 = ((THPTensor*)res1)->cdata;
      THTensor* arg_res2 = ((THPTensor*)res2)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_A = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_A))->cdata;
      const char* arg_transpose = (__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_transpose) == Py_True ? T : N;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(trtrs)(LIBRARY_STATE arg_res1, arg_res2, arg_self, arg_A, U, arg_transpose, N);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, res1, res2);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 3 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_A) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_A)) == THPTensorClass &&
          __kw_unitriangular && PyBool_Check(__kw_unitriangular)) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _res1_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_res1_guard.get()) return NULL;
      THPTensor* res1 = _res1_guard.get();
      
      THPTensorPtr _res2_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_res2_guard.get()) return NULL;
      THPTensor* res2 = _res2_guard.get();
      
      
      THTensor* arg_res1 = ((THPTensor*)res1)->cdata;
      THTensor* arg_res2 = ((THPTensor*)res2)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_A = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_A))->cdata;
      const char* arg_unitriangular = __kw_unitriangular == Py_True ? U : N;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(trtrs)(LIBRARY_STATE arg_res1, arg_res2, arg_self, arg_A, U, N, arg_unitriangular);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, res1, res2);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_A) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_A)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _res1_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_res1_guard.get()) return NULL;
      THPTensor* res1 = _res1_guard.get();
      
      THPTensorPtr _res2_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_res2_guard.get()) return NULL;
      THPTensor* res2 = _res2_guard.get();
      
      
      THTensor* arg_res1 = ((THPTensor*)res1)->cdata;
      THTensor* arg_res2 = ((THPTensor*)res2)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_A = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_A))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(trtrs)(LIBRARY_STATE arg_res1, arg_res2, arg_self, arg_A, U, N, N);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, res1, res2);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.trtrs", 8, "(" THPTensorStr " source, " THPTensorStr " A, #tuple[" THPTensorStr ", " THPTensorStr "] out)", "(" THPTensorStr " source, " THPTensorStr " A, bool upper, #tuple[" THPTensorStr ", " THPTensorStr "] out)", "(" THPTensorStr " source, " THPTensorStr " A, bool transpose, #tuple[" THPTensorStr ", " THPTensorStr "] out)", "(" THPTensorStr " source, " THPTensorStr " A, bool unitriangular, #tuple[" THPTensorStr ", " THPTensorStr "] out)", "(" THPTensorStr " source, " THPTensorStr " A, bool upper, bool transpose, #tuple[" THPTensorStr ", " THPTensorStr "] out)", "(" THPTensorStr " source, " THPTensorStr " A, bool upper, bool unitriangular, #tuple[" THPTensorStr ", " THPTensorStr "] out)", "(" THPTensorStr " source, " THPTensorStr " A, bool transpose, bool unitriangular, #tuple[" THPTensorStr ", " THPTensorStr "] out)", "(" THPTensorStr " source, " THPTensorStr " A, bool upper, bool transpose, bool unitriangular, #tuple[" THPTensorStr ", " THPTensorStr "] out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_FLOAT) || defined(TH_REAL_IS_DOUBLE) || CUDA_FLOAT || CUDA_DOUBLE)
PyObject * THPTensor_(symeig)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_eigenvectors = NULL;
    PyObject *__kw_upper = NULL;
    if (kwargs) {
      __kw_eigenvectors = PyDict_GetItemString(kwargs, "eigenvectors");
      __kw_upper = PyDict_GetItemString(kwargs, "upper");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPTensorClass &&
          (__tuplecount > 0 || __kw_eigenvectors) && PyBool_Check((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_eigenvectors)) &&
          (__tuplecount > 1 || __kw_upper) && PyBool_Check((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_upper))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_res1 = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THTensor* arg_res2 = ((THPTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      const char* arg_eigenvectors = (__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_eigenvectors) == Py_True ? V : N;
      const char* arg_upper = (__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_upper) == Py_True ? U : L;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(syev)(LIBRARY_STATE arg_res1, arg_res2, arg_self, arg_eigenvectors, arg_upper);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPTensorClass &&
          (__tuplecount > 0 || __kw_eigenvectors) && PyBool_Check((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_eigenvectors))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_res1 = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THTensor* arg_res2 = ((THPTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      const char* arg_eigenvectors = (__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_eigenvectors) == Py_True ? V : N;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(syev)(LIBRARY_STATE arg_res1, arg_res2, arg_self, arg_eigenvectors, U);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPTensorClass &&
          __kw_upper && PyBool_Check(__kw_upper)) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_res1 = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THTensor* arg_res2 = ((THPTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      const char* arg_upper = __kw_upper == Py_True ? U : L;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(syev)(LIBRARY_STATE arg_res1, arg_res2, arg_self, N, arg_upper);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_eigenvectors) && PyBool_Check((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_eigenvectors)) &&
          (__tuplecount > 1 || __kw_upper) && PyBool_Check((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_upper))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _res1_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_res1_guard.get()) return NULL;
      THPTensor* res1 = _res1_guard.get();
      
      THPTensorPtr _res2_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_res2_guard.get()) return NULL;
      THPTensor* res2 = _res2_guard.get();
      
      
      THTensor* arg_res1 = ((THPTensor*)res1)->cdata;
      THTensor* arg_res2 = ((THPTensor*)res2)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      const char* arg_eigenvectors = (__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_eigenvectors) == Py_True ? V : N;
      const char* arg_upper = (__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_upper) == Py_True ? U : L;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(syev)(LIBRARY_STATE arg_res1, arg_res2, arg_self, arg_eigenvectors, arg_upper);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, res1, res2);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 1 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_res1 = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THTensor* arg_res2 = ((THPTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(syev)(LIBRARY_STATE arg_res1, arg_res2, arg_self, N, U);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_eigenvectors) && PyBool_Check((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_eigenvectors))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _res1_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_res1_guard.get()) return NULL;
      THPTensor* res1 = _res1_guard.get();
      
      THPTensorPtr _res2_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_res2_guard.get()) return NULL;
      THPTensor* res2 = _res2_guard.get();
      
      
      THTensor* arg_res1 = ((THPTensor*)res1)->cdata;
      THTensor* arg_res2 = ((THPTensor*)res2)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      const char* arg_eigenvectors = (__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_eigenvectors) == Py_True ? V : N;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(syev)(LIBRARY_STATE arg_res1, arg_res2, arg_self, arg_eigenvectors, U);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, res1, res2);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          __kw_upper && PyBool_Check(__kw_upper)) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _res1_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_res1_guard.get()) return NULL;
      THPTensor* res1 = _res1_guard.get();
      
      THPTensorPtr _res2_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_res2_guard.get()) return NULL;
      THPTensor* res2 = _res2_guard.get();
      
      
      THTensor* arg_res1 = ((THPTensor*)res1)->cdata;
      THTensor* arg_res2 = ((THPTensor*)res2)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      const char* arg_upper = __kw_upper == Py_True ? U : L;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(syev)(LIBRARY_STATE arg_res1, arg_res2, arg_self, N, arg_upper);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, res1, res2);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _res1_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_res1_guard.get()) return NULL;
      THPTensor* res1 = _res1_guard.get();
      
      THPTensorPtr _res2_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_res2_guard.get()) return NULL;
      THPTensor* res2 = _res2_guard.get();
      
      
      THTensor* arg_res1 = ((THPTensor*)res1)->cdata;
      THTensor* arg_res2 = ((THPTensor*)res2)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(syev)(LIBRARY_STATE arg_res1, arg_res2, arg_self, N, U);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, res1, res2);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "symeig", 4, "(#tuple[" THPTensorStr ", " THPTensorStr "] out)", "(bool upper, #tuple[" THPTensorStr ", " THPTensorStr "] out)", "(bool eigenvectors, #tuple[" THPTensorStr ", " THPTensorStr "] out)", "(bool eigenvectors, bool upper, #tuple[" THPTensorStr ", " THPTensorStr "] out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_FLOAT) || defined(TH_REAL_IS_DOUBLE) || CUDA_FLOAT || CUDA_DOUBLE)
PyObject * THPTensor_stateless_(symeig)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    PyObject *__kw_eigenvectors = NULL;
    PyObject *__kw_upper = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
      __kw_eigenvectors = PyDict_GetItemString(kwargs, "eigenvectors");
      __kw_upper = PyDict_GetItemString(kwargs, "upper");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 4 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_eigenvectors) && PyBool_Check((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_eigenvectors)) &&
          (__tuplecount > 2 || __kw_upper) && PyBool_Check((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_upper))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_res1 = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THTensor* arg_res2 = ((THPTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      const char* arg_eigenvectors = (__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_eigenvectors) == Py_True ? V : N;
      const char* arg_upper = (__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_upper) == Py_True ? U : L;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(syev)(LIBRARY_STATE arg_res1, arg_res2, arg_self, arg_eigenvectors, arg_upper);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_eigenvectors) && PyBool_Check((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_eigenvectors))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_res1 = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THTensor* arg_res2 = ((THPTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      const char* arg_eigenvectors = (__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_eigenvectors) == Py_True ? V : N;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(syev)(LIBRARY_STATE arg_res1, arg_res2, arg_self, arg_eigenvectors, U);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          __kw_upper && PyBool_Check(__kw_upper)) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_res1 = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THTensor* arg_res2 = ((THPTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      const char* arg_upper = __kw_upper == Py_True ? U : L;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(syev)(LIBRARY_STATE arg_res1, arg_res2, arg_self, N, arg_upper);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 3 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_eigenvectors) && PyBool_Check((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_eigenvectors)) &&
          (__tuplecount > 2 || __kw_upper) && PyBool_Check((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_upper))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _res1_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_res1_guard.get()) return NULL;
      THPTensor* res1 = _res1_guard.get();
      
      THPTensorPtr _res2_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_res2_guard.get()) return NULL;
      THPTensor* res2 = _res2_guard.get();
      
      
      THTensor* arg_res1 = ((THPTensor*)res1)->cdata;
      THTensor* arg_res2 = ((THPTensor*)res2)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      const char* arg_eigenvectors = (__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_eigenvectors) == Py_True ? V : N;
      const char* arg_upper = (__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_upper) == Py_True ? U : L;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(syev)(LIBRARY_STATE arg_res1, arg_res2, arg_self, arg_eigenvectors, arg_upper);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, res1, res2);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_res1 = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THTensor* arg_res2 = ((THPTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(syev)(LIBRARY_STATE arg_res1, arg_res2, arg_self, N, U);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_eigenvectors) && PyBool_Check((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_eigenvectors))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _res1_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_res1_guard.get()) return NULL;
      THPTensor* res1 = _res1_guard.get();
      
      THPTensorPtr _res2_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_res2_guard.get()) return NULL;
      THPTensor* res2 = _res2_guard.get();
      
      
      THTensor* arg_res1 = ((THPTensor*)res1)->cdata;
      THTensor* arg_res2 = ((THPTensor*)res2)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      const char* arg_eigenvectors = (__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_eigenvectors) == Py_True ? V : N;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(syev)(LIBRARY_STATE arg_res1, arg_res2, arg_self, arg_eigenvectors, U);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, res1, res2);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          __kw_upper && PyBool_Check(__kw_upper)) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _res1_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_res1_guard.get()) return NULL;
      THPTensor* res1 = _res1_guard.get();
      
      THPTensorPtr _res2_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_res2_guard.get()) return NULL;
      THPTensor* res2 = _res2_guard.get();
      
      
      THTensor* arg_res1 = ((THPTensor*)res1)->cdata;
      THTensor* arg_res2 = ((THPTensor*)res2)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      const char* arg_upper = __kw_upper == Py_True ? U : L;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(syev)(LIBRARY_STATE arg_res1, arg_res2, arg_self, N, arg_upper);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, res1, res2);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _res1_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_res1_guard.get()) return NULL;
      THPTensor* res1 = _res1_guard.get();
      
      THPTensorPtr _res2_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_res2_guard.get()) return NULL;
      THPTensor* res2 = _res2_guard.get();
      
      
      THTensor* arg_res1 = ((THPTensor*)res1)->cdata;
      THTensor* arg_res2 = ((THPTensor*)res2)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(syev)(LIBRARY_STATE arg_res1, arg_res2, arg_self, N, U);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, res1, res2);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.symeig", 4, "(" THPTensorStr " source, #tuple[" THPTensorStr ", " THPTensorStr "] out)", "(" THPTensorStr " source, bool upper, #tuple[" THPTensorStr ", " THPTensorStr "] out)", "(" THPTensorStr " source, bool eigenvectors, #tuple[" THPTensorStr ", " THPTensorStr "] out)", "(" THPTensorStr " source, bool eigenvectors, bool upper, #tuple[" THPTensorStr ", " THPTensorStr "] out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_FLOAT) || defined(TH_REAL_IS_DOUBLE) || CUDA_FLOAT || CUDA_DOUBLE)
PyObject * THPTensor_(eig)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_eigenvectors = NULL;
    if (kwargs) {
      __kw_eigenvectors = PyDict_GetItemString(kwargs, "eigenvectors");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPTensorClass &&
          (__tuplecount > 0 || __kw_eigenvectors) && PyBool_Check((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_eigenvectors))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_res1 = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THTensor* arg_res2 = ((THPTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      const char* arg_eigenvectors = (__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_eigenvectors) == Py_True ? V : N;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(geev)(LIBRARY_STATE arg_res1, arg_res2, arg_self, arg_eigenvectors);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 1 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_res1 = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THTensor* arg_res2 = ((THPTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(geev)(LIBRARY_STATE arg_res1, arg_res2, arg_self, N);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_eigenvectors) && PyBool_Check((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_eigenvectors))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _res1_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_res1_guard.get()) return NULL;
      THPTensor* res1 = _res1_guard.get();
      
      THPTensorPtr _res2_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_res2_guard.get()) return NULL;
      THPTensor* res2 = _res2_guard.get();
      
      
      THTensor* arg_res1 = ((THPTensor*)res1)->cdata;
      THTensor* arg_res2 = ((THPTensor*)res2)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      const char* arg_eigenvectors = (__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_eigenvectors) == Py_True ? V : N;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(geev)(LIBRARY_STATE arg_res1, arg_res2, arg_self, arg_eigenvectors);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, res1, res2);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _res1_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_res1_guard.get()) return NULL;
      THPTensor* res1 = _res1_guard.get();
      
      THPTensorPtr _res2_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_res2_guard.get()) return NULL;
      THPTensor* res2 = _res2_guard.get();
      
      
      THTensor* arg_res1 = ((THPTensor*)res1)->cdata;
      THTensor* arg_res2 = ((THPTensor*)res2)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(geev)(LIBRARY_STATE arg_res1, arg_res2, arg_self, N);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, res1, res2);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "eig", 2, "(#tuple[" THPTensorStr ", " THPTensorStr "] out)", "(bool eigenvectors, #tuple[" THPTensorStr ", " THPTensorStr "] out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_FLOAT) || defined(TH_REAL_IS_DOUBLE) || CUDA_FLOAT || CUDA_DOUBLE)
PyObject * THPTensor_stateless_(eig)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    PyObject *__kw_eigenvectors = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
      __kw_eigenvectors = PyDict_GetItemString(kwargs, "eigenvectors");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_eigenvectors) && PyBool_Check((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_eigenvectors))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_res1 = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THTensor* arg_res2 = ((THPTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      const char* arg_eigenvectors = (__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_eigenvectors) == Py_True ? V : N;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(geev)(LIBRARY_STATE arg_res1, arg_res2, arg_self, arg_eigenvectors);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_res1 = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THTensor* arg_res2 = ((THPTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(geev)(LIBRARY_STATE arg_res1, arg_res2, arg_self, N);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_eigenvectors) && PyBool_Check((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_eigenvectors))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _res1_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_res1_guard.get()) return NULL;
      THPTensor* res1 = _res1_guard.get();
      
      THPTensorPtr _res2_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_res2_guard.get()) return NULL;
      THPTensor* res2 = _res2_guard.get();
      
      
      THTensor* arg_res1 = ((THPTensor*)res1)->cdata;
      THTensor* arg_res2 = ((THPTensor*)res2)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      const char* arg_eigenvectors = (__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_eigenvectors) == Py_True ? V : N;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(geev)(LIBRARY_STATE arg_res1, arg_res2, arg_self, arg_eigenvectors);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, res1, res2);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _res1_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_res1_guard.get()) return NULL;
      THPTensor* res1 = _res1_guard.get();
      
      THPTensorPtr _res2_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_res2_guard.get()) return NULL;
      THPTensor* res2 = _res2_guard.get();
      
      
      THTensor* arg_res1 = ((THPTensor*)res1)->cdata;
      THTensor* arg_res2 = ((THPTensor*)res2)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(geev)(LIBRARY_STATE arg_res1, arg_res2, arg_self, N);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, res1, res2);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.eig", 2, "(" THPTensorStr " source, #tuple[" THPTensorStr ", " THPTensorStr "] out)", "(" THPTensorStr " source, bool eigenvectors, #tuple[" THPTensorStr ", " THPTensorStr "] out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_FLOAT) || defined(TH_REAL_IS_DOUBLE) || CUDA_FLOAT || CUDA_DOUBLE)
PyObject * THPTensor_(svd)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_some = NULL;
    if (kwargs) {
      __kw_some = PyDict_GetItemString(kwargs, "some");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 3 &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 2)) == THPTensorClass &&
          (__tuplecount > 0 || __kw_some) && PyBool_Check((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_some))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_res1 = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THTensor* arg_res2 = ((THPTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_res3 = ((THPTensor*)PyTuple_GET_ITEM(___out, 2))->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      const char* arg_some = (__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_some) == Py_True ? S : A;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(gesvd)(LIBRARY_STATE arg_res1, arg_res2, arg_res3, arg_self, arg_some);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(3, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1), PyTuple_GET_ITEM(___out, 2));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 3 &&
          __argcount == 1 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_res1 = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THTensor* arg_res2 = ((THPTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_res3 = ((THPTensor*)PyTuple_GET_ITEM(___out, 2))->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(gesvd)(LIBRARY_STATE arg_res1, arg_res2, arg_res3, arg_self, S);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(3, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1), PyTuple_GET_ITEM(___out, 2));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_some) && PyBool_Check((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_some))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _res1_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_res1_guard.get()) return NULL;
      THPTensor* res1 = _res1_guard.get();
      
      THPTensorPtr _res2_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_res2_guard.get()) return NULL;
      THPTensor* res2 = _res2_guard.get();
      
      THPTensorPtr _res3_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_res3_guard.get()) return NULL;
      THPTensor* res3 = _res3_guard.get();
      
      
      THTensor* arg_res1 = ((THPTensor*)res1)->cdata;
      THTensor* arg_res2 = ((THPTensor*)res2)->cdata;
      THTensor* arg_res3 = ((THPTensor*)res3)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      const char* arg_some = (__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_some) == Py_True ? S : A;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(gesvd)(LIBRARY_STATE arg_res1, arg_res2, arg_res3, arg_self, arg_some);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(3, res1, res2, res3);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _res1_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_res1_guard.get()) return NULL;
      THPTensor* res1 = _res1_guard.get();
      
      THPTensorPtr _res2_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_res2_guard.get()) return NULL;
      THPTensor* res2 = _res2_guard.get();
      
      THPTensorPtr _res3_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_res3_guard.get()) return NULL;
      THPTensor* res3 = _res3_guard.get();
      
      
      THTensor* arg_res1 = ((THPTensor*)res1)->cdata;
      THTensor* arg_res2 = ((THPTensor*)res2)->cdata;
      THTensor* arg_res3 = ((THPTensor*)res3)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(gesvd)(LIBRARY_STATE arg_res1, arg_res2, arg_res3, arg_self, S);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(3, res1, res2, res3);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "svd", 2, "(#tuple[" THPTensorStr ", " THPTensorStr ", " THPTensorStr "] out)", "(bool some, #tuple[" THPTensorStr ", " THPTensorStr ", " THPTensorStr "] out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_FLOAT) || defined(TH_REAL_IS_DOUBLE) || CUDA_FLOAT || CUDA_DOUBLE)
PyObject * THPTensor_stateless_(svd)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    PyObject *__kw_some = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
      __kw_some = PyDict_GetItemString(kwargs, "some");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 3 &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 2)) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_some) && PyBool_Check((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_some))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_res1 = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THTensor* arg_res2 = ((THPTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_res3 = ((THPTensor*)PyTuple_GET_ITEM(___out, 2))->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      const char* arg_some = (__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_some) == Py_True ? S : A;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(gesvd)(LIBRARY_STATE arg_res1, arg_res2, arg_res3, arg_self, arg_some);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(3, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1), PyTuple_GET_ITEM(___out, 2));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 3 &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 2)) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_res1 = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THTensor* arg_res2 = ((THPTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_res3 = ((THPTensor*)PyTuple_GET_ITEM(___out, 2))->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(gesvd)(LIBRARY_STATE arg_res1, arg_res2, arg_res3, arg_self, S);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(3, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1), PyTuple_GET_ITEM(___out, 2));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_some) && PyBool_Check((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_some))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _res1_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_res1_guard.get()) return NULL;
      THPTensor* res1 = _res1_guard.get();
      
      THPTensorPtr _res2_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_res2_guard.get()) return NULL;
      THPTensor* res2 = _res2_guard.get();
      
      THPTensorPtr _res3_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_res3_guard.get()) return NULL;
      THPTensor* res3 = _res3_guard.get();
      
      
      THTensor* arg_res1 = ((THPTensor*)res1)->cdata;
      THTensor* arg_res2 = ((THPTensor*)res2)->cdata;
      THTensor* arg_res3 = ((THPTensor*)res3)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      const char* arg_some = (__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_some) == Py_True ? S : A;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(gesvd)(LIBRARY_STATE arg_res1, arg_res2, arg_res3, arg_self, arg_some);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(3, res1, res2, res3);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _res1_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_res1_guard.get()) return NULL;
      THPTensor* res1 = _res1_guard.get();
      
      THPTensorPtr _res2_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_res2_guard.get()) return NULL;
      THPTensor* res2 = _res2_guard.get();
      
      THPTensorPtr _res3_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_res3_guard.get()) return NULL;
      THPTensor* res3 = _res3_guard.get();
      
      
      THTensor* arg_res1 = ((THPTensor*)res1)->cdata;
      THTensor* arg_res2 = ((THPTensor*)res2)->cdata;
      THTensor* arg_res3 = ((THPTensor*)res3)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(gesvd)(LIBRARY_STATE arg_res1, arg_res2, arg_res3, arg_self, S);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(3, res1, res2, res3);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.svd", 2, "(" THPTensorStr " source, #tuple[" THPTensorStr ", " THPTensorStr ", " THPTensorStr "] out)", "(" THPTensorStr " source, bool some, #tuple[" THPTensorStr ", " THPTensorStr ", " THPTensorStr "] out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_FLOAT) || defined(TH_REAL_IS_DOUBLE) || CUDA_FLOAT || CUDA_DOUBLE)
PyObject * THPTensor_(inverse)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    
    if (kwargs) {
      
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 1 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_output = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(getri)(LIBRARY_STATE arg_output, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _output_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_output_guard.get()) return NULL;
      THPTensor* output = _output_guard.get();
      
      
      THTensor* arg_output = ((THPTensor*)output)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(getri)(LIBRARY_STATE arg_output, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(output);
        return (PyObject*)(output);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "inverse", 1, "(#" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_FLOAT) || defined(TH_REAL_IS_DOUBLE) || CUDA_FLOAT || CUDA_DOUBLE)
PyObject * THPTensor_stateless_(inverse)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_output = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(getri)(LIBRARY_STATE arg_output, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _output_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_output_guard.get()) return NULL;
      THPTensor* output = _output_guard.get();
      
      
      THTensor* arg_output = ((THPTensor*)output)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(getri)(LIBRARY_STATE arg_output, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(output);
        return (PyObject*)(output);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.inverse", 1, "(" THPTensorStr " source, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_FLOAT) || defined(TH_REAL_IS_DOUBLE) || CUDA_FLOAT || CUDA_DOUBLE)
PyObject * THPTensor_(potrf)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_upper = NULL;
    if (kwargs) {
      __kw_upper = PyDict_GetItemString(kwargs, "upper");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_upper) && PyBool_Check((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_upper))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_output = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      const char* arg_upper = (__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_upper) == Py_True ? U : L;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(potrf)(LIBRARY_STATE arg_output, arg_self, arg_upper);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_upper) && PyBool_Check((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_upper))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _output_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_output_guard.get()) return NULL;
      THPTensor* output = _output_guard.get();
      
      
      THTensor* arg_output = ((THPTensor*)output)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      const char* arg_upper = (__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_upper) == Py_True ? U : L;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(potrf)(LIBRARY_STATE arg_output, arg_self, arg_upper);
        Py_BLOCK_THREADS;
        Py_INCREF(output);
        return (PyObject*)(output);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 1 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_output = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(potrf)(LIBRARY_STATE arg_output, arg_self, U);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _output_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_output_guard.get()) return NULL;
      THPTensor* output = _output_guard.get();
      
      
      THTensor* arg_output = ((THPTensor*)output)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(potrf)(LIBRARY_STATE arg_output, arg_self, U);
        Py_BLOCK_THREADS;
        Py_INCREF(output);
        return (PyObject*)(output);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "potrf", 2, "(#" THPTensorStr " out)", "(bool upper, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_FLOAT) || defined(TH_REAL_IS_DOUBLE) || CUDA_FLOAT || CUDA_DOUBLE)
PyObject * THPTensor_stateless_(potrf)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    PyObject *__kw_upper = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
      __kw_upper = PyDict_GetItemString(kwargs, "upper");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_upper) && PyBool_Check((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_upper))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_output = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      const char* arg_upper = (__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_upper) == Py_True ? U : L;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(potrf)(LIBRARY_STATE arg_output, arg_self, arg_upper);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_upper) && PyBool_Check((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_upper))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _output_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_output_guard.get()) return NULL;
      THPTensor* output = _output_guard.get();
      
      
      THTensor* arg_output = ((THPTensor*)output)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      const char* arg_upper = (__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_upper) == Py_True ? U : L;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(potrf)(LIBRARY_STATE arg_output, arg_self, arg_upper);
        Py_BLOCK_THREADS;
        Py_INCREF(output);
        return (PyObject*)(output);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_output = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(potrf)(LIBRARY_STATE arg_output, arg_self, U);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _output_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_output_guard.get()) return NULL;
      THPTensor* output = _output_guard.get();
      
      
      THTensor* arg_output = ((THPTensor*)output)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(potrf)(LIBRARY_STATE arg_output, arg_self, U);
        Py_BLOCK_THREADS;
        Py_INCREF(output);
        return (PyObject*)(output);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.potrf", 2, "(" THPTensorStr " source, #" THPTensorStr " out)", "(" THPTensorStr " source, bool upper, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_FLOAT) || defined(TH_REAL_IS_DOUBLE) || CUDA_FLOAT || CUDA_DOUBLE)
PyObject * THPTensor_(potrs)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_input2 = NULL;
    PyObject *__kw_upper = NULL;
    if (kwargs) {
      __kw_input2 = PyDict_GetItemString(kwargs, "input2");
      __kw_upper = PyDict_GetItemString(kwargs, "upper");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_input2) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_input2)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_upper) && PyBool_Check((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_upper))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_input2 = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_input2))->cdata;
      const char* arg_upper = (__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_upper) == Py_True ? U : L;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(potrs)(LIBRARY_STATE arg_result, arg_self, arg_input2, arg_upper);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_input2) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_input2)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_upper) && PyBool_Check((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_upper))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_input2 = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_input2))->cdata;
      const char* arg_upper = (__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_upper) == Py_True ? U : L;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(potrs)(LIBRARY_STATE arg_result, arg_self, arg_input2, arg_upper);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_input2) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_input2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_input2 = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_input2))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(potrs)(LIBRARY_STATE arg_result, arg_self, arg_input2, U);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_input2) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_input2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_input2 = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_input2))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(potrs)(LIBRARY_STATE arg_result, arg_self, arg_input2, U);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "potrs", 2, "(" THPTensorStr " input2, #" THPTensorStr " out)", "(" THPTensorStr " input2, bool upper, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_FLOAT) || defined(TH_REAL_IS_DOUBLE) || CUDA_FLOAT || CUDA_DOUBLE)
PyObject * THPTensor_stateless_(potrs)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    PyObject *__kw_input2 = NULL;
    PyObject *__kw_upper = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
      __kw_input2 = PyDict_GetItemString(kwargs, "input2");
      __kw_upper = PyDict_GetItemString(kwargs, "upper");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 4 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_input2) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_input2)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_upper) && PyBool_Check((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_upper))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_input2 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_input2))->cdata;
      const char* arg_upper = (__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_upper) == Py_True ? U : L;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(potrs)(LIBRARY_STATE arg_result, arg_self, arg_input2, arg_upper);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 3 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_input2) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_input2)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_upper) && PyBool_Check((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_upper))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_input2 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_input2))->cdata;
      const char* arg_upper = (__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_upper) == Py_True ? U : L;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(potrs)(LIBRARY_STATE arg_result, arg_self, arg_input2, arg_upper);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_input2) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_input2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_input2 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_input2))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(potrs)(LIBRARY_STATE arg_result, arg_self, arg_input2, U);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_input2) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_input2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_input2 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_input2))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(potrs)(LIBRARY_STATE arg_result, arg_self, arg_input2, U);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.potrs", 2, "(" THPTensorStr " source, " THPTensorStr " input2, #" THPTensorStr " out)", "(" THPTensorStr " source, " THPTensorStr " input2, bool upper, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_FLOAT) || defined(TH_REAL_IS_DOUBLE) || CUDA_FLOAT || CUDA_DOUBLE)
PyObject * THPTensor_(potri)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_upper = NULL;
    if (kwargs) {
      __kw_upper = PyDict_GetItemString(kwargs, "upper");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_upper) && PyBool_Check((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_upper))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_output = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      const char* arg_upper = (__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_upper) == Py_True ? U : L;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(potri)(LIBRARY_STATE arg_output, arg_self, arg_upper);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_upper) && PyBool_Check((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_upper))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _output_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_output_guard.get()) return NULL;
      THPTensor* output = _output_guard.get();
      
      
      THTensor* arg_output = ((THPTensor*)output)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      const char* arg_upper = (__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_upper) == Py_True ? U : L;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(potri)(LIBRARY_STATE arg_output, arg_self, arg_upper);
        Py_BLOCK_THREADS;
        Py_INCREF(output);
        return (PyObject*)(output);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 1 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_output = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(potri)(LIBRARY_STATE arg_output, arg_self, U);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _output_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_output_guard.get()) return NULL;
      THPTensor* output = _output_guard.get();
      
      
      THTensor* arg_output = ((THPTensor*)output)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(potri)(LIBRARY_STATE arg_output, arg_self, U);
        Py_BLOCK_THREADS;
        Py_INCREF(output);
        return (PyObject*)(output);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "potri", 2, "(#" THPTensorStr " out)", "(bool upper, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_FLOAT) || defined(TH_REAL_IS_DOUBLE) || CUDA_FLOAT || CUDA_DOUBLE)
PyObject * THPTensor_stateless_(potri)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    PyObject *__kw_upper = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
      __kw_upper = PyDict_GetItemString(kwargs, "upper");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_upper) && PyBool_Check((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_upper))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_output = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      const char* arg_upper = (__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_upper) == Py_True ? U : L;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(potri)(LIBRARY_STATE arg_output, arg_self, arg_upper);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_upper) && PyBool_Check((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_upper))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _output_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_output_guard.get()) return NULL;
      THPTensor* output = _output_guard.get();
      
      
      THTensor* arg_output = ((THPTensor*)output)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      const char* arg_upper = (__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_upper) == Py_True ? U : L;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(potri)(LIBRARY_STATE arg_output, arg_self, arg_upper);
        Py_BLOCK_THREADS;
        Py_INCREF(output);
        return (PyObject*)(output);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_output = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(potri)(LIBRARY_STATE arg_output, arg_self, U);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _output_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_output_guard.get()) return NULL;
      THPTensor* output = _output_guard.get();
      
      
      THTensor* arg_output = ((THPTensor*)output)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(potri)(LIBRARY_STATE arg_output, arg_self, U);
        Py_BLOCK_THREADS;
        Py_INCREF(output);
        return (PyObject*)(output);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.potri", 2, "(" THPTensorStr " source, #" THPTensorStr " out)", "(" THPTensorStr " source, bool upper, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_FLOAT) || defined(TH_REAL_IS_DOUBLE))
PyObject * THPTensor_(pstrf)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_upper = NULL;
    PyObject *__kw_tol = NULL;
    if (kwargs) {
      __kw_upper = PyDict_GetItemString(kwargs, "upper");
      __kw_tol = PyDict_GetItemString(kwargs, "tol");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPIntegerTensorClass &&
          (__tuplecount > 0 || __kw_upper) && PyBool_Check((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_upper)) &&
          (__tuplecount > 1 || __kw_tol) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_tol))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_res1 = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THIntegerTensor* arg_res2 = ((THPIntegerTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      const char* arg_upper = (__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_upper) == Py_True ? U : L;
      real arg_tol = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_tol));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(pstrf)(LIBRARY_STATE arg_res1, arg_res2, arg_self, arg_upper, arg_tol);
        Py_BLOCK_THREADS;
        THIntTensor_sub(((THPIntTensor*)PyTuple_GET_ITEM(___out, 1))->cdata, ((THPIntTensor*)PyTuple_GET_ITEM(___out, 1))->cdata, 1);
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPIntegerTensorClass &&
          (__tuplecount > 0 || __kw_upper) && PyBool_Check((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_upper))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_res1 = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THIntegerTensor* arg_res2 = ((THPIntegerTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      const char* arg_upper = (__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_upper) == Py_True ? U : L;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(pstrf)(LIBRARY_STATE arg_res1, arg_res2, arg_self, arg_upper, -1);
        Py_BLOCK_THREADS;
        THIntTensor_sub(((THPIntTensor*)PyTuple_GET_ITEM(___out, 1))->cdata, ((THPIntTensor*)PyTuple_GET_ITEM(___out, 1))->cdata, 1);
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPIntegerTensorClass &&
          (__tuplecount > 0 || __kw_tol) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tol))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_res1 = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THIntegerTensor* arg_res2 = ((THPIntegerTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_tol = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tol));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(pstrf)(LIBRARY_STATE arg_res1, arg_res2, arg_self, U, arg_tol);
        Py_BLOCK_THREADS;
        THIntTensor_sub(((THPIntTensor*)PyTuple_GET_ITEM(___out, 1))->cdata, ((THPIntTensor*)PyTuple_GET_ITEM(___out, 1))->cdata, 1);
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_upper) && PyBool_Check((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_upper)) &&
          (__tuplecount > 1 || __kw_tol) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_tol))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _res1_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_res1_guard.get()) return NULL;
      THPTensor* res1 = _res1_guard.get();
      
      #if IS_CUDA
      THCPIntTensorPtr _res2_guard((THCPIntTensor*) THCPIntTensor_NewEmpty());
      if (!_res2_guard.get()) return NULL;
      THCPIntTensor* res2 = _res2_guard.get();
      
      #else
      THPIntTensorPtr _res2_guard((THPIntTensor*) THPIntTensor_NewEmpty());
      if (!_res2_guard.get()) return NULL;
      THPIntTensor* res2 = _res2_guard.get();
      
      #endif
      
      
      THTensor* arg_res1 = ((THPTensor*)res1)->cdata;
      THIntegerTensor* arg_res2 = ((THPIntegerTensor*)res2)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      const char* arg_upper = (__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_upper) == Py_True ? U : L;
      real arg_tol = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_tol));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(pstrf)(LIBRARY_STATE arg_res1, arg_res2, arg_self, arg_upper, arg_tol);
        Py_BLOCK_THREADS;
        THIntTensor_sub(((THPIntTensor*)res2)->cdata, ((THPIntTensor*)res2)->cdata, 1);
        return PyTuple_Pack(2, res1, res2);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 1 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPIntegerTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_res1 = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THIntegerTensor* arg_res2 = ((THPIntegerTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(pstrf)(LIBRARY_STATE arg_res1, arg_res2, arg_self, U, -1);
        Py_BLOCK_THREADS;
        THIntTensor_sub(((THPIntTensor*)PyTuple_GET_ITEM(___out, 1))->cdata, ((THPIntTensor*)PyTuple_GET_ITEM(___out, 1))->cdata, 1);
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_upper) && PyBool_Check((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_upper))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _res1_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_res1_guard.get()) return NULL;
      THPTensor* res1 = _res1_guard.get();
      
      #if IS_CUDA
      THCPIntTensorPtr _res2_guard((THCPIntTensor*) THCPIntTensor_NewEmpty());
      if (!_res2_guard.get()) return NULL;
      THCPIntTensor* res2 = _res2_guard.get();
      
      #else
      THPIntTensorPtr _res2_guard((THPIntTensor*) THPIntTensor_NewEmpty());
      if (!_res2_guard.get()) return NULL;
      THPIntTensor* res2 = _res2_guard.get();
      
      #endif
      
      
      THTensor* arg_res1 = ((THPTensor*)res1)->cdata;
      THIntegerTensor* arg_res2 = ((THPIntegerTensor*)res2)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      const char* arg_upper = (__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_upper) == Py_True ? U : L;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(pstrf)(LIBRARY_STATE arg_res1, arg_res2, arg_self, arg_upper, -1);
        Py_BLOCK_THREADS;
        THIntTensor_sub(((THPIntTensor*)res2)->cdata, ((THPIntTensor*)res2)->cdata, 1);
        return PyTuple_Pack(2, res1, res2);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_tol) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tol))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _res1_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_res1_guard.get()) return NULL;
      THPTensor* res1 = _res1_guard.get();
      
      #if IS_CUDA
      THCPIntTensorPtr _res2_guard((THCPIntTensor*) THCPIntTensor_NewEmpty());
      if (!_res2_guard.get()) return NULL;
      THCPIntTensor* res2 = _res2_guard.get();
      
      #else
      THPIntTensorPtr _res2_guard((THPIntTensor*) THPIntTensor_NewEmpty());
      if (!_res2_guard.get()) return NULL;
      THPIntTensor* res2 = _res2_guard.get();
      
      #endif
      
      
      THTensor* arg_res1 = ((THPTensor*)res1)->cdata;
      THIntegerTensor* arg_res2 = ((THPIntegerTensor*)res2)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_tol = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tol));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(pstrf)(LIBRARY_STATE arg_res1, arg_res2, arg_self, U, arg_tol);
        Py_BLOCK_THREADS;
        THIntTensor_sub(((THPIntTensor*)res2)->cdata, ((THPIntTensor*)res2)->cdata, 1);
        return PyTuple_Pack(2, res1, res2);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _res1_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_res1_guard.get()) return NULL;
      THPTensor* res1 = _res1_guard.get();
      
      #if IS_CUDA
      THCPIntTensorPtr _res2_guard((THCPIntTensor*) THCPIntTensor_NewEmpty());
      if (!_res2_guard.get()) return NULL;
      THCPIntTensor* res2 = _res2_guard.get();
      
      #else
      THPIntTensorPtr _res2_guard((THPIntTensor*) THPIntTensor_NewEmpty());
      if (!_res2_guard.get()) return NULL;
      THPIntTensor* res2 = _res2_guard.get();
      
      #endif
      
      
      THTensor* arg_res1 = ((THPTensor*)res1)->cdata;
      THIntegerTensor* arg_res2 = ((THPIntegerTensor*)res2)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(pstrf)(LIBRARY_STATE arg_res1, arg_res2, arg_self, U, -1);
        Py_BLOCK_THREADS;
        THIntTensor_sub(((THPIntTensor*)res2)->cdata, ((THPIntTensor*)res2)->cdata, 1);
        return PyTuple_Pack(2, res1, res2);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "pstrf", 4, "(#tuple[" THPTensorStr ", " THPModuleStr "IntTensor] out)", "(bool upper, #tuple[" THPTensorStr ", " THPModuleStr "IntTensor] out)", "(" RealStr " tol, #tuple[" THPTensorStr ", " THPModuleStr "IntTensor] out)", "(bool upper, " RealStr " tol, #tuple[" THPTensorStr ", " THPModuleStr "IntTensor] out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_FLOAT) || defined(TH_REAL_IS_DOUBLE))
PyObject * THPTensor_stateless_(pstrf)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    PyObject *__kw_upper = NULL;
    PyObject *__kw_tol = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
      __kw_upper = PyDict_GetItemString(kwargs, "upper");
      __kw_tol = PyDict_GetItemString(kwargs, "tol");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 4 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPIntegerTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_upper) && PyBool_Check((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_upper)) &&
          (__tuplecount > 2 || __kw_tol) && THPUtils_(checkReal)((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_tol))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_res1 = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THIntegerTensor* arg_res2 = ((THPIntegerTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      const char* arg_upper = (__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_upper) == Py_True ? U : L;
      real arg_tol = THPUtils_(unpackReal)((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_tol));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(pstrf)(LIBRARY_STATE arg_res1, arg_res2, arg_self, arg_upper, arg_tol);
        Py_BLOCK_THREADS;
        THIntTensor_sub(((THPIntTensor*)PyTuple_GET_ITEM(___out, 1))->cdata, ((THPIntTensor*)PyTuple_GET_ITEM(___out, 1))->cdata, 1);
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPIntegerTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_upper) && PyBool_Check((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_upper))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_res1 = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THIntegerTensor* arg_res2 = ((THPIntegerTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      const char* arg_upper = (__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_upper) == Py_True ? U : L;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(pstrf)(LIBRARY_STATE arg_res1, arg_res2, arg_self, arg_upper, -1);
        Py_BLOCK_THREADS;
        THIntTensor_sub(((THPIntTensor*)PyTuple_GET_ITEM(___out, 1))->cdata, ((THPIntTensor*)PyTuple_GET_ITEM(___out, 1))->cdata, 1);
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPIntegerTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_tol) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_tol))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_res1 = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THIntegerTensor* arg_res2 = ((THPIntegerTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      real arg_tol = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_tol));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(pstrf)(LIBRARY_STATE arg_res1, arg_res2, arg_self, U, arg_tol);
        Py_BLOCK_THREADS;
        THIntTensor_sub(((THPIntTensor*)PyTuple_GET_ITEM(___out, 1))->cdata, ((THPIntTensor*)PyTuple_GET_ITEM(___out, 1))->cdata, 1);
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 3 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_upper) && PyBool_Check((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_upper)) &&
          (__tuplecount > 2 || __kw_tol) && THPUtils_(checkReal)((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_tol))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _res1_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_res1_guard.get()) return NULL;
      THPTensor* res1 = _res1_guard.get();
      
      #if IS_CUDA
      THCPIntTensorPtr _res2_guard((THCPIntTensor*) THCPIntTensor_NewEmpty());
      if (!_res2_guard.get()) return NULL;
      THCPIntTensor* res2 = _res2_guard.get();
      
      #else
      THPIntTensorPtr _res2_guard((THPIntTensor*) THPIntTensor_NewEmpty());
      if (!_res2_guard.get()) return NULL;
      THPIntTensor* res2 = _res2_guard.get();
      
      #endif
      
      
      THTensor* arg_res1 = ((THPTensor*)res1)->cdata;
      THIntegerTensor* arg_res2 = ((THPIntegerTensor*)res2)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      const char* arg_upper = (__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_upper) == Py_True ? U : L;
      real arg_tol = THPUtils_(unpackReal)((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_tol));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(pstrf)(LIBRARY_STATE arg_res1, arg_res2, arg_self, arg_upper, arg_tol);
        Py_BLOCK_THREADS;
        THIntTensor_sub(((THPIntTensor*)res2)->cdata, ((THPIntTensor*)res2)->cdata, 1);
        return PyTuple_Pack(2, res1, res2);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPIntegerTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_res1 = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THIntegerTensor* arg_res2 = ((THPIntegerTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(pstrf)(LIBRARY_STATE arg_res1, arg_res2, arg_self, U, -1);
        Py_BLOCK_THREADS;
        THIntTensor_sub(((THPIntTensor*)PyTuple_GET_ITEM(___out, 1))->cdata, ((THPIntTensor*)PyTuple_GET_ITEM(___out, 1))->cdata, 1);
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_upper) && PyBool_Check((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_upper))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _res1_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_res1_guard.get()) return NULL;
      THPTensor* res1 = _res1_guard.get();
      
      #if IS_CUDA
      THCPIntTensorPtr _res2_guard((THCPIntTensor*) THCPIntTensor_NewEmpty());
      if (!_res2_guard.get()) return NULL;
      THCPIntTensor* res2 = _res2_guard.get();
      
      #else
      THPIntTensorPtr _res2_guard((THPIntTensor*) THPIntTensor_NewEmpty());
      if (!_res2_guard.get()) return NULL;
      THPIntTensor* res2 = _res2_guard.get();
      
      #endif
      
      
      THTensor* arg_res1 = ((THPTensor*)res1)->cdata;
      THIntegerTensor* arg_res2 = ((THPIntegerTensor*)res2)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      const char* arg_upper = (__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_upper) == Py_True ? U : L;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(pstrf)(LIBRARY_STATE arg_res1, arg_res2, arg_self, arg_upper, -1);
        Py_BLOCK_THREADS;
        THIntTensor_sub(((THPIntTensor*)res2)->cdata, ((THPIntTensor*)res2)->cdata, 1);
        return PyTuple_Pack(2, res1, res2);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_tol) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_tol))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _res1_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_res1_guard.get()) return NULL;
      THPTensor* res1 = _res1_guard.get();
      
      #if IS_CUDA
      THCPIntTensorPtr _res2_guard((THCPIntTensor*) THCPIntTensor_NewEmpty());
      if (!_res2_guard.get()) return NULL;
      THCPIntTensor* res2 = _res2_guard.get();
      
      #else
      THPIntTensorPtr _res2_guard((THPIntTensor*) THPIntTensor_NewEmpty());
      if (!_res2_guard.get()) return NULL;
      THPIntTensor* res2 = _res2_guard.get();
      
      #endif
      
      
      THTensor* arg_res1 = ((THPTensor*)res1)->cdata;
      THIntegerTensor* arg_res2 = ((THPIntegerTensor*)res2)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      real arg_tol = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_tol));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(pstrf)(LIBRARY_STATE arg_res1, arg_res2, arg_self, U, arg_tol);
        Py_BLOCK_THREADS;
        THIntTensor_sub(((THPIntTensor*)res2)->cdata, ((THPIntTensor*)res2)->cdata, 1);
        return PyTuple_Pack(2, res1, res2);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _res1_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_res1_guard.get()) return NULL;
      THPTensor* res1 = _res1_guard.get();
      
      #if IS_CUDA
      THCPIntTensorPtr _res2_guard((THCPIntTensor*) THCPIntTensor_NewEmpty());
      if (!_res2_guard.get()) return NULL;
      THCPIntTensor* res2 = _res2_guard.get();
      
      #else
      THPIntTensorPtr _res2_guard((THPIntTensor*) THPIntTensor_NewEmpty());
      if (!_res2_guard.get()) return NULL;
      THPIntTensor* res2 = _res2_guard.get();
      
      #endif
      
      
      THTensor* arg_res1 = ((THPTensor*)res1)->cdata;
      THIntegerTensor* arg_res2 = ((THPIntegerTensor*)res2)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(pstrf)(LIBRARY_STATE arg_res1, arg_res2, arg_self, U, -1);
        Py_BLOCK_THREADS;
        THIntTensor_sub(((THPIntTensor*)res2)->cdata, ((THPIntTensor*)res2)->cdata, 1);
        return PyTuple_Pack(2, res1, res2);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.pstrf", 4, "(" THPTensorStr " source, #tuple[" THPTensorStr ", " THPModuleStr "IntTensor] out)", "(" THPTensorStr " source, bool upper, #tuple[" THPTensorStr ", " THPModuleStr "IntTensor] out)", "(" THPTensorStr " source, " RealStr " tol, #tuple[" THPTensorStr ", " THPModuleStr "IntTensor] out)", "(" THPTensorStr " source, bool upper, " RealStr " tol, #tuple[" THPTensorStr ", " THPModuleStr "IntTensor] out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_FLOAT) || defined(TH_REAL_IS_DOUBLE) || CUDA_FLOAT || CUDA_DOUBLE)
PyObject * THPTensor_(qr)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    
    if (kwargs) {
      
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 1 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_res1 = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THTensor* arg_res2 = ((THPTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(qr)(LIBRARY_STATE arg_res1, arg_res2, arg_self);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _res1_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_res1_guard.get()) return NULL;
      THPTensor* res1 = _res1_guard.get();
      
      THPTensorPtr _res2_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_res2_guard.get()) return NULL;
      THPTensor* res2 = _res2_guard.get();
      
      
      THTensor* arg_res1 = ((THPTensor*)res1)->cdata;
      THTensor* arg_res2 = ((THPTensor*)res2)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(qr)(LIBRARY_STATE arg_res1, arg_res2, arg_self);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, res1, res2);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "qr", 1, "(#tuple[" THPTensorStr ", " THPTensorStr "] out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_FLOAT) || defined(TH_REAL_IS_DOUBLE) || CUDA_FLOAT || CUDA_DOUBLE)
PyObject * THPTensor_stateless_(qr)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_res1 = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THTensor* arg_res2 = ((THPTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(qr)(LIBRARY_STATE arg_res1, arg_res2, arg_self);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _res1_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_res1_guard.get()) return NULL;
      THPTensor* res1 = _res1_guard.get();
      
      THPTensorPtr _res2_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_res2_guard.get()) return NULL;
      THPTensor* res2 = _res2_guard.get();
      
      
      THTensor* arg_res1 = ((THPTensor*)res1)->cdata;
      THTensor* arg_res2 = ((THPTensor*)res2)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(qr)(LIBRARY_STATE arg_res1, arg_res2, arg_self);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, res1, res2);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.qr", 1, "(" THPTensorStr " source, #tuple[" THPTensorStr ", " THPTensorStr "] out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_FLOAT) || defined(TH_REAL_IS_DOUBLE) || CUDA_FLOAT || CUDA_DOUBLE)
PyObject * THPTensor_(geqrf)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    
    if (kwargs) {
      
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 1 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_res1 = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THTensor* arg_res2 = ((THPTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(geqrf)(LIBRARY_STATE arg_res1, arg_res2, arg_self);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _res1_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_res1_guard.get()) return NULL;
      THPTensor* res1 = _res1_guard.get();
      
      THPTensorPtr _res2_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_res2_guard.get()) return NULL;
      THPTensor* res2 = _res2_guard.get();
      
      
      THTensor* arg_res1 = ((THPTensor*)res1)->cdata;
      THTensor* arg_res2 = ((THPTensor*)res2)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(geqrf)(LIBRARY_STATE arg_res1, arg_res2, arg_self);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, res1, res2);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "geqrf", 1, "(#tuple[" THPTensorStr ", " THPTensorStr "] out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_FLOAT) || defined(TH_REAL_IS_DOUBLE) || CUDA_FLOAT || CUDA_DOUBLE)
PyObject * THPTensor_stateless_(geqrf)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_res1 = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THTensor* arg_res2 = ((THPTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(geqrf)(LIBRARY_STATE arg_res1, arg_res2, arg_self);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _res1_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_res1_guard.get()) return NULL;
      THPTensor* res1 = _res1_guard.get();
      
      THPTensorPtr _res2_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_res2_guard.get()) return NULL;
      THPTensor* res2 = _res2_guard.get();
      
      
      THTensor* arg_res1 = ((THPTensor*)res1)->cdata;
      THTensor* arg_res2 = ((THPTensor*)res2)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(geqrf)(LIBRARY_STATE arg_res1, arg_res2, arg_self);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, res1, res2);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.geqrf", 1, "(" THPTensorStr " source, #tuple[" THPTensorStr ", " THPTensorStr "] out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_FLOAT) || defined(TH_REAL_IS_DOUBLE))
PyObject * THPTensor_(orgqr)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_input2 = NULL;
    if (kwargs) {
      __kw_input2 = PyDict_GetItemString(kwargs, "input2");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_input2) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_input2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_input2 = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_input2))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(orgqr)(LIBRARY_STATE arg_result, arg_self, arg_input2);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, ___out, self);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_input2) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_input2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_input2 = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_input2))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(orgqr)(LIBRARY_STATE arg_result, arg_self, arg_input2);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, result, self);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "orgqr", 1, "(" THPTensorStr " input2, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_FLOAT) || defined(TH_REAL_IS_DOUBLE))
PyObject * THPTensor_stateless_(orgqr)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    PyObject *__kw_input2 = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
      __kw_input2 = PyDict_GetItemString(kwargs, "input2");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_input2) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_input2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_input2 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_input2))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(orgqr)(LIBRARY_STATE arg_result, arg_self, arg_input2);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, ___out, (__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_input2) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_input2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_input2 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_input2))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(orgqr)(LIBRARY_STATE arg_result, arg_self, arg_input2);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, result, (__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.orgqr", 1, "(" THPTensorStr " source, " THPTensorStr " input2, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_FLOAT) || defined(TH_REAL_IS_DOUBLE))
PyObject * THPTensor_(ormqr)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_input2 = NULL;
    PyObject *__kw_input3 = NULL;
    PyObject *__kw_left = NULL;
    PyObject *__kw_transpose = NULL;
    if (kwargs) {
      __kw_input2 = PyDict_GetItemString(kwargs, "input2");
      __kw_input3 = PyDict_GetItemString(kwargs, "input3");
      __kw_left = PyDict_GetItemString(kwargs, "left");
      __kw_transpose = PyDict_GetItemString(kwargs, "transpose");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 5 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_input2) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_input2)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_input3) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_input3)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_left) && PyBool_Check((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_left)) &&
          (__tuplecount > 3 || __kw_transpose) && PyBool_Check((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_transpose))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_input2 = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_input2))->cdata;
      THTensor* arg_input3 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_input3))->cdata;
      const char* arg_left = (__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_left) == Py_True ? L : R;
      const char* arg_transpose = (__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_transpose) == Py_True ? T : N;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(ormqr)(LIBRARY_STATE arg_result, arg_self, arg_input2, arg_input3, arg_left, arg_transpose);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, ___out, self);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 4 &&
          (__tuplecount > 0 || __kw_input2) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_input2)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_input3) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_input3)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_left) && PyBool_Check((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_left)) &&
          (__tuplecount > 3 || __kw_transpose) && PyBool_Check((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_transpose))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_input2 = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_input2))->cdata;
      THTensor* arg_input3 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_input3))->cdata;
      const char* arg_left = (__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_left) == Py_True ? L : R;
      const char* arg_transpose = (__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_transpose) == Py_True ? T : N;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(ormqr)(LIBRARY_STATE arg_result, arg_self, arg_input2, arg_input3, arg_left, arg_transpose);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, result, self);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 4 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_input2) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_input2)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_input3) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_input3)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_left) && PyBool_Check((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_left))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_input2 = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_input2))->cdata;
      THTensor* arg_input3 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_input3))->cdata;
      const char* arg_left = (__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_left) == Py_True ? L : R;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(ormqr)(LIBRARY_STATE arg_result, arg_self, arg_input2, arg_input3, arg_left, N);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, ___out, self);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 4 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_input2) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_input2)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_input3) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_input3)) == THPTensorClass &&
          __kw_transpose && PyBool_Check(__kw_transpose)) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_input2 = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_input2))->cdata;
      THTensor* arg_input3 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_input3))->cdata;
      const char* arg_transpose = __kw_transpose == Py_True ? T : N;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(ormqr)(LIBRARY_STATE arg_result, arg_self, arg_input2, arg_input3, L, arg_transpose);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, ___out, self);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 3 &&
          (__tuplecount > 0 || __kw_input2) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_input2)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_input3) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_input3)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_left) && PyBool_Check((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_left))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_input2 = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_input2))->cdata;
      THTensor* arg_input3 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_input3))->cdata;
      const char* arg_left = (__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_left) == Py_True ? L : R;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(ormqr)(LIBRARY_STATE arg_result, arg_self, arg_input2, arg_input3, arg_left, N);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, result, self);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 3 &&
          (__tuplecount > 0 || __kw_input2) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_input2)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_input3) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_input3)) == THPTensorClass &&
          __kw_transpose && PyBool_Check(__kw_transpose)) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_input2 = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_input2))->cdata;
      THTensor* arg_input3 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_input3))->cdata;
      const char* arg_transpose = __kw_transpose == Py_True ? T : N;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(ormqr)(LIBRARY_STATE arg_result, arg_self, arg_input2, arg_input3, L, arg_transpose);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, result, self);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_input2) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_input2)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_input3) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_input3)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_input2 = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_input2))->cdata;
      THTensor* arg_input3 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_input3))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(ormqr)(LIBRARY_STATE arg_result, arg_self, arg_input2, arg_input3, L, N);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, ___out, self);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_input2) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_input2)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_input3) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_input3)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_input2 = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_input2))->cdata;
      THTensor* arg_input3 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_input3))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(ormqr)(LIBRARY_STATE arg_result, arg_self, arg_input2, arg_input3, L, N);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, result, self);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "ormqr", 4, "(" THPTensorStr " input2, " THPTensorStr " input3, #" THPTensorStr " out)", "(" THPTensorStr " input2, " THPTensorStr " input3, bool left, #" THPTensorStr " out)", "(" THPTensorStr " input2, " THPTensorStr " input3, bool transpose, #" THPTensorStr " out)", "(" THPTensorStr " input2, " THPTensorStr " input3, bool left, bool transpose, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_FLOAT) || defined(TH_REAL_IS_DOUBLE))
PyObject * THPTensor_stateless_(ormqr)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    PyObject *__kw_input2 = NULL;
    PyObject *__kw_input3 = NULL;
    PyObject *__kw_left = NULL;
    PyObject *__kw_transpose = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
      __kw_input2 = PyDict_GetItemString(kwargs, "input2");
      __kw_input3 = PyDict_GetItemString(kwargs, "input3");
      __kw_left = PyDict_GetItemString(kwargs, "left");
      __kw_transpose = PyDict_GetItemString(kwargs, "transpose");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 6 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_input2) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_input2)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_input3) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_input3)) == THPTensorClass &&
          (__tuplecount > 3 || __kw_left) && PyBool_Check((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_left)) &&
          (__tuplecount > 4 || __kw_transpose) && PyBool_Check((__tuplecount > 4 ? PyTuple_GET_ITEM(args, 4) : __kw_transpose))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_input2 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_input2))->cdata;
      THTensor* arg_input3 = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_input3))->cdata;
      const char* arg_left = (__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_left) == Py_True ? L : R;
      const char* arg_transpose = (__tuplecount > 4 ? PyTuple_GET_ITEM(args, 4) : __kw_transpose) == Py_True ? T : N;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(ormqr)(LIBRARY_STATE arg_result, arg_self, arg_input2, arg_input3, arg_left, arg_transpose);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, ___out, (__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 5 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_input2) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_input2)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_input3) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_input3)) == THPTensorClass &&
          (__tuplecount > 3 || __kw_left) && PyBool_Check((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_left)) &&
          (__tuplecount > 4 || __kw_transpose) && PyBool_Check((__tuplecount > 4 ? PyTuple_GET_ITEM(args, 4) : __kw_transpose))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_input2 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_input2))->cdata;
      THTensor* arg_input3 = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_input3))->cdata;
      const char* arg_left = (__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_left) == Py_True ? L : R;
      const char* arg_transpose = (__tuplecount > 4 ? PyTuple_GET_ITEM(args, 4) : __kw_transpose) == Py_True ? T : N;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(ormqr)(LIBRARY_STATE arg_result, arg_self, arg_input2, arg_input3, arg_left, arg_transpose);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, result, (__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 5 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_input2) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_input2)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_input3) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_input3)) == THPTensorClass &&
          (__tuplecount > 3 || __kw_left) && PyBool_Check((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_left))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_input2 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_input2))->cdata;
      THTensor* arg_input3 = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_input3))->cdata;
      const char* arg_left = (__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_left) == Py_True ? L : R;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(ormqr)(LIBRARY_STATE arg_result, arg_self, arg_input2, arg_input3, arg_left, N);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, ___out, (__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 5 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_input2) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_input2)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_input3) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_input3)) == THPTensorClass &&
          __kw_transpose && PyBool_Check(__kw_transpose)) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_input2 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_input2))->cdata;
      THTensor* arg_input3 = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_input3))->cdata;
      const char* arg_transpose = __kw_transpose == Py_True ? T : N;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(ormqr)(LIBRARY_STATE arg_result, arg_self, arg_input2, arg_input3, L, arg_transpose);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, ___out, (__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 4 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_input2) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_input2)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_input3) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_input3)) == THPTensorClass &&
          (__tuplecount > 3 || __kw_left) && PyBool_Check((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_left))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_input2 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_input2))->cdata;
      THTensor* arg_input3 = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_input3))->cdata;
      const char* arg_left = (__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_left) == Py_True ? L : R;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(ormqr)(LIBRARY_STATE arg_result, arg_self, arg_input2, arg_input3, arg_left, N);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, result, (__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 4 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_input2) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_input2)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_input3) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_input3)) == THPTensorClass &&
          __kw_transpose && PyBool_Check(__kw_transpose)) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_input2 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_input2))->cdata;
      THTensor* arg_input3 = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_input3))->cdata;
      const char* arg_transpose = __kw_transpose == Py_True ? T : N;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(ormqr)(LIBRARY_STATE arg_result, arg_self, arg_input2, arg_input3, L, arg_transpose);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, result, (__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 4 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_input2) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_input2)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_input3) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_input3)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_input2 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_input2))->cdata;
      THTensor* arg_input3 = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_input3))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(ormqr)(LIBRARY_STATE arg_result, arg_self, arg_input2, arg_input3, L, N);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, ___out, (__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 3 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_input2) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_input2)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_input3) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_input3)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_input2 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_input2))->cdata;
      THTensor* arg_input3 = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_input3))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(ormqr)(LIBRARY_STATE arg_result, arg_self, arg_input2, arg_input3, L, N);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, result, (__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.ormqr", 4, "(" THPTensorStr " source, " THPTensorStr " input2, " THPTensorStr " input3, #" THPTensorStr " out)", "(" THPTensorStr " source, " THPTensorStr " input2, " THPTensorStr " input3, bool left, #" THPTensorStr " out)", "(" THPTensorStr " source, " THPTensorStr " input2, " THPTensorStr " input3, bool transpose, #" THPTensorStr " out)", "(" THPTensorStr " source, " THPTensorStr " input2, " THPTensorStr " input3, bool left, bool transpose, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_(btrifact)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_info = NULL;
    PyObject *__kw_pivot = NULL;
    if (kwargs) {
      __kw_info = PyDict_GetItemString(kwargs, "info");
      __kw_pivot = PyDict_GetItemString(kwargs, "pivot");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPIntegerTensorClass &&
          __kw_info && (PyObject*)Py_TYPE(__kw_info) == THPIntegerTensorClass &&
          __kw_pivot && PyBool_Check(__kw_pivot)) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THIntegerTensor* arg_pivots = ((THPIntegerTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THIntegerTensor* arg_info = ((THPIntegerTensor*)__kw_info)->cdata;
      bool arg_pivot = (__kw_pivot == Py_True ? true : false);
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(btrifact)(LIBRARY_STATE arg_result, arg_pivots, arg_info, arg_pivot, arg_self);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPIntegerTensorClass &&
          __kw_info && (PyObject*)Py_TYPE(__kw_info) == THPIntegerTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THIntegerTensor* arg_pivots = ((THPIntegerTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THIntegerTensor* arg_info = ((THPIntegerTensor*)__kw_info)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(btrifact)(LIBRARY_STATE arg_result, arg_pivots, arg_info, true, arg_self);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPIntegerTensorClass &&
          __kw_pivot && PyBool_Check(__kw_pivot)) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THIntegerTensor* arg_pivots = ((THPIntegerTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      bool arg_pivot = (__kw_pivot == Py_True ? true : false);
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(btrifact)(LIBRARY_STATE arg_result, arg_pivots, NULL, arg_pivot, arg_self);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          __kw_info && (PyObject*)Py_TYPE(__kw_info) == THPIntegerTensorClass &&
          __kw_pivot && PyBool_Check(__kw_pivot)) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      #if IS_CUDA
      THCPIntTensorPtr _pivots_guard((THCPIntTensor*) THCPIntTensor_NewEmpty());
      if (!_pivots_guard.get()) return NULL;
      THCPIntTensor* pivots = _pivots_guard.get();
      
      #else
      THPIntTensorPtr _pivots_guard((THPIntTensor*) THPIntTensor_NewEmpty());
      if (!_pivots_guard.get()) return NULL;
      THPIntTensor* pivots = _pivots_guard.get();
      
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THIntegerTensor* arg_pivots = ((THPIntegerTensor*)pivots)->cdata;
      THIntegerTensor* arg_info = ((THPIntegerTensor*)__kw_info)->cdata;
      bool arg_pivot = (__kw_pivot == Py_True ? true : false);
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(btrifact)(LIBRARY_STATE arg_result, arg_pivots, arg_info, arg_pivot, arg_self);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, result, pivots);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 1 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPIntegerTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THIntegerTensor* arg_pivots = ((THPIntegerTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(btrifact)(LIBRARY_STATE arg_result, arg_pivots, NULL, true, arg_self);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          __kw_info && (PyObject*)Py_TYPE(__kw_info) == THPIntegerTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      #if IS_CUDA
      THCPIntTensorPtr _pivots_guard((THCPIntTensor*) THCPIntTensor_NewEmpty());
      if (!_pivots_guard.get()) return NULL;
      THCPIntTensor* pivots = _pivots_guard.get();
      
      #else
      THPIntTensorPtr _pivots_guard((THPIntTensor*) THPIntTensor_NewEmpty());
      if (!_pivots_guard.get()) return NULL;
      THPIntTensor* pivots = _pivots_guard.get();
      
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THIntegerTensor* arg_pivots = ((THPIntegerTensor*)pivots)->cdata;
      THIntegerTensor* arg_info = ((THPIntegerTensor*)__kw_info)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(btrifact)(LIBRARY_STATE arg_result, arg_pivots, arg_info, true, arg_self);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, result, pivots);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          __kw_pivot && PyBool_Check(__kw_pivot)) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      #if IS_CUDA
      THCPIntTensorPtr _pivots_guard((THCPIntTensor*) THCPIntTensor_NewEmpty());
      if (!_pivots_guard.get()) return NULL;
      THCPIntTensor* pivots = _pivots_guard.get();
      
      #else
      THPIntTensorPtr _pivots_guard((THPIntTensor*) THPIntTensor_NewEmpty());
      if (!_pivots_guard.get()) return NULL;
      THPIntTensor* pivots = _pivots_guard.get();
      
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THIntegerTensor* arg_pivots = ((THPIntegerTensor*)pivots)->cdata;
      bool arg_pivot = (__kw_pivot == Py_True ? true : false);
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(btrifact)(LIBRARY_STATE arg_result, arg_pivots, NULL, arg_pivot, arg_self);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, result, pivots);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      #if IS_CUDA
      THCPIntTensorPtr _pivots_guard((THCPIntTensor*) THCPIntTensor_NewEmpty());
      if (!_pivots_guard.get()) return NULL;
      THCPIntTensor* pivots = _pivots_guard.get();
      
      #else
      THPIntTensorPtr _pivots_guard((THPIntTensor*) THPIntTensor_NewEmpty());
      if (!_pivots_guard.get()) return NULL;
      THPIntTensor* pivots = _pivots_guard.get();
      
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THIntegerTensor* arg_pivots = ((THPIntegerTensor*)pivots)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(btrifact)(LIBRARY_STATE arg_result, arg_pivots, NULL, true, arg_self);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, result, pivots);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "btrifact", 4, "(#tuple[" THPTensorStr ", " THPModuleStr "IntTensor] out)", "(bool pivot, #tuple[" THPTensorStr ", " THPModuleStr "IntTensor] out)", "(" THPModuleStr "IntTensor info, #tuple[" THPTensorStr ", " THPModuleStr "IntTensor] out)", "(" THPModuleStr "IntTensor info, bool pivot, #tuple[" THPTensorStr ", " THPModuleStr "IntTensor] out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_stateless_(btrifact)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_info = NULL;
    PyObject *__kw_pivot = NULL;
    PyObject *__kw_source = NULL;
    if (kwargs) {
      __kw_info = PyDict_GetItemString(kwargs, "info");
      __kw_pivot = PyDict_GetItemString(kwargs, "pivot");
      __kw_source = PyDict_GetItemString(kwargs, "source");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 4 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPIntegerTensorClass &&
          __kw_info && (PyObject*)Py_TYPE(__kw_info) == THPIntegerTensorClass &&
          __kw_pivot && PyBool_Check(__kw_pivot) &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THIntegerTensor* arg_pivots = ((THPIntegerTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THIntegerTensor* arg_info = ((THPIntegerTensor*)__kw_info)->cdata;
      bool arg_pivot = (__kw_pivot == Py_True ? true : false);
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(btrifact)(LIBRARY_STATE arg_result, arg_pivots, arg_info, arg_pivot, arg_self);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPIntegerTensorClass &&
          __kw_info && (PyObject*)Py_TYPE(__kw_info) == THPIntegerTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THIntegerTensor* arg_pivots = ((THPIntegerTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THIntegerTensor* arg_info = ((THPIntegerTensor*)__kw_info)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(btrifact)(LIBRARY_STATE arg_result, arg_pivots, arg_info, true, arg_self);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPIntegerTensorClass &&
          __kw_pivot && PyBool_Check(__kw_pivot) &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THIntegerTensor* arg_pivots = ((THPIntegerTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      bool arg_pivot = (__kw_pivot == Py_True ? true : false);
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(btrifact)(LIBRARY_STATE arg_result, arg_pivots, NULL, arg_pivot, arg_self);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 3 &&
          __kw_info && (PyObject*)Py_TYPE(__kw_info) == THPIntegerTensorClass &&
          __kw_pivot && PyBool_Check(__kw_pivot) &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      #if IS_CUDA
      THCPIntTensorPtr _pivots_guard((THCPIntTensor*) THCPIntTensor_NewEmpty());
      if (!_pivots_guard.get()) return NULL;
      THCPIntTensor* pivots = _pivots_guard.get();
      
      #else
      THPIntTensorPtr _pivots_guard((THPIntTensor*) THPIntTensor_NewEmpty());
      if (!_pivots_guard.get()) return NULL;
      THPIntTensor* pivots = _pivots_guard.get();
      
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THIntegerTensor* arg_pivots = ((THPIntegerTensor*)pivots)->cdata;
      THIntegerTensor* arg_info = ((THPIntegerTensor*)__kw_info)->cdata;
      bool arg_pivot = (__kw_pivot == Py_True ? true : false);
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(btrifact)(LIBRARY_STATE arg_result, arg_pivots, arg_info, arg_pivot, arg_self);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, result, pivots);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPIntegerTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THIntegerTensor* arg_pivots = ((THPIntegerTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(btrifact)(LIBRARY_STATE arg_result, arg_pivots, NULL, true, arg_self);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          __kw_info && (PyObject*)Py_TYPE(__kw_info) == THPIntegerTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      #if IS_CUDA
      THCPIntTensorPtr _pivots_guard((THCPIntTensor*) THCPIntTensor_NewEmpty());
      if (!_pivots_guard.get()) return NULL;
      THCPIntTensor* pivots = _pivots_guard.get();
      
      #else
      THPIntTensorPtr _pivots_guard((THPIntTensor*) THPIntTensor_NewEmpty());
      if (!_pivots_guard.get()) return NULL;
      THPIntTensor* pivots = _pivots_guard.get();
      
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THIntegerTensor* arg_pivots = ((THPIntegerTensor*)pivots)->cdata;
      THIntegerTensor* arg_info = ((THPIntegerTensor*)__kw_info)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(btrifact)(LIBRARY_STATE arg_result, arg_pivots, arg_info, true, arg_self);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, result, pivots);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          __kw_pivot && PyBool_Check(__kw_pivot) &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      #if IS_CUDA
      THCPIntTensorPtr _pivots_guard((THCPIntTensor*) THCPIntTensor_NewEmpty());
      if (!_pivots_guard.get()) return NULL;
      THCPIntTensor* pivots = _pivots_guard.get();
      
      #else
      THPIntTensorPtr _pivots_guard((THPIntTensor*) THPIntTensor_NewEmpty());
      if (!_pivots_guard.get()) return NULL;
      THPIntTensor* pivots = _pivots_guard.get();
      
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THIntegerTensor* arg_pivots = ((THPIntegerTensor*)pivots)->cdata;
      bool arg_pivot = (__kw_pivot == Py_True ? true : false);
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(btrifact)(LIBRARY_STATE arg_result, arg_pivots, NULL, arg_pivot, arg_self);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, result, pivots);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      #if IS_CUDA
      THCPIntTensorPtr _pivots_guard((THCPIntTensor*) THCPIntTensor_NewEmpty());
      if (!_pivots_guard.get()) return NULL;
      THCPIntTensor* pivots = _pivots_guard.get();
      
      #else
      THPIntTensorPtr _pivots_guard((THPIntTensor*) THPIntTensor_NewEmpty());
      if (!_pivots_guard.get()) return NULL;
      THPIntTensor* pivots = _pivots_guard.get();
      
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THIntegerTensor* arg_pivots = ((THPIntegerTensor*)pivots)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(btrifact)(LIBRARY_STATE arg_result, arg_pivots, NULL, true, arg_self);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, result, pivots);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.btrifact", 4, "(" THPTensorStr " source, #tuple[" THPTensorStr ", " THPModuleStr "IntTensor] out)", "(bool pivot, " THPTensorStr " source, #tuple[" THPTensorStr ", " THPModuleStr "IntTensor] out)", "(" THPModuleStr "IntTensor info, " THPTensorStr " source, #tuple[" THPTensorStr ", " THPModuleStr "IntTensor] out)", "(" THPModuleStr "IntTensor info, bool pivot, " THPTensorStr " source, #tuple[" THPTensorStr ", " THPModuleStr "IntTensor] out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_(btrifact_with_info)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_pivot = NULL;
    if (kwargs) {
      __kw_pivot = PyDict_GetItemString(kwargs, "pivot");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 3 &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPIntegerTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 2)) == THPIntegerTensorClass &&
          __kw_pivot && PyBool_Check(__kw_pivot)) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THIntegerTensor* arg_pivots = ((THPIntegerTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THIntegerTensor* arg_info = ((THPIntegerTensor*)PyTuple_GET_ITEM(___out, 2))->cdata;
      bool arg_pivot = (__kw_pivot == Py_True ? true : false);
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(btrifact)(LIBRARY_STATE arg_result, arg_pivots, arg_info, arg_pivot, arg_self);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(3, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1), PyTuple_GET_ITEM(___out, 2));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 3 &&
          __argcount == 1 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPIntegerTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 2)) == THPIntegerTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THIntegerTensor* arg_pivots = ((THPIntegerTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THIntegerTensor* arg_info = ((THPIntegerTensor*)PyTuple_GET_ITEM(___out, 2))->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(btrifact)(LIBRARY_STATE arg_result, arg_pivots, arg_info, true, arg_self);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(3, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1), PyTuple_GET_ITEM(___out, 2));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          __kw_pivot && PyBool_Check(__kw_pivot)) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      #if IS_CUDA
      THCPIntTensorPtr _pivots_guard((THCPIntTensor*) THCPIntTensor_NewEmpty());
      if (!_pivots_guard.get()) return NULL;
      THCPIntTensor* pivots = _pivots_guard.get();
      
      #else
      THPIntTensorPtr _pivots_guard((THPIntTensor*) THPIntTensor_NewEmpty());
      if (!_pivots_guard.get()) return NULL;
      THPIntTensor* pivots = _pivots_guard.get();
      
      #endif
      
      #if IS_CUDA
      THCPIntTensorPtr _info_guard((THCPIntTensor*) THCPIntTensor_NewEmpty());
      if (!_info_guard.get()) return NULL;
      THCPIntTensor* info = _info_guard.get();
      
      #else
      THPIntTensorPtr _info_guard((THPIntTensor*) THPIntTensor_NewEmpty());
      if (!_info_guard.get()) return NULL;
      THPIntTensor* info = _info_guard.get();
      
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THIntegerTensor* arg_pivots = ((THPIntegerTensor*)pivots)->cdata;
      THIntegerTensor* arg_info = ((THPIntegerTensor*)info)->cdata;
      bool arg_pivot = (__kw_pivot == Py_True ? true : false);
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(btrifact)(LIBRARY_STATE arg_result, arg_pivots, arg_info, arg_pivot, arg_self);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(3, result, pivots, info);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      #if IS_CUDA
      THCPIntTensorPtr _pivots_guard((THCPIntTensor*) THCPIntTensor_NewEmpty());
      if (!_pivots_guard.get()) return NULL;
      THCPIntTensor* pivots = _pivots_guard.get();
      
      #else
      THPIntTensorPtr _pivots_guard((THPIntTensor*) THPIntTensor_NewEmpty());
      if (!_pivots_guard.get()) return NULL;
      THPIntTensor* pivots = _pivots_guard.get();
      
      #endif
      
      #if IS_CUDA
      THCPIntTensorPtr _info_guard((THCPIntTensor*) THCPIntTensor_NewEmpty());
      if (!_info_guard.get()) return NULL;
      THCPIntTensor* info = _info_guard.get();
      
      #else
      THPIntTensorPtr _info_guard((THPIntTensor*) THPIntTensor_NewEmpty());
      if (!_info_guard.get()) return NULL;
      THPIntTensor* info = _info_guard.get();
      
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THIntegerTensor* arg_pivots = ((THPIntegerTensor*)pivots)->cdata;
      THIntegerTensor* arg_info = ((THPIntegerTensor*)info)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(btrifact)(LIBRARY_STATE arg_result, arg_pivots, arg_info, true, arg_self);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(3, result, pivots, info);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "btrifact_with_info", 2, "(#tuple[" THPTensorStr ", " THPModuleStr "IntTensor, " THPModuleStr "IntTensor] out)", "(bool pivot, #tuple[" THPTensorStr ", " THPModuleStr "IntTensor, " THPModuleStr "IntTensor] out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_stateless_(btrifact_with_info)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_pivot = NULL;
    PyObject *__kw_source = NULL;
    if (kwargs) {
      __kw_pivot = PyDict_GetItemString(kwargs, "pivot");
      __kw_source = PyDict_GetItemString(kwargs, "source");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 3 &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPIntegerTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 2)) == THPIntegerTensorClass &&
          __kw_pivot && PyBool_Check(__kw_pivot) &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THIntegerTensor* arg_pivots = ((THPIntegerTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THIntegerTensor* arg_info = ((THPIntegerTensor*)PyTuple_GET_ITEM(___out, 2))->cdata;
      bool arg_pivot = (__kw_pivot == Py_True ? true : false);
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(btrifact)(LIBRARY_STATE arg_result, arg_pivots, arg_info, arg_pivot, arg_self);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(3, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1), PyTuple_GET_ITEM(___out, 2));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 3 &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPIntegerTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 2)) == THPIntegerTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THIntegerTensor* arg_pivots = ((THPIntegerTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THIntegerTensor* arg_info = ((THPIntegerTensor*)PyTuple_GET_ITEM(___out, 2))->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(btrifact)(LIBRARY_STATE arg_result, arg_pivots, arg_info, true, arg_self);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(3, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1), PyTuple_GET_ITEM(___out, 2));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          __kw_pivot && PyBool_Check(__kw_pivot) &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      #if IS_CUDA
      THCPIntTensorPtr _pivots_guard((THCPIntTensor*) THCPIntTensor_NewEmpty());
      if (!_pivots_guard.get()) return NULL;
      THCPIntTensor* pivots = _pivots_guard.get();
      
      #else
      THPIntTensorPtr _pivots_guard((THPIntTensor*) THPIntTensor_NewEmpty());
      if (!_pivots_guard.get()) return NULL;
      THPIntTensor* pivots = _pivots_guard.get();
      
      #endif
      
      #if IS_CUDA
      THCPIntTensorPtr _info_guard((THCPIntTensor*) THCPIntTensor_NewEmpty());
      if (!_info_guard.get()) return NULL;
      THCPIntTensor* info = _info_guard.get();
      
      #else
      THPIntTensorPtr _info_guard((THPIntTensor*) THPIntTensor_NewEmpty());
      if (!_info_guard.get()) return NULL;
      THPIntTensor* info = _info_guard.get();
      
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THIntegerTensor* arg_pivots = ((THPIntegerTensor*)pivots)->cdata;
      THIntegerTensor* arg_info = ((THPIntegerTensor*)info)->cdata;
      bool arg_pivot = (__kw_pivot == Py_True ? true : false);
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(btrifact)(LIBRARY_STATE arg_result, arg_pivots, arg_info, arg_pivot, arg_self);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(3, result, pivots, info);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      #if IS_CUDA
      THCPIntTensorPtr _pivots_guard((THCPIntTensor*) THCPIntTensor_NewEmpty());
      if (!_pivots_guard.get()) return NULL;
      THCPIntTensor* pivots = _pivots_guard.get();
      
      #else
      THPIntTensorPtr _pivots_guard((THPIntTensor*) THPIntTensor_NewEmpty());
      if (!_pivots_guard.get()) return NULL;
      THPIntTensor* pivots = _pivots_guard.get();
      
      #endif
      
      #if IS_CUDA
      THCPIntTensorPtr _info_guard((THCPIntTensor*) THCPIntTensor_NewEmpty());
      if (!_info_guard.get()) return NULL;
      THCPIntTensor* info = _info_guard.get();
      
      #else
      THPIntTensorPtr _info_guard((THPIntTensor*) THPIntTensor_NewEmpty());
      if (!_info_guard.get()) return NULL;
      THPIntTensor* info = _info_guard.get();
      
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THIntegerTensor* arg_pivots = ((THPIntegerTensor*)pivots)->cdata;
      THIntegerTensor* arg_info = ((THPIntegerTensor*)info)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(btrifact)(LIBRARY_STATE arg_result, arg_pivots, arg_info, true, arg_self);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(3, result, pivots, info);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.btrifact_with_info", 2, "(" THPTensorStr " source, #tuple[" THPTensorStr ", " THPModuleStr "IntTensor, " THPModuleStr "IntTensor] out)", "(bool pivot, " THPTensorStr " source, #tuple[" THPTensorStr ", " THPModuleStr "IntTensor, " THPModuleStr "IntTensor] out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_(btrisolve)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_LU_data = NULL;
    PyObject *__kw_LU_pivots = NULL;
    if (kwargs) {
      __kw_LU_data = PyDict_GetItemString(kwargs, "LU_data");
      __kw_LU_pivots = PyDict_GetItemString(kwargs, "LU_pivots");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_LU_data) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_LU_data)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_LU_pivots) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_LU_pivots)) == THPIntegerTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_LU_data = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_LU_data))->cdata;
      THIntegerTensor* arg_LU_pivots = ((THPIntegerTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_LU_pivots))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(btrisolve)(LIBRARY_STATE arg_result, arg_self, arg_LU_data, arg_LU_pivots);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_LU_data) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_LU_data)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_LU_pivots) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_LU_pivots)) == THPIntegerTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_LU_data = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_LU_data))->cdata;
      THIntegerTensor* arg_LU_pivots = ((THPIntegerTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_LU_pivots))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(btrisolve)(LIBRARY_STATE arg_result, arg_self, arg_LU_data, arg_LU_pivots);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "btrisolve", 1, "(" THPTensorStr " LU_data, " THPModuleStr "IntTensor LU_pivots, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_stateless_(btrisolve)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    PyObject *__kw_LU_data = NULL;
    PyObject *__kw_LU_pivots = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
      __kw_LU_data = PyDict_GetItemString(kwargs, "LU_data");
      __kw_LU_pivots = PyDict_GetItemString(kwargs, "LU_pivots");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 4 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_LU_data) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_LU_data)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_LU_pivots) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_LU_pivots)) == THPIntegerTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_LU_data = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_LU_data))->cdata;
      THIntegerTensor* arg_LU_pivots = ((THPIntegerTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_LU_pivots))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(btrisolve)(LIBRARY_STATE arg_result, arg_self, arg_LU_data, arg_LU_pivots);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 3 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_LU_data) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_LU_data)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_LU_pivots) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_LU_pivots)) == THPIntegerTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_LU_data = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_LU_data))->cdata;
      THIntegerTensor* arg_LU_pivots = ((THPIntegerTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_LU_pivots))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(btrisolve)(LIBRARY_STATE arg_result, arg_self, arg_LU_data, arg_LU_pivots);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.btrisolve", 1, "(" THPTensorStr " source, " THPTensorStr " LU_data, " THPModuleStr "IntTensor LU_pivots, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_(lt)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_value = NULL;
    PyObject *__kw_other = NULL;
    if (kwargs) {
      __kw_value = PyDict_GetItemString(kwargs, "value");
      __kw_other = PyDict_GetItemString(kwargs, "other");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPBoolTensorClass &&
          (__tuplecount > 0 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THBoolTensor* arg_result = ((THPBoolTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(ltValue)(LIBRARY_STATE arg_result, arg_self, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPBoolTensorClass &&
          (__tuplecount > 0 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THBoolTensor* arg_result = ((THPBoolTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_self_guard.get(), arg_other_guard.get(),
              arg_self, arg_other,
              "self", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(ltTensor)(LIBRARY_STATE arg_result, arg_self, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_other = arg_other_save;
      
      
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      #if IS_CUDA
      THCPByteTensorPtr _result_guard((THCPByteTensor*) THCPByteTensor_NewEmpty());
      if (!_result_guard.get()) return NULL;
      THCPByteTensor* result = _result_guard.get();
      
      #else
      THPByteTensorPtr _result_guard((THPByteTensor*) THPByteTensor_NewEmpty());
      if (!_result_guard.get()) return NULL;
      THPByteTensor* result = _result_guard.get();
      
      #endif
      
      
      THBoolTensor* arg_result = ((THPBoolTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(ltValue)(LIBRARY_STATE arg_result, arg_self, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      #if IS_CUDA
      THCPByteTensorPtr _result_guard((THCPByteTensor*) THCPByteTensor_NewEmpty());
      if (!_result_guard.get()) return NULL;
      THCPByteTensor* result = _result_guard.get();
      
      #else
      THPByteTensorPtr _result_guard((THPByteTensor*) THPByteTensor_NewEmpty());
      if (!_result_guard.get()) return NULL;
      THPByteTensor* result = _result_guard.get();
      
      #endif
      
      
      THBoolTensor* arg_result = ((THPBoolTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_self_guard.get(), arg_other_guard.get(),
              arg_self, arg_other,
              "self", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(ltTensor)(LIBRARY_STATE arg_result, arg_self, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_other = arg_other_save;
      
      
    
    }

    THPUtils_invalidArguments(args, kwargs, "lt", 2, "(" RealStr " value, #" THPModuleStr "ByteTensor out)", "(" THPTensorStr " other, #" THPModuleStr "ByteTensor out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_(lt_)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_value = NULL;
    PyObject *__kw_other = NULL;
    if (kwargs) {
      __kw_value = PyDict_GetItemString(kwargs, "value");
      __kw_other = PyDict_GetItemString(kwargs, "other");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 1 &&
          (__tuplecount > 0 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(ltValueT)(LIBRARY_STATE arg_self, arg_self, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 1 &&
          (__tuplecount > 0 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other))->cdata;
      
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_inplace1(LIBRARY_STATE arg_other_guard.get(), arg_other, arg_self,
              "other", "self", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(ltTensorT)(LIBRARY_STATE arg_self, arg_self, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_other = arg_other_save;
      
      
    
    }

    THPUtils_invalidArguments(args, kwargs, "lt_", 2, "(" RealStr " value)", "(" THPTensorStr " other)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_stateless_(lt)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_tensor = NULL;
    PyObject *__kw_value = NULL;
    PyObject *__kw_other = NULL;
    if (kwargs) {
      __kw_tensor = PyDict_GetItemString(kwargs, "tensor");
      __kw_value = PyDict_GetItemString(kwargs, "value");
      __kw_other = PyDict_GetItemString(kwargs, "other");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPBoolTensorClass &&
          (__tuplecount > 0 || __kw_tensor) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THBoolTensor* arg_result = ((THPBoolTensor*)___out)->cdata;
      THTensor* arg_tensor = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor))->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(ltValue)(LIBRARY_STATE arg_result, arg_tensor, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPBoolTensorClass &&
          (__tuplecount > 0 || __kw_tensor) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THBoolTensor* arg_result = ((THPBoolTensor*)___out)->cdata;
      THTensor* arg_tensor = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor))->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other))->cdata;
      
      THTensor *arg_tensor_save = arg_tensor;
      THTensorPtr arg_tensor_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_tensor->size, arg_tensor->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_tensor_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_tensor_guard.get(), arg_other_guard.get(),
              arg_tensor, arg_other,
              "tensor", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_tensor = arg_tensor_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(ltTensor)(LIBRARY_STATE arg_result, arg_tensor, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_tensor = arg_tensor_save;
      arg_other = arg_other_save;
      
      
    
    } else if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_tensor) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_tensor = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor))->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(ltValueT)(LIBRARY_STATE arg_result, arg_tensor, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_tensor) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_tensor = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor))->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other))->cdata;
      
      THTensor *arg_tensor_save = arg_tensor;
      THTensorPtr arg_tensor_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_tensor->size, arg_tensor->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_tensor_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_tensor_guard.get(), arg_other_guard.get(),
              arg_tensor, arg_other,
              "tensor", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_tensor = arg_tensor_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(ltTensorT)(LIBRARY_STATE arg_result, arg_tensor, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_tensor = arg_tensor_save;
      arg_other = arg_other_save;
      
      
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_tensor) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      #if IS_CUDA
      THCPByteTensorPtr _result_guard((THCPByteTensor*) THCPByteTensor_NewEmpty());
      if (!_result_guard.get()) return NULL;
      THCPByteTensor* result = _result_guard.get();
      
      #else
      THPByteTensorPtr _result_guard((THPByteTensor*) THPByteTensor_NewEmpty());
      if (!_result_guard.get()) return NULL;
      THPByteTensor* result = _result_guard.get();
      
      #endif
      
      
      THBoolTensor* arg_result = ((THPBoolTensor*)result)->cdata;
      THTensor* arg_tensor = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor))->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(ltValue)(LIBRARY_STATE arg_result, arg_tensor, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_tensor) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      #if IS_CUDA
      THCPByteTensorPtr _result_guard((THCPByteTensor*) THCPByteTensor_NewEmpty());
      if (!_result_guard.get()) return NULL;
      THCPByteTensor* result = _result_guard.get();
      
      #else
      THPByteTensorPtr _result_guard((THPByteTensor*) THPByteTensor_NewEmpty());
      if (!_result_guard.get()) return NULL;
      THPByteTensor* result = _result_guard.get();
      
      #endif
      
      
      THBoolTensor* arg_result = ((THPBoolTensor*)result)->cdata;
      THTensor* arg_tensor = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor))->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other))->cdata;
      
      THTensor *arg_tensor_save = arg_tensor;
      THTensorPtr arg_tensor_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_tensor->size, arg_tensor->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_tensor_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_tensor_guard.get(), arg_other_guard.get(),
              arg_tensor, arg_other,
              "tensor", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_tensor = arg_tensor_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(ltTensor)(LIBRARY_STATE arg_result, arg_tensor, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_tensor = arg_tensor_save;
      arg_other = arg_other_save;
      
      
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_tensor) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_tensor = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor))->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(ltValueT)(LIBRARY_STATE arg_result, arg_tensor, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_tensor) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_tensor = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor))->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other))->cdata;
      
      THTensor *arg_tensor_save = arg_tensor;
      THTensorPtr arg_tensor_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_tensor->size, arg_tensor->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_tensor_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_tensor_guard.get(), arg_other_guard.get(),
              arg_tensor, arg_other,
              "tensor", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_tensor = arg_tensor_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(ltTensorT)(LIBRARY_STATE arg_result, arg_tensor, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_tensor = arg_tensor_save;
      arg_other = arg_other_save;
      
      
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.lt", 4, "(" THPTensorStr " tensor, " RealStr " value, #" THPTensorStr " out)", "(" THPTensorStr " tensor, " THPTensorStr " other, #" THPTensorStr " out)", "(" THPTensorStr " tensor, " RealStr " value, #" THPModuleStr "ByteTensor out)", "(" THPTensorStr " tensor, " THPTensorStr " other, #" THPModuleStr "ByteTensor out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif



#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_(gt)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_value = NULL;
    PyObject *__kw_other = NULL;
    if (kwargs) {
      __kw_value = PyDict_GetItemString(kwargs, "value");
      __kw_other = PyDict_GetItemString(kwargs, "other");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPBoolTensorClass &&
          (__tuplecount > 0 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THBoolTensor* arg_result = ((THPBoolTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(gtValue)(LIBRARY_STATE arg_result, arg_self, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPBoolTensorClass &&
          (__tuplecount > 0 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THBoolTensor* arg_result = ((THPBoolTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_self_guard.get(), arg_other_guard.get(),
              arg_self, arg_other,
              "self", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(gtTensor)(LIBRARY_STATE arg_result, arg_self, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_other = arg_other_save;
      
      
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      #if IS_CUDA
      THCPByteTensorPtr _result_guard((THCPByteTensor*) THCPByteTensor_NewEmpty());
      if (!_result_guard.get()) return NULL;
      THCPByteTensor* result = _result_guard.get();
      
      #else
      THPByteTensorPtr _result_guard((THPByteTensor*) THPByteTensor_NewEmpty());
      if (!_result_guard.get()) return NULL;
      THPByteTensor* result = _result_guard.get();
      
      #endif
      
      
      THBoolTensor* arg_result = ((THPBoolTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(gtValue)(LIBRARY_STATE arg_result, arg_self, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      #if IS_CUDA
      THCPByteTensorPtr _result_guard((THCPByteTensor*) THCPByteTensor_NewEmpty());
      if (!_result_guard.get()) return NULL;
      THCPByteTensor* result = _result_guard.get();
      
      #else
      THPByteTensorPtr _result_guard((THPByteTensor*) THPByteTensor_NewEmpty());
      if (!_result_guard.get()) return NULL;
      THPByteTensor* result = _result_guard.get();
      
      #endif
      
      
      THBoolTensor* arg_result = ((THPBoolTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_self_guard.get(), arg_other_guard.get(),
              arg_self, arg_other,
              "self", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(gtTensor)(LIBRARY_STATE arg_result, arg_self, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_other = arg_other_save;
      
      
    
    }

    THPUtils_invalidArguments(args, kwargs, "gt", 2, "(" RealStr " value, #" THPModuleStr "ByteTensor out)", "(" THPTensorStr " other, #" THPModuleStr "ByteTensor out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_(gt_)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_value = NULL;
    PyObject *__kw_other = NULL;
    if (kwargs) {
      __kw_value = PyDict_GetItemString(kwargs, "value");
      __kw_other = PyDict_GetItemString(kwargs, "other");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 1 &&
          (__tuplecount > 0 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(gtValueT)(LIBRARY_STATE arg_self, arg_self, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 1 &&
          (__tuplecount > 0 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other))->cdata;
      
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_inplace1(LIBRARY_STATE arg_other_guard.get(), arg_other, arg_self,
              "other", "self", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(gtTensorT)(LIBRARY_STATE arg_self, arg_self, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_other = arg_other_save;
      
      
    
    }

    THPUtils_invalidArguments(args, kwargs, "gt_", 2, "(" RealStr " value)", "(" THPTensorStr " other)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_stateless_(gt)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_tensor = NULL;
    PyObject *__kw_value = NULL;
    PyObject *__kw_other = NULL;
    if (kwargs) {
      __kw_tensor = PyDict_GetItemString(kwargs, "tensor");
      __kw_value = PyDict_GetItemString(kwargs, "value");
      __kw_other = PyDict_GetItemString(kwargs, "other");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPBoolTensorClass &&
          (__tuplecount > 0 || __kw_tensor) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THBoolTensor* arg_result = ((THPBoolTensor*)___out)->cdata;
      THTensor* arg_tensor = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor))->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(gtValue)(LIBRARY_STATE arg_result, arg_tensor, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPBoolTensorClass &&
          (__tuplecount > 0 || __kw_tensor) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THBoolTensor* arg_result = ((THPBoolTensor*)___out)->cdata;
      THTensor* arg_tensor = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor))->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other))->cdata;
      
      THTensor *arg_tensor_save = arg_tensor;
      THTensorPtr arg_tensor_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_tensor->size, arg_tensor->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_tensor_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_tensor_guard.get(), arg_other_guard.get(),
              arg_tensor, arg_other,
              "tensor", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_tensor = arg_tensor_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(gtTensor)(LIBRARY_STATE arg_result, arg_tensor, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_tensor = arg_tensor_save;
      arg_other = arg_other_save;
      
      
    
    } else if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_tensor) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_tensor = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor))->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(gtValueT)(LIBRARY_STATE arg_result, arg_tensor, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_tensor) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_tensor = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor))->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other))->cdata;
      
      THTensor *arg_tensor_save = arg_tensor;
      THTensorPtr arg_tensor_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_tensor->size, arg_tensor->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_tensor_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_tensor_guard.get(), arg_other_guard.get(),
              arg_tensor, arg_other,
              "tensor", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_tensor = arg_tensor_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(gtTensorT)(LIBRARY_STATE arg_result, arg_tensor, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_tensor = arg_tensor_save;
      arg_other = arg_other_save;
      
      
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_tensor) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      #if IS_CUDA
      THCPByteTensorPtr _result_guard((THCPByteTensor*) THCPByteTensor_NewEmpty());
      if (!_result_guard.get()) return NULL;
      THCPByteTensor* result = _result_guard.get();
      
      #else
      THPByteTensorPtr _result_guard((THPByteTensor*) THPByteTensor_NewEmpty());
      if (!_result_guard.get()) return NULL;
      THPByteTensor* result = _result_guard.get();
      
      #endif
      
      
      THBoolTensor* arg_result = ((THPBoolTensor*)result)->cdata;
      THTensor* arg_tensor = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor))->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(gtValue)(LIBRARY_STATE arg_result, arg_tensor, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_tensor) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      #if IS_CUDA
      THCPByteTensorPtr _result_guard((THCPByteTensor*) THCPByteTensor_NewEmpty());
      if (!_result_guard.get()) return NULL;
      THCPByteTensor* result = _result_guard.get();
      
      #else
      THPByteTensorPtr _result_guard((THPByteTensor*) THPByteTensor_NewEmpty());
      if (!_result_guard.get()) return NULL;
      THPByteTensor* result = _result_guard.get();
      
      #endif
      
      
      THBoolTensor* arg_result = ((THPBoolTensor*)result)->cdata;
      THTensor* arg_tensor = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor))->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other))->cdata;
      
      THTensor *arg_tensor_save = arg_tensor;
      THTensorPtr arg_tensor_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_tensor->size, arg_tensor->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_tensor_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_tensor_guard.get(), arg_other_guard.get(),
              arg_tensor, arg_other,
              "tensor", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_tensor = arg_tensor_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(gtTensor)(LIBRARY_STATE arg_result, arg_tensor, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_tensor = arg_tensor_save;
      arg_other = arg_other_save;
      
      
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_tensor) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_tensor = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor))->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(gtValueT)(LIBRARY_STATE arg_result, arg_tensor, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_tensor) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_tensor = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor))->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other))->cdata;
      
      THTensor *arg_tensor_save = arg_tensor;
      THTensorPtr arg_tensor_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_tensor->size, arg_tensor->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_tensor_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_tensor_guard.get(), arg_other_guard.get(),
              arg_tensor, arg_other,
              "tensor", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_tensor = arg_tensor_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(gtTensorT)(LIBRARY_STATE arg_result, arg_tensor, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_tensor = arg_tensor_save;
      arg_other = arg_other_save;
      
      
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.gt", 4, "(" THPTensorStr " tensor, " RealStr " value, #" THPTensorStr " out)", "(" THPTensorStr " tensor, " THPTensorStr " other, #" THPTensorStr " out)", "(" THPTensorStr " tensor, " RealStr " value, #" THPModuleStr "ByteTensor out)", "(" THPTensorStr " tensor, " THPTensorStr " other, #" THPModuleStr "ByteTensor out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif



#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_(le)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_value = NULL;
    PyObject *__kw_other = NULL;
    if (kwargs) {
      __kw_value = PyDict_GetItemString(kwargs, "value");
      __kw_other = PyDict_GetItemString(kwargs, "other");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPBoolTensorClass &&
          (__tuplecount > 0 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THBoolTensor* arg_result = ((THPBoolTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(leValue)(LIBRARY_STATE arg_result, arg_self, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPBoolTensorClass &&
          (__tuplecount > 0 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THBoolTensor* arg_result = ((THPBoolTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_self_guard.get(), arg_other_guard.get(),
              arg_self, arg_other,
              "self", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(leTensor)(LIBRARY_STATE arg_result, arg_self, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_other = arg_other_save;
      
      
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      #if IS_CUDA
      THCPByteTensorPtr _result_guard((THCPByteTensor*) THCPByteTensor_NewEmpty());
      if (!_result_guard.get()) return NULL;
      THCPByteTensor* result = _result_guard.get();
      
      #else
      THPByteTensorPtr _result_guard((THPByteTensor*) THPByteTensor_NewEmpty());
      if (!_result_guard.get()) return NULL;
      THPByteTensor* result = _result_guard.get();
      
      #endif
      
      
      THBoolTensor* arg_result = ((THPBoolTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(leValue)(LIBRARY_STATE arg_result, arg_self, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      #if IS_CUDA
      THCPByteTensorPtr _result_guard((THCPByteTensor*) THCPByteTensor_NewEmpty());
      if (!_result_guard.get()) return NULL;
      THCPByteTensor* result = _result_guard.get();
      
      #else
      THPByteTensorPtr _result_guard((THPByteTensor*) THPByteTensor_NewEmpty());
      if (!_result_guard.get()) return NULL;
      THPByteTensor* result = _result_guard.get();
      
      #endif
      
      
      THBoolTensor* arg_result = ((THPBoolTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_self_guard.get(), arg_other_guard.get(),
              arg_self, arg_other,
              "self", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(leTensor)(LIBRARY_STATE arg_result, arg_self, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_other = arg_other_save;
      
      
    
    }

    THPUtils_invalidArguments(args, kwargs, "le", 2, "(" RealStr " value, #" THPModuleStr "ByteTensor out)", "(" THPTensorStr " other, #" THPModuleStr "ByteTensor out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_(le_)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_value = NULL;
    PyObject *__kw_other = NULL;
    if (kwargs) {
      __kw_value = PyDict_GetItemString(kwargs, "value");
      __kw_other = PyDict_GetItemString(kwargs, "other");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 1 &&
          (__tuplecount > 0 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(leValueT)(LIBRARY_STATE arg_self, arg_self, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 1 &&
          (__tuplecount > 0 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other))->cdata;
      
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_inplace1(LIBRARY_STATE arg_other_guard.get(), arg_other, arg_self,
              "other", "self", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(leTensorT)(LIBRARY_STATE arg_self, arg_self, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_other = arg_other_save;
      
      
    
    }

    THPUtils_invalidArguments(args, kwargs, "le_", 2, "(" RealStr " value)", "(" THPTensorStr " other)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_stateless_(le)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_tensor = NULL;
    PyObject *__kw_value = NULL;
    PyObject *__kw_other = NULL;
    if (kwargs) {
      __kw_tensor = PyDict_GetItemString(kwargs, "tensor");
      __kw_value = PyDict_GetItemString(kwargs, "value");
      __kw_other = PyDict_GetItemString(kwargs, "other");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPBoolTensorClass &&
          (__tuplecount > 0 || __kw_tensor) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THBoolTensor* arg_result = ((THPBoolTensor*)___out)->cdata;
      THTensor* arg_tensor = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor))->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(leValue)(LIBRARY_STATE arg_result, arg_tensor, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPBoolTensorClass &&
          (__tuplecount > 0 || __kw_tensor) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THBoolTensor* arg_result = ((THPBoolTensor*)___out)->cdata;
      THTensor* arg_tensor = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor))->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other))->cdata;
      
      THTensor *arg_tensor_save = arg_tensor;
      THTensorPtr arg_tensor_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_tensor->size, arg_tensor->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_tensor_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_tensor_guard.get(), arg_other_guard.get(),
              arg_tensor, arg_other,
              "tensor", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_tensor = arg_tensor_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(leTensor)(LIBRARY_STATE arg_result, arg_tensor, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_tensor = arg_tensor_save;
      arg_other = arg_other_save;
      
      
    
    } else if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_tensor) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_tensor = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor))->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(leValueT)(LIBRARY_STATE arg_result, arg_tensor, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_tensor) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_tensor = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor))->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other))->cdata;
      
      THTensor *arg_tensor_save = arg_tensor;
      THTensorPtr arg_tensor_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_tensor->size, arg_tensor->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_tensor_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_tensor_guard.get(), arg_other_guard.get(),
              arg_tensor, arg_other,
              "tensor", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_tensor = arg_tensor_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(leTensorT)(LIBRARY_STATE arg_result, arg_tensor, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_tensor = arg_tensor_save;
      arg_other = arg_other_save;
      
      
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_tensor) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      #if IS_CUDA
      THCPByteTensorPtr _result_guard((THCPByteTensor*) THCPByteTensor_NewEmpty());
      if (!_result_guard.get()) return NULL;
      THCPByteTensor* result = _result_guard.get();
      
      #else
      THPByteTensorPtr _result_guard((THPByteTensor*) THPByteTensor_NewEmpty());
      if (!_result_guard.get()) return NULL;
      THPByteTensor* result = _result_guard.get();
      
      #endif
      
      
      THBoolTensor* arg_result = ((THPBoolTensor*)result)->cdata;
      THTensor* arg_tensor = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor))->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(leValue)(LIBRARY_STATE arg_result, arg_tensor, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_tensor) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      #if IS_CUDA
      THCPByteTensorPtr _result_guard((THCPByteTensor*) THCPByteTensor_NewEmpty());
      if (!_result_guard.get()) return NULL;
      THCPByteTensor* result = _result_guard.get();
      
      #else
      THPByteTensorPtr _result_guard((THPByteTensor*) THPByteTensor_NewEmpty());
      if (!_result_guard.get()) return NULL;
      THPByteTensor* result = _result_guard.get();
      
      #endif
      
      
      THBoolTensor* arg_result = ((THPBoolTensor*)result)->cdata;
      THTensor* arg_tensor = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor))->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other))->cdata;
      
      THTensor *arg_tensor_save = arg_tensor;
      THTensorPtr arg_tensor_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_tensor->size, arg_tensor->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_tensor_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_tensor_guard.get(), arg_other_guard.get(),
              arg_tensor, arg_other,
              "tensor", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_tensor = arg_tensor_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(leTensor)(LIBRARY_STATE arg_result, arg_tensor, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_tensor = arg_tensor_save;
      arg_other = arg_other_save;
      
      
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_tensor) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_tensor = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor))->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(leValueT)(LIBRARY_STATE arg_result, arg_tensor, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_tensor) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_tensor = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor))->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other))->cdata;
      
      THTensor *arg_tensor_save = arg_tensor;
      THTensorPtr arg_tensor_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_tensor->size, arg_tensor->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_tensor_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_tensor_guard.get(), arg_other_guard.get(),
              arg_tensor, arg_other,
              "tensor", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_tensor = arg_tensor_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(leTensorT)(LIBRARY_STATE arg_result, arg_tensor, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_tensor = arg_tensor_save;
      arg_other = arg_other_save;
      
      
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.le", 4, "(" THPTensorStr " tensor, " RealStr " value, #" THPTensorStr " out)", "(" THPTensorStr " tensor, " THPTensorStr " other, #" THPTensorStr " out)", "(" THPTensorStr " tensor, " RealStr " value, #" THPModuleStr "ByteTensor out)", "(" THPTensorStr " tensor, " THPTensorStr " other, #" THPModuleStr "ByteTensor out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif



#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_(ge)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_value = NULL;
    PyObject *__kw_other = NULL;
    if (kwargs) {
      __kw_value = PyDict_GetItemString(kwargs, "value");
      __kw_other = PyDict_GetItemString(kwargs, "other");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPBoolTensorClass &&
          (__tuplecount > 0 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THBoolTensor* arg_result = ((THPBoolTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(geValue)(LIBRARY_STATE arg_result, arg_self, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPBoolTensorClass &&
          (__tuplecount > 0 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THBoolTensor* arg_result = ((THPBoolTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_self_guard.get(), arg_other_guard.get(),
              arg_self, arg_other,
              "self", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(geTensor)(LIBRARY_STATE arg_result, arg_self, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_other = arg_other_save;
      
      
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      #if IS_CUDA
      THCPByteTensorPtr _result_guard((THCPByteTensor*) THCPByteTensor_NewEmpty());
      if (!_result_guard.get()) return NULL;
      THCPByteTensor* result = _result_guard.get();
      
      #else
      THPByteTensorPtr _result_guard((THPByteTensor*) THPByteTensor_NewEmpty());
      if (!_result_guard.get()) return NULL;
      THPByteTensor* result = _result_guard.get();
      
      #endif
      
      
      THBoolTensor* arg_result = ((THPBoolTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(geValue)(LIBRARY_STATE arg_result, arg_self, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      #if IS_CUDA
      THCPByteTensorPtr _result_guard((THCPByteTensor*) THCPByteTensor_NewEmpty());
      if (!_result_guard.get()) return NULL;
      THCPByteTensor* result = _result_guard.get();
      
      #else
      THPByteTensorPtr _result_guard((THPByteTensor*) THPByteTensor_NewEmpty());
      if (!_result_guard.get()) return NULL;
      THPByteTensor* result = _result_guard.get();
      
      #endif
      
      
      THBoolTensor* arg_result = ((THPBoolTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_self_guard.get(), arg_other_guard.get(),
              arg_self, arg_other,
              "self", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(geTensor)(LIBRARY_STATE arg_result, arg_self, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_other = arg_other_save;
      
      
    
    }

    THPUtils_invalidArguments(args, kwargs, "ge", 2, "(" RealStr " value, #" THPModuleStr "ByteTensor out)", "(" THPTensorStr " other, #" THPModuleStr "ByteTensor out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_(ge_)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_value = NULL;
    PyObject *__kw_other = NULL;
    if (kwargs) {
      __kw_value = PyDict_GetItemString(kwargs, "value");
      __kw_other = PyDict_GetItemString(kwargs, "other");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 1 &&
          (__tuplecount > 0 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(geValueT)(LIBRARY_STATE arg_self, arg_self, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 1 &&
          (__tuplecount > 0 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other))->cdata;
      
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_inplace1(LIBRARY_STATE arg_other_guard.get(), arg_other, arg_self,
              "other", "self", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(geTensorT)(LIBRARY_STATE arg_self, arg_self, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_other = arg_other_save;
      
      
    
    }

    THPUtils_invalidArguments(args, kwargs, "ge_", 2, "(" RealStr " value)", "(" THPTensorStr " other)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_stateless_(ge)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_tensor = NULL;
    PyObject *__kw_value = NULL;
    PyObject *__kw_other = NULL;
    if (kwargs) {
      __kw_tensor = PyDict_GetItemString(kwargs, "tensor");
      __kw_value = PyDict_GetItemString(kwargs, "value");
      __kw_other = PyDict_GetItemString(kwargs, "other");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPBoolTensorClass &&
          (__tuplecount > 0 || __kw_tensor) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THBoolTensor* arg_result = ((THPBoolTensor*)___out)->cdata;
      THTensor* arg_tensor = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor))->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(geValue)(LIBRARY_STATE arg_result, arg_tensor, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPBoolTensorClass &&
          (__tuplecount > 0 || __kw_tensor) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THBoolTensor* arg_result = ((THPBoolTensor*)___out)->cdata;
      THTensor* arg_tensor = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor))->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other))->cdata;
      
      THTensor *arg_tensor_save = arg_tensor;
      THTensorPtr arg_tensor_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_tensor->size, arg_tensor->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_tensor_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_tensor_guard.get(), arg_other_guard.get(),
              arg_tensor, arg_other,
              "tensor", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_tensor = arg_tensor_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(geTensor)(LIBRARY_STATE arg_result, arg_tensor, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_tensor = arg_tensor_save;
      arg_other = arg_other_save;
      
      
    
    } else if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_tensor) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_tensor = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor))->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(geValueT)(LIBRARY_STATE arg_result, arg_tensor, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_tensor) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_tensor = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor))->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other))->cdata;
      
      THTensor *arg_tensor_save = arg_tensor;
      THTensorPtr arg_tensor_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_tensor->size, arg_tensor->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_tensor_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_tensor_guard.get(), arg_other_guard.get(),
              arg_tensor, arg_other,
              "tensor", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_tensor = arg_tensor_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(geTensorT)(LIBRARY_STATE arg_result, arg_tensor, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_tensor = arg_tensor_save;
      arg_other = arg_other_save;
      
      
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_tensor) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      #if IS_CUDA
      THCPByteTensorPtr _result_guard((THCPByteTensor*) THCPByteTensor_NewEmpty());
      if (!_result_guard.get()) return NULL;
      THCPByteTensor* result = _result_guard.get();
      
      #else
      THPByteTensorPtr _result_guard((THPByteTensor*) THPByteTensor_NewEmpty());
      if (!_result_guard.get()) return NULL;
      THPByteTensor* result = _result_guard.get();
      
      #endif
      
      
      THBoolTensor* arg_result = ((THPBoolTensor*)result)->cdata;
      THTensor* arg_tensor = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor))->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(geValue)(LIBRARY_STATE arg_result, arg_tensor, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_tensor) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      #if IS_CUDA
      THCPByteTensorPtr _result_guard((THCPByteTensor*) THCPByteTensor_NewEmpty());
      if (!_result_guard.get()) return NULL;
      THCPByteTensor* result = _result_guard.get();
      
      #else
      THPByteTensorPtr _result_guard((THPByteTensor*) THPByteTensor_NewEmpty());
      if (!_result_guard.get()) return NULL;
      THPByteTensor* result = _result_guard.get();
      
      #endif
      
      
      THBoolTensor* arg_result = ((THPBoolTensor*)result)->cdata;
      THTensor* arg_tensor = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor))->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other))->cdata;
      
      THTensor *arg_tensor_save = arg_tensor;
      THTensorPtr arg_tensor_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_tensor->size, arg_tensor->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_tensor_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_tensor_guard.get(), arg_other_guard.get(),
              arg_tensor, arg_other,
              "tensor", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_tensor = arg_tensor_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(geTensor)(LIBRARY_STATE arg_result, arg_tensor, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_tensor = arg_tensor_save;
      arg_other = arg_other_save;
      
      
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_tensor) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_tensor = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor))->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(geValueT)(LIBRARY_STATE arg_result, arg_tensor, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_tensor) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_tensor = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor))->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other))->cdata;
      
      THTensor *arg_tensor_save = arg_tensor;
      THTensorPtr arg_tensor_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_tensor->size, arg_tensor->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_tensor_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_tensor_guard.get(), arg_other_guard.get(),
              arg_tensor, arg_other,
              "tensor", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_tensor = arg_tensor_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(geTensorT)(LIBRARY_STATE arg_result, arg_tensor, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_tensor = arg_tensor_save;
      arg_other = arg_other_save;
      
      
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.ge", 4, "(" THPTensorStr " tensor, " RealStr " value, #" THPTensorStr " out)", "(" THPTensorStr " tensor, " THPTensorStr " other, #" THPTensorStr " out)", "(" THPTensorStr " tensor, " RealStr " value, #" THPModuleStr "ByteTensor out)", "(" THPTensorStr " tensor, " THPTensorStr " other, #" THPModuleStr "ByteTensor out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif



#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_(eq)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_value = NULL;
    PyObject *__kw_other = NULL;
    if (kwargs) {
      __kw_value = PyDict_GetItemString(kwargs, "value");
      __kw_other = PyDict_GetItemString(kwargs, "other");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPBoolTensorClass &&
          (__tuplecount > 0 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THBoolTensor* arg_result = ((THPBoolTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(eqValue)(LIBRARY_STATE arg_result, arg_self, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPBoolTensorClass &&
          (__tuplecount > 0 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THBoolTensor* arg_result = ((THPBoolTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_self_guard.get(), arg_other_guard.get(),
              arg_self, arg_other,
              "self", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(eqTensor)(LIBRARY_STATE arg_result, arg_self, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_other = arg_other_save;
      
      
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      #if IS_CUDA
      THCPByteTensorPtr _result_guard((THCPByteTensor*) THCPByteTensor_NewEmpty());
      if (!_result_guard.get()) return NULL;
      THCPByteTensor* result = _result_guard.get();
      
      #else
      THPByteTensorPtr _result_guard((THPByteTensor*) THPByteTensor_NewEmpty());
      if (!_result_guard.get()) return NULL;
      THPByteTensor* result = _result_guard.get();
      
      #endif
      
      
      THBoolTensor* arg_result = ((THPBoolTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(eqValue)(LIBRARY_STATE arg_result, arg_self, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      #if IS_CUDA
      THCPByteTensorPtr _result_guard((THCPByteTensor*) THCPByteTensor_NewEmpty());
      if (!_result_guard.get()) return NULL;
      THCPByteTensor* result = _result_guard.get();
      
      #else
      THPByteTensorPtr _result_guard((THPByteTensor*) THPByteTensor_NewEmpty());
      if (!_result_guard.get()) return NULL;
      THPByteTensor* result = _result_guard.get();
      
      #endif
      
      
      THBoolTensor* arg_result = ((THPBoolTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_self_guard.get(), arg_other_guard.get(),
              arg_self, arg_other,
              "self", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(eqTensor)(LIBRARY_STATE arg_result, arg_self, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_other = arg_other_save;
      
      
    
    }

    THPUtils_invalidArguments(args, kwargs, "eq", 2, "(" RealStr " value, #" THPModuleStr "ByteTensor out)", "(" THPTensorStr " other, #" THPModuleStr "ByteTensor out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_(eq_)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_value = NULL;
    PyObject *__kw_other = NULL;
    if (kwargs) {
      __kw_value = PyDict_GetItemString(kwargs, "value");
      __kw_other = PyDict_GetItemString(kwargs, "other");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 1 &&
          (__tuplecount > 0 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(eqValueT)(LIBRARY_STATE arg_self, arg_self, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 1 &&
          (__tuplecount > 0 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other))->cdata;
      
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_inplace1(LIBRARY_STATE arg_other_guard.get(), arg_other, arg_self,
              "other", "self", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(eqTensorT)(LIBRARY_STATE arg_self, arg_self, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_other = arg_other_save;
      
      
    
    }

    THPUtils_invalidArguments(args, kwargs, "eq_", 2, "(" RealStr " value)", "(" THPTensorStr " other)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_stateless_(eq)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_tensor = NULL;
    PyObject *__kw_value = NULL;
    PyObject *__kw_other = NULL;
    if (kwargs) {
      __kw_tensor = PyDict_GetItemString(kwargs, "tensor");
      __kw_value = PyDict_GetItemString(kwargs, "value");
      __kw_other = PyDict_GetItemString(kwargs, "other");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPBoolTensorClass &&
          (__tuplecount > 0 || __kw_tensor) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THBoolTensor* arg_result = ((THPBoolTensor*)___out)->cdata;
      THTensor* arg_tensor = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor))->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(eqValue)(LIBRARY_STATE arg_result, arg_tensor, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPBoolTensorClass &&
          (__tuplecount > 0 || __kw_tensor) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THBoolTensor* arg_result = ((THPBoolTensor*)___out)->cdata;
      THTensor* arg_tensor = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor))->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other))->cdata;
      
      THTensor *arg_tensor_save = arg_tensor;
      THTensorPtr arg_tensor_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_tensor->size, arg_tensor->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_tensor_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_tensor_guard.get(), arg_other_guard.get(),
              arg_tensor, arg_other,
              "tensor", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_tensor = arg_tensor_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(eqTensor)(LIBRARY_STATE arg_result, arg_tensor, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_tensor = arg_tensor_save;
      arg_other = arg_other_save;
      
      
    
    } else if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_tensor) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_tensor = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor))->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(eqValueT)(LIBRARY_STATE arg_result, arg_tensor, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_tensor) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_tensor = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor))->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other))->cdata;
      
      THTensor *arg_tensor_save = arg_tensor;
      THTensorPtr arg_tensor_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_tensor->size, arg_tensor->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_tensor_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_tensor_guard.get(), arg_other_guard.get(),
              arg_tensor, arg_other,
              "tensor", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_tensor = arg_tensor_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(eqTensorT)(LIBRARY_STATE arg_result, arg_tensor, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_tensor = arg_tensor_save;
      arg_other = arg_other_save;
      
      
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_tensor) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      #if IS_CUDA
      THCPByteTensorPtr _result_guard((THCPByteTensor*) THCPByteTensor_NewEmpty());
      if (!_result_guard.get()) return NULL;
      THCPByteTensor* result = _result_guard.get();
      
      #else
      THPByteTensorPtr _result_guard((THPByteTensor*) THPByteTensor_NewEmpty());
      if (!_result_guard.get()) return NULL;
      THPByteTensor* result = _result_guard.get();
      
      #endif
      
      
      THBoolTensor* arg_result = ((THPBoolTensor*)result)->cdata;
      THTensor* arg_tensor = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor))->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(eqValue)(LIBRARY_STATE arg_result, arg_tensor, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_tensor) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      #if IS_CUDA
      THCPByteTensorPtr _result_guard((THCPByteTensor*) THCPByteTensor_NewEmpty());
      if (!_result_guard.get()) return NULL;
      THCPByteTensor* result = _result_guard.get();
      
      #else
      THPByteTensorPtr _result_guard((THPByteTensor*) THPByteTensor_NewEmpty());
      if (!_result_guard.get()) return NULL;
      THPByteTensor* result = _result_guard.get();
      
      #endif
      
      
      THBoolTensor* arg_result = ((THPBoolTensor*)result)->cdata;
      THTensor* arg_tensor = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor))->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other))->cdata;
      
      THTensor *arg_tensor_save = arg_tensor;
      THTensorPtr arg_tensor_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_tensor->size, arg_tensor->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_tensor_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_tensor_guard.get(), arg_other_guard.get(),
              arg_tensor, arg_other,
              "tensor", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_tensor = arg_tensor_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(eqTensor)(LIBRARY_STATE arg_result, arg_tensor, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_tensor = arg_tensor_save;
      arg_other = arg_other_save;
      
      
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_tensor) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_tensor = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor))->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(eqValueT)(LIBRARY_STATE arg_result, arg_tensor, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_tensor) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_tensor = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor))->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other))->cdata;
      
      THTensor *arg_tensor_save = arg_tensor;
      THTensorPtr arg_tensor_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_tensor->size, arg_tensor->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_tensor_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_tensor_guard.get(), arg_other_guard.get(),
              arg_tensor, arg_other,
              "tensor", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_tensor = arg_tensor_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(eqTensorT)(LIBRARY_STATE arg_result, arg_tensor, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_tensor = arg_tensor_save;
      arg_other = arg_other_save;
      
      
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.eq", 4, "(" THPTensorStr " tensor, " RealStr " value, #" THPTensorStr " out)", "(" THPTensorStr " tensor, " THPTensorStr " other, #" THPTensorStr " out)", "(" THPTensorStr " tensor, " RealStr " value, #" THPModuleStr "ByteTensor out)", "(" THPTensorStr " tensor, " THPTensorStr " other, #" THPModuleStr "ByteTensor out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif



#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_(ne)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_value = NULL;
    PyObject *__kw_other = NULL;
    if (kwargs) {
      __kw_value = PyDict_GetItemString(kwargs, "value");
      __kw_other = PyDict_GetItemString(kwargs, "other");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPBoolTensorClass &&
          (__tuplecount > 0 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THBoolTensor* arg_result = ((THPBoolTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(neValue)(LIBRARY_STATE arg_result, arg_self, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPBoolTensorClass &&
          (__tuplecount > 0 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THBoolTensor* arg_result = ((THPBoolTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_self_guard.get(), arg_other_guard.get(),
              arg_self, arg_other,
              "self", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(neTensor)(LIBRARY_STATE arg_result, arg_self, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_other = arg_other_save;
      
      
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      #if IS_CUDA
      THCPByteTensorPtr _result_guard((THCPByteTensor*) THCPByteTensor_NewEmpty());
      if (!_result_guard.get()) return NULL;
      THCPByteTensor* result = _result_guard.get();
      
      #else
      THPByteTensorPtr _result_guard((THPByteTensor*) THPByteTensor_NewEmpty());
      if (!_result_guard.get()) return NULL;
      THPByteTensor* result = _result_guard.get();
      
      #endif
      
      
      THBoolTensor* arg_result = ((THPBoolTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(neValue)(LIBRARY_STATE arg_result, arg_self, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      #if IS_CUDA
      THCPByteTensorPtr _result_guard((THCPByteTensor*) THCPByteTensor_NewEmpty());
      if (!_result_guard.get()) return NULL;
      THCPByteTensor* result = _result_guard.get();
      
      #else
      THPByteTensorPtr _result_guard((THPByteTensor*) THPByteTensor_NewEmpty());
      if (!_result_guard.get()) return NULL;
      THPByteTensor* result = _result_guard.get();
      
      #endif
      
      
      THBoolTensor* arg_result = ((THPBoolTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_self_guard.get(), arg_other_guard.get(),
              arg_self, arg_other,
              "self", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(neTensor)(LIBRARY_STATE arg_result, arg_self, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_other = arg_other_save;
      
      
    
    }

    THPUtils_invalidArguments(args, kwargs, "ne", 2, "(" RealStr " value, #" THPModuleStr "ByteTensor out)", "(" THPTensorStr " other, #" THPModuleStr "ByteTensor out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_(ne_)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_value = NULL;
    PyObject *__kw_other = NULL;
    if (kwargs) {
      __kw_value = PyDict_GetItemString(kwargs, "value");
      __kw_other = PyDict_GetItemString(kwargs, "other");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 1 &&
          (__tuplecount > 0 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(neValueT)(LIBRARY_STATE arg_self, arg_self, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 1 &&
          (__tuplecount > 0 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other))->cdata;
      
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_inplace1(LIBRARY_STATE arg_other_guard.get(), arg_other, arg_self,
              "other", "self", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(neTensorT)(LIBRARY_STATE arg_self, arg_self, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_other = arg_other_save;
      
      
    
    }

    THPUtils_invalidArguments(args, kwargs, "ne_", 2, "(" RealStr " value)", "(" THPTensorStr " other)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_stateless_(ne)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_tensor = NULL;
    PyObject *__kw_value = NULL;
    PyObject *__kw_other = NULL;
    if (kwargs) {
      __kw_tensor = PyDict_GetItemString(kwargs, "tensor");
      __kw_value = PyDict_GetItemString(kwargs, "value");
      __kw_other = PyDict_GetItemString(kwargs, "other");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPBoolTensorClass &&
          (__tuplecount > 0 || __kw_tensor) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THBoolTensor* arg_result = ((THPBoolTensor*)___out)->cdata;
      THTensor* arg_tensor = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor))->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(neValue)(LIBRARY_STATE arg_result, arg_tensor, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPBoolTensorClass &&
          (__tuplecount > 0 || __kw_tensor) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THBoolTensor* arg_result = ((THPBoolTensor*)___out)->cdata;
      THTensor* arg_tensor = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor))->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other))->cdata;
      
      THTensor *arg_tensor_save = arg_tensor;
      THTensorPtr arg_tensor_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_tensor->size, arg_tensor->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_tensor_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_tensor_guard.get(), arg_other_guard.get(),
              arg_tensor, arg_other,
              "tensor", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_tensor = arg_tensor_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(neTensor)(LIBRARY_STATE arg_result, arg_tensor, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_tensor = arg_tensor_save;
      arg_other = arg_other_save;
      
      
    
    } else if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_tensor) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_tensor = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor))->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(neValueT)(LIBRARY_STATE arg_result, arg_tensor, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_tensor) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_tensor = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor))->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other))->cdata;
      
      THTensor *arg_tensor_save = arg_tensor;
      THTensorPtr arg_tensor_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_tensor->size, arg_tensor->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_tensor_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_tensor_guard.get(), arg_other_guard.get(),
              arg_tensor, arg_other,
              "tensor", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_tensor = arg_tensor_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(neTensorT)(LIBRARY_STATE arg_result, arg_tensor, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_tensor = arg_tensor_save;
      arg_other = arg_other_save;
      
      
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_tensor) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      #if IS_CUDA
      THCPByteTensorPtr _result_guard((THCPByteTensor*) THCPByteTensor_NewEmpty());
      if (!_result_guard.get()) return NULL;
      THCPByteTensor* result = _result_guard.get();
      
      #else
      THPByteTensorPtr _result_guard((THPByteTensor*) THPByteTensor_NewEmpty());
      if (!_result_guard.get()) return NULL;
      THPByteTensor* result = _result_guard.get();
      
      #endif
      
      
      THBoolTensor* arg_result = ((THPBoolTensor*)result)->cdata;
      THTensor* arg_tensor = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor))->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(neValue)(LIBRARY_STATE arg_result, arg_tensor, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_tensor) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      #if IS_CUDA
      THCPByteTensorPtr _result_guard((THCPByteTensor*) THCPByteTensor_NewEmpty());
      if (!_result_guard.get()) return NULL;
      THCPByteTensor* result = _result_guard.get();
      
      #else
      THPByteTensorPtr _result_guard((THPByteTensor*) THPByteTensor_NewEmpty());
      if (!_result_guard.get()) return NULL;
      THPByteTensor* result = _result_guard.get();
      
      #endif
      
      
      THBoolTensor* arg_result = ((THPBoolTensor*)result)->cdata;
      THTensor* arg_tensor = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor))->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other))->cdata;
      
      THTensor *arg_tensor_save = arg_tensor;
      THTensorPtr arg_tensor_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_tensor->size, arg_tensor->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_tensor_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_tensor_guard.get(), arg_other_guard.get(),
              arg_tensor, arg_other,
              "tensor", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_tensor = arg_tensor_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(neTensor)(LIBRARY_STATE arg_result, arg_tensor, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_tensor = arg_tensor_save;
      arg_other = arg_other_save;
      
      
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_tensor) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_tensor = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor))->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(neValueT)(LIBRARY_STATE arg_result, arg_tensor, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_tensor) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_tensor = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_tensor))->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other))->cdata;
      
      THTensor *arg_tensor_save = arg_tensor;
      THTensorPtr arg_tensor_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_tensor->size, arg_tensor->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_tensor_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_tensor_guard.get(), arg_other_guard.get(),
              arg_tensor, arg_other,
              "tensor", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_tensor = arg_tensor_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(neTensorT)(LIBRARY_STATE arg_result, arg_tensor, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_tensor = arg_tensor_save;
      arg_other = arg_other_save;
      
      
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.ne", 4, "(" THPTensorStr " tensor, " RealStr " value, #" THPTensorStr " out)", "(" THPTensorStr " tensor, " THPTensorStr " other, #" THPTensorStr " out)", "(" THPTensorStr " tensor, " RealStr " value, #" THPModuleStr "ByteTensor out)", "(" THPTensorStr " tensor, " THPTensorStr " other, #" THPModuleStr "ByteTensor out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_(min)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_dim = NULL;
    PyObject *__kw_keepdim = NULL;
    PyObject *__kw_other = NULL;
    if (kwargs) {
      __kw_dim = PyDict_GetItemString(kwargs, "dim");
      __kw_keepdim = PyDict_GetItemString(kwargs, "keepdim");
      __kw_other = PyDict_GetItemString(kwargs, "other");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPIndexTensorClass &&
          (__tuplecount > 0 || __kw_dim) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim)) &&
          (__tuplecount > 1 || __kw_keepdim) && PyBool_Check((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_keepdim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_min = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THIndexTensor* arg_min_indices = ((THPIndexTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim));
      bool arg_keepdim = ((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_keepdim) == Py_True ? true : false);
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(min)(LIBRARY_STATE arg_min, arg_min_indices, arg_self, arg_dim, arg_keepdim);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPIndexTensorClass &&
          (__tuplecount > 0 || __kw_dim) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_min = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THIndexTensor* arg_min_indices = ((THPIndexTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim));
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        maybeThrowBackCompatKeepdimWarn("min");
        Py_UNBLOCK_THREADS;
        THTensor_(min)(LIBRARY_STATE arg_min, arg_min_indices, arg_self, arg_dim, false);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_self_guard.get(), arg_other_guard.get(),
              arg_self, arg_other,
              "self", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cmin)(LIBRARY_STATE arg_result, arg_self, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_other = arg_other_save;
      
      
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_dim) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim)) &&
          (__tuplecount > 1 || __kw_keepdim) && PyBool_Check((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_keepdim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _min_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_min_guard.get()) return NULL;
      THPTensor* min = _min_guard.get();
      
      #if IS_CUDA
      THCPLongTensorPtr _min_indices_guard((THCPLongTensor*) THCPLongTensor_NewEmpty());
      if (!_min_indices_guard.get()) return NULL;
      THCPLongTensor* min_indices = _min_indices_guard.get();
      
      #else
      THPLongTensorPtr _min_indices_guard((THPLongTensor*) THPLongTensor_NewEmpty());
      if (!_min_indices_guard.get()) return NULL;
      THPLongTensor* min_indices = _min_indices_guard.get();
      
      #endif
      
      
      THTensor* arg_min = ((THPTensor*)min)->cdata;
      THIndexTensor* arg_min_indices = ((THPIndexTensor*)min_indices)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim));
      bool arg_keepdim = ((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_keepdim) == Py_True ? true : false);
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(min)(LIBRARY_STATE arg_min, arg_min_indices, arg_self, arg_dim, arg_keepdim);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, min, min_indices);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_self_guard.get(), arg_other_guard.get(),
              arg_self, arg_other,
              "self", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cmin)(LIBRARY_STATE arg_result, arg_self, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_other = arg_other_save;
      
      
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_dim) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _min_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_min_guard.get()) return NULL;
      THPTensor* min = _min_guard.get();
      
      #if IS_CUDA
      THCPLongTensorPtr _min_indices_guard((THCPLongTensor*) THCPLongTensor_NewEmpty());
      if (!_min_indices_guard.get()) return NULL;
      THCPLongTensor* min_indices = _min_indices_guard.get();
      
      #else
      THPLongTensorPtr _min_indices_guard((THPLongTensor*) THPLongTensor_NewEmpty());
      if (!_min_indices_guard.get()) return NULL;
      THPLongTensor* min_indices = _min_indices_guard.get();
      
      #endif
      
      
      THTensor* arg_min = ((THPTensor*)min)->cdata;
      THIndexTensor* arg_min_indices = ((THPIndexTensor*)min_indices)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim));
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        maybeThrowBackCompatKeepdimWarn("min");
        Py_UNBLOCK_THREADS;
        THTensor_(min)(LIBRARY_STATE arg_min, arg_min_indices, arg_self, arg_dim, false);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, min, min_indices);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        real __result = THTensor_(minall)(LIBRARY_STATE arg_self);
        Py_BLOCK_THREADS;
        return THPUtils_(newReal)(__result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "min", 4, "no arguments", "(" THPTensorStr " other, #" THPTensorStr " out)", "(int dim, #tuple[" THPTensorStr ", " THPModuleStr "LongTensor] out)", "(int dim, bool keepdim, #tuple[" THPTensorStr ", " THPModuleStr "LongTensor] out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_stateless_(min)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    PyObject *__kw_dim = NULL;
    PyObject *__kw_keepdim = NULL;
    PyObject *__kw_other = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
      __kw_dim = PyDict_GetItemString(kwargs, "dim");
      __kw_keepdim = PyDict_GetItemString(kwargs, "keepdim");
      __kw_other = PyDict_GetItemString(kwargs, "other");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 4 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPIndexTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_dim) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim)) &&
          (__tuplecount > 2 || __kw_keepdim) && PyBool_Check((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_keepdim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_min = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THIndexTensor* arg_min_indices = ((THPIndexTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim));
      bool arg_keepdim = ((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_keepdim) == Py_True ? true : false);
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(min)(LIBRARY_STATE arg_min, arg_min_indices, arg_self, arg_dim, arg_keepdim);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPIndexTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_dim) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_min = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THIndexTensor* arg_min_indices = ((THPIndexTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim));
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        maybeThrowBackCompatKeepdimWarn("min");
        Py_UNBLOCK_THREADS;
        THTensor_(min)(LIBRARY_STATE arg_min, arg_min_indices, arg_self, arg_dim, false);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_self_guard.get(), arg_other_guard.get(),
              arg_self, arg_other,
              "self", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cmin)(LIBRARY_STATE arg_result, arg_self, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_other = arg_other_save;
      
      
    
    } else if (___out == NULL &&
          __argcount == 3 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_dim) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim)) &&
          (__tuplecount > 2 || __kw_keepdim) && PyBool_Check((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_keepdim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _min_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_min_guard.get()) return NULL;
      THPTensor* min = _min_guard.get();
      
      #if IS_CUDA
      THCPLongTensorPtr _min_indices_guard((THCPLongTensor*) THCPLongTensor_NewEmpty());
      if (!_min_indices_guard.get()) return NULL;
      THCPLongTensor* min_indices = _min_indices_guard.get();
      
      #else
      THPLongTensorPtr _min_indices_guard((THPLongTensor*) THPLongTensor_NewEmpty());
      if (!_min_indices_guard.get()) return NULL;
      THPLongTensor* min_indices = _min_indices_guard.get();
      
      #endif
      
      
      THTensor* arg_min = ((THPTensor*)min)->cdata;
      THIndexTensor* arg_min_indices = ((THPIndexTensor*)min_indices)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim));
      bool arg_keepdim = ((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_keepdim) == Py_True ? true : false);
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(min)(LIBRARY_STATE arg_min, arg_min_indices, arg_self, arg_dim, arg_keepdim);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, min, min_indices);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_self_guard.get(), arg_other_guard.get(),
              arg_self, arg_other,
              "self", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cmin)(LIBRARY_STATE arg_result, arg_self, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_other = arg_other_save;
      
      
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_dim) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _min_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_min_guard.get()) return NULL;
      THPTensor* min = _min_guard.get();
      
      #if IS_CUDA
      THCPLongTensorPtr _min_indices_guard((THCPLongTensor*) THCPLongTensor_NewEmpty());
      if (!_min_indices_guard.get()) return NULL;
      THCPLongTensor* min_indices = _min_indices_guard.get();
      
      #else
      THPLongTensorPtr _min_indices_guard((THPLongTensor*) THPLongTensor_NewEmpty());
      if (!_min_indices_guard.get()) return NULL;
      THPLongTensor* min_indices = _min_indices_guard.get();
      
      #endif
      
      
      THTensor* arg_min = ((THPTensor*)min)->cdata;
      THIndexTensor* arg_min_indices = ((THPIndexTensor*)min_indices)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim));
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        maybeThrowBackCompatKeepdimWarn("min");
        Py_UNBLOCK_THREADS;
        THTensor_(min)(LIBRARY_STATE arg_min, arg_min_indices, arg_self, arg_dim, false);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, min, min_indices);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        real __result = THTensor_(minall)(LIBRARY_STATE arg_self);
        Py_BLOCK_THREADS;
        return THPUtils_(newReal)(__result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.min", 4, "(" THPTensorStr " source)", "(" THPTensorStr " source, " THPTensorStr " other, #" THPTensorStr " out)", "(" THPTensorStr " source, int dim, #tuple[" THPTensorStr ", " THPModuleStr "LongTensor] out)", "(" THPTensorStr " source, int dim, bool keepdim, #tuple[" THPTensorStr ", " THPModuleStr "LongTensor] out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_(max)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_dim = NULL;
    PyObject *__kw_keepdim = NULL;
    PyObject *__kw_other = NULL;
    if (kwargs) {
      __kw_dim = PyDict_GetItemString(kwargs, "dim");
      __kw_keepdim = PyDict_GetItemString(kwargs, "keepdim");
      __kw_other = PyDict_GetItemString(kwargs, "other");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPIndexTensorClass &&
          (__tuplecount > 0 || __kw_dim) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim)) &&
          (__tuplecount > 1 || __kw_keepdim) && PyBool_Check((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_keepdim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_max = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THIndexTensor* arg_max_indices = ((THPIndexTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim));
      bool arg_keepdim = ((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_keepdim) == Py_True ? true : false);
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(max)(LIBRARY_STATE arg_max, arg_max_indices, arg_self, arg_dim, arg_keepdim);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPIndexTensorClass &&
          (__tuplecount > 0 || __kw_dim) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_max = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THIndexTensor* arg_max_indices = ((THPIndexTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim));
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        maybeThrowBackCompatKeepdimWarn("max");
        Py_UNBLOCK_THREADS;
        THTensor_(max)(LIBRARY_STATE arg_max, arg_max_indices, arg_self, arg_dim, false);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_self_guard.get(), arg_other_guard.get(),
              arg_self, arg_other,
              "self", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cmax)(LIBRARY_STATE arg_result, arg_self, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_other = arg_other_save;
      
      
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_dim) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim)) &&
          (__tuplecount > 1 || __kw_keepdim) && PyBool_Check((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_keepdim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _max_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_max_guard.get()) return NULL;
      THPTensor* max = _max_guard.get();
      
      #if IS_CUDA
      THCPLongTensorPtr _max_indices_guard((THCPLongTensor*) THCPLongTensor_NewEmpty());
      if (!_max_indices_guard.get()) return NULL;
      THCPLongTensor* max_indices = _max_indices_guard.get();
      
      #else
      THPLongTensorPtr _max_indices_guard((THPLongTensor*) THPLongTensor_NewEmpty());
      if (!_max_indices_guard.get()) return NULL;
      THPLongTensor* max_indices = _max_indices_guard.get();
      
      #endif
      
      
      THTensor* arg_max = ((THPTensor*)max)->cdata;
      THIndexTensor* arg_max_indices = ((THPIndexTensor*)max_indices)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim));
      bool arg_keepdim = ((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_keepdim) == Py_True ? true : false);
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(max)(LIBRARY_STATE arg_max, arg_max_indices, arg_self, arg_dim, arg_keepdim);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, max, max_indices);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_self_guard.get(), arg_other_guard.get(),
              arg_self, arg_other,
              "self", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cmax)(LIBRARY_STATE arg_result, arg_self, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_other = arg_other_save;
      
      
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_dim) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _max_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_max_guard.get()) return NULL;
      THPTensor* max = _max_guard.get();
      
      #if IS_CUDA
      THCPLongTensorPtr _max_indices_guard((THCPLongTensor*) THCPLongTensor_NewEmpty());
      if (!_max_indices_guard.get()) return NULL;
      THCPLongTensor* max_indices = _max_indices_guard.get();
      
      #else
      THPLongTensorPtr _max_indices_guard((THPLongTensor*) THPLongTensor_NewEmpty());
      if (!_max_indices_guard.get()) return NULL;
      THPLongTensor* max_indices = _max_indices_guard.get();
      
      #endif
      
      
      THTensor* arg_max = ((THPTensor*)max)->cdata;
      THIndexTensor* arg_max_indices = ((THPIndexTensor*)max_indices)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim));
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        maybeThrowBackCompatKeepdimWarn("max");
        Py_UNBLOCK_THREADS;
        THTensor_(max)(LIBRARY_STATE arg_max, arg_max_indices, arg_self, arg_dim, false);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, max, max_indices);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        real __result = THTensor_(maxall)(LIBRARY_STATE arg_self);
        Py_BLOCK_THREADS;
        return THPUtils_(newReal)(__result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "max", 4, "no arguments", "(" THPTensorStr " other, #" THPTensorStr " out)", "(int dim, #tuple[" THPTensorStr ", " THPModuleStr "LongTensor] out)", "(int dim, bool keepdim, #tuple[" THPTensorStr ", " THPModuleStr "LongTensor] out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_stateless_(max)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    PyObject *__kw_dim = NULL;
    PyObject *__kw_keepdim = NULL;
    PyObject *__kw_other = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
      __kw_dim = PyDict_GetItemString(kwargs, "dim");
      __kw_keepdim = PyDict_GetItemString(kwargs, "keepdim");
      __kw_other = PyDict_GetItemString(kwargs, "other");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 4 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPIndexTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_dim) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim)) &&
          (__tuplecount > 2 || __kw_keepdim) && PyBool_Check((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_keepdim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_max = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THIndexTensor* arg_max_indices = ((THPIndexTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim));
      bool arg_keepdim = ((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_keepdim) == Py_True ? true : false);
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(max)(LIBRARY_STATE arg_max, arg_max_indices, arg_self, arg_dim, arg_keepdim);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPIndexTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_dim) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_max = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THIndexTensor* arg_max_indices = ((THPIndexTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim));
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        maybeThrowBackCompatKeepdimWarn("max");
        Py_UNBLOCK_THREADS;
        THTensor_(max)(LIBRARY_STATE arg_max, arg_max_indices, arg_self, arg_dim, false);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_self_guard.get(), arg_other_guard.get(),
              arg_self, arg_other,
              "self", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cmax)(LIBRARY_STATE arg_result, arg_self, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_other = arg_other_save;
      
      
    
    } else if (___out == NULL &&
          __argcount == 3 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_dim) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim)) &&
          (__tuplecount > 2 || __kw_keepdim) && PyBool_Check((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_keepdim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _max_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_max_guard.get()) return NULL;
      THPTensor* max = _max_guard.get();
      
      #if IS_CUDA
      THCPLongTensorPtr _max_indices_guard((THCPLongTensor*) THCPLongTensor_NewEmpty());
      if (!_max_indices_guard.get()) return NULL;
      THCPLongTensor* max_indices = _max_indices_guard.get();
      
      #else
      THPLongTensorPtr _max_indices_guard((THPLongTensor*) THPLongTensor_NewEmpty());
      if (!_max_indices_guard.get()) return NULL;
      THPLongTensor* max_indices = _max_indices_guard.get();
      
      #endif
      
      
      THTensor* arg_max = ((THPTensor*)max)->cdata;
      THIndexTensor* arg_max_indices = ((THPIndexTensor*)max_indices)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim));
      bool arg_keepdim = ((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_keepdim) == Py_True ? true : false);
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(max)(LIBRARY_STATE arg_max, arg_max_indices, arg_self, arg_dim, arg_keepdim);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, max, max_indices);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THTensor* arg_other = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other))->cdata;
      
      THTensor *arg_self_save = arg_self;
      THTensorPtr arg_self_guard(nullptr);
      THTensor *arg_other_save = arg_other;
      THTensorPtr arg_other_guard(nullptr);
      
      bool try_expand = !THSize_isSameSizeAs(arg_self->size, arg_self->nDimension,
          arg_other->size, arg_other->nDimension);
      if (try_expand) {
        bool expand_success = false;
        try {
          arg_self_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          arg_other_guard =
          THTensor_(new)(LIBRARY_STATE_NOARGS);
          
          expand_outplace2(LIBRARY_STATE arg_self_guard.get(), arg_other_guard.get(),
              arg_self, arg_other,
              "self", "other", !false);
          expand_success = true;
        }
      catch (std::exception &e) {}
        if(expand_success) {
          arg_self = arg_self_guard.get();
          arg_other = arg_other_guard.get();
        }
      }
      
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cmax)(LIBRARY_STATE arg_result, arg_self, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
      arg_self = arg_self_save;
      arg_other = arg_other_save;
      
      
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_dim) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _max_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_max_guard.get()) return NULL;
      THPTensor* max = _max_guard.get();
      
      #if IS_CUDA
      THCPLongTensorPtr _max_indices_guard((THCPLongTensor*) THCPLongTensor_NewEmpty());
      if (!_max_indices_guard.get()) return NULL;
      THCPLongTensor* max_indices = _max_indices_guard.get();
      
      #else
      THPLongTensorPtr _max_indices_guard((THPLongTensor*) THPLongTensor_NewEmpty());
      if (!_max_indices_guard.get()) return NULL;
      THPLongTensor* max_indices = _max_indices_guard.get();
      
      #endif
      
      
      THTensor* arg_max = ((THPTensor*)max)->cdata;
      THIndexTensor* arg_max_indices = ((THPIndexTensor*)max_indices)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim));
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        maybeThrowBackCompatKeepdimWarn("max");
        Py_UNBLOCK_THREADS;
        THTensor_(max)(LIBRARY_STATE arg_max, arg_max_indices, arg_self, arg_dim, false);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, max, max_indices);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        real __result = THTensor_(maxall)(LIBRARY_STATE arg_self);
        Py_BLOCK_THREADS;
        return THPUtils_(newReal)(__result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.max", 4, "(" THPTensorStr " source)", "(" THPTensorStr " source, " THPTensorStr " other, #" THPTensorStr " out)", "(" THPTensorStr " source, int dim, #tuple[" THPTensorStr ", " THPModuleStr "LongTensor] out)", "(" THPTensorStr " source, int dim, bool keepdim, #tuple[" THPTensorStr ", " THPModuleStr "LongTensor] out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF) && (!IS_CUDA)
PyObject * THPTensor_(kthvalue)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_k = NULL;
    PyObject *__kw_dim = NULL;
    PyObject *__kw_keepdim = NULL;
    if (kwargs) {
      __kw_k = PyDict_GetItemString(kwargs, "k");
      __kw_dim = PyDict_GetItemString(kwargs, "dim");
      __kw_keepdim = PyDict_GetItemString(kwargs, "keepdim");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 4 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPIndexTensorClass &&
          (__tuplecount > 0 || __kw_k) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_k)) &&
          (__tuplecount > 1 || __kw_dim) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim)) &&
          (__tuplecount > 2 || __kw_keepdim) && PyBool_Check((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_keepdim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_values = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THIndexTensor* arg_indices = ((THPIndexTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_k = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_k));
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim));
      bool arg_keepdim = ((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_keepdim) == Py_True ? true : false);
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(kthvalue)(LIBRARY_STATE arg_values, arg_indices, arg_self, arg_k, arg_dim, arg_keepdim);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPIndexTensorClass &&
          (__tuplecount > 0 || __kw_k) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_k)) &&
          (__tuplecount > 1 || __kw_keepdim) && PyBool_Check((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_keepdim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_values = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THIndexTensor* arg_indices = ((THPIndexTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_k = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_k));
      bool arg_keepdim = ((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_keepdim) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        int64_t __last_dim = THTensor_(nDimension)(LIBRARY_STATE ((THPTensor*)self)->cdata)-1;
        Py_UNBLOCK_THREADS;
        THTensor_(kthvalue)(LIBRARY_STATE arg_values, arg_indices, arg_self, arg_k, __last_dim, arg_keepdim);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPIndexTensorClass &&
          (__tuplecount > 0 || __kw_k) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_k)) &&
          (__tuplecount > 1 || __kw_dim) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_values = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THIndexTensor* arg_indices = ((THPIndexTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_k = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_k));
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim));
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        maybeThrowBackCompatKeepdimWarn("kthvalue");
        Py_UNBLOCK_THREADS;
        THTensor_(kthvalue)(LIBRARY_STATE arg_values, arg_indices, arg_self, arg_k, arg_dim, false);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPIndexTensorClass &&
          (__tuplecount > 0 || __kw_k) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_k))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_values = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THIndexTensor* arg_indices = ((THPIndexTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_k = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_k));
      
      PyThreadState *_save = NULL;
      try {
        int64_t __last_dim = THTensor_(nDimension)(LIBRARY_STATE ((THPTensor*)self)->cdata)-1;
        maybeThrowBackCompatKeepdimWarn("kthvalue");
        
        Py_UNBLOCK_THREADS;
        THTensor_(kthvalue)(LIBRARY_STATE arg_values, arg_indices, arg_self, arg_k, __last_dim, false);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 3 &&
          (__tuplecount > 0 || __kw_k) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_k)) &&
          (__tuplecount > 1 || __kw_dim) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim)) &&
          (__tuplecount > 2 || __kw_keepdim) && PyBool_Check((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_keepdim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _values_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_values_guard.get()) return NULL;
      THPTensor* values = _values_guard.get();
      
      #if IS_CUDA
      THCPLongTensorPtr _indices_guard((THCPLongTensor*) THCPLongTensor_NewEmpty());
      if (!_indices_guard.get()) return NULL;
      THCPLongTensor* indices = _indices_guard.get();
      
      #else
      THPLongTensorPtr _indices_guard((THPLongTensor*) THPLongTensor_NewEmpty());
      if (!_indices_guard.get()) return NULL;
      THPLongTensor* indices = _indices_guard.get();
      
      #endif
      
      
      THTensor* arg_values = ((THPTensor*)values)->cdata;
      THIndexTensor* arg_indices = ((THPIndexTensor*)indices)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_k = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_k));
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim));
      bool arg_keepdim = ((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_keepdim) == Py_True ? true : false);
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(kthvalue)(LIBRARY_STATE arg_values, arg_indices, arg_self, arg_k, arg_dim, arg_keepdim);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, values, indices);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_k) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_k)) &&
          (__tuplecount > 1 || __kw_keepdim) && PyBool_Check((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_keepdim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _values_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_values_guard.get()) return NULL;
      THPTensor* values = _values_guard.get();
      
      #if IS_CUDA
      THCPLongTensorPtr _indices_guard((THCPLongTensor*) THCPLongTensor_NewEmpty());
      if (!_indices_guard.get()) return NULL;
      THCPLongTensor* indices = _indices_guard.get();
      
      #else
      THPLongTensorPtr _indices_guard((THPLongTensor*) THPLongTensor_NewEmpty());
      if (!_indices_guard.get()) return NULL;
      THPLongTensor* indices = _indices_guard.get();
      
      #endif
      
      
      THTensor* arg_values = ((THPTensor*)values)->cdata;
      THIndexTensor* arg_indices = ((THPIndexTensor*)indices)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_k = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_k));
      bool arg_keepdim = ((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_keepdim) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        int64_t __last_dim = THTensor_(nDimension)(LIBRARY_STATE ((THPTensor*)self)->cdata)-1;
        Py_UNBLOCK_THREADS;
        THTensor_(kthvalue)(LIBRARY_STATE arg_values, arg_indices, arg_self, arg_k, __last_dim, arg_keepdim);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, values, indices);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_k) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_k)) &&
          (__tuplecount > 1 || __kw_dim) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _values_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_values_guard.get()) return NULL;
      THPTensor* values = _values_guard.get();
      
      #if IS_CUDA
      THCPLongTensorPtr _indices_guard((THCPLongTensor*) THCPLongTensor_NewEmpty());
      if (!_indices_guard.get()) return NULL;
      THCPLongTensor* indices = _indices_guard.get();
      
      #else
      THPLongTensorPtr _indices_guard((THPLongTensor*) THPLongTensor_NewEmpty());
      if (!_indices_guard.get()) return NULL;
      THPLongTensor* indices = _indices_guard.get();
      
      #endif
      
      
      THTensor* arg_values = ((THPTensor*)values)->cdata;
      THIndexTensor* arg_indices = ((THPIndexTensor*)indices)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_k = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_k));
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim));
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        maybeThrowBackCompatKeepdimWarn("kthvalue");
        Py_UNBLOCK_THREADS;
        THTensor_(kthvalue)(LIBRARY_STATE arg_values, arg_indices, arg_self, arg_k, arg_dim, false);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, values, indices);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_k) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_k))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _values_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_values_guard.get()) return NULL;
      THPTensor* values = _values_guard.get();
      
      #if IS_CUDA
      THCPLongTensorPtr _indices_guard((THCPLongTensor*) THCPLongTensor_NewEmpty());
      if (!_indices_guard.get()) return NULL;
      THCPLongTensor* indices = _indices_guard.get();
      
      #else
      THPLongTensorPtr _indices_guard((THPLongTensor*) THPLongTensor_NewEmpty());
      if (!_indices_guard.get()) return NULL;
      THPLongTensor* indices = _indices_guard.get();
      
      #endif
      
      
      THTensor* arg_values = ((THPTensor*)values)->cdata;
      THIndexTensor* arg_indices = ((THPIndexTensor*)indices)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_k = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_k));
      
      PyThreadState *_save = NULL;
      try {
        int64_t __last_dim = THTensor_(nDimension)(LIBRARY_STATE ((THPTensor*)self)->cdata)-1;
        maybeThrowBackCompatKeepdimWarn("kthvalue");
        
        Py_UNBLOCK_THREADS;
        THTensor_(kthvalue)(LIBRARY_STATE arg_values, arg_indices, arg_self, arg_k, __last_dim, false);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, values, indices);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "kthvalue", 4, "(int k, #tuple[" THPTensorStr ", " THPModuleStr "LongTensor] out)", "(int k, int dim, #tuple[" THPTensorStr ", " THPModuleStr "LongTensor] out)", "(int k, bool keepdim, #tuple[" THPTensorStr ", " THPModuleStr "LongTensor] out)", "(int k, int dim, bool keepdim, #tuple[" THPTensorStr ", " THPModuleStr "LongTensor] out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF) && (!IS_CUDA)
PyObject * THPTensor_stateless_(kthvalue)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    PyObject *__kw_k = NULL;
    PyObject *__kw_dim = NULL;
    PyObject *__kw_keepdim = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
      __kw_k = PyDict_GetItemString(kwargs, "k");
      __kw_dim = PyDict_GetItemString(kwargs, "dim");
      __kw_keepdim = PyDict_GetItemString(kwargs, "keepdim");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 5 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPIndexTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_k) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_k)) &&
          (__tuplecount > 2 || __kw_dim) && THPUtils_checkLong((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_dim)) &&
          (__tuplecount > 3 || __kw_keepdim) && PyBool_Check((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_keepdim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_values = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THIndexTensor* arg_indices = ((THPIndexTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int64_t arg_k = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_k));
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_dim));
      bool arg_keepdim = ((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_keepdim) == Py_True ? true : false);
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(kthvalue)(LIBRARY_STATE arg_values, arg_indices, arg_self, arg_k, arg_dim, arg_keepdim);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 4 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPIndexTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_k) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_k)) &&
          (__tuplecount > 2 || __kw_keepdim) && PyBool_Check((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_keepdim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_values = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THIndexTensor* arg_indices = ((THPIndexTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int64_t arg_k = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_k));
      bool arg_keepdim = ((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_keepdim) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        int64_t __last_dim = THTensor_(nDimension)(LIBRARY_STATE ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata)-1;
        Py_UNBLOCK_THREADS;
        THTensor_(kthvalue)(LIBRARY_STATE arg_values, arg_indices, arg_self, arg_k, __last_dim, arg_keepdim);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 4 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPIndexTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_k) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_k)) &&
          (__tuplecount > 2 || __kw_dim) && THPUtils_checkLong((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_dim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_values = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THIndexTensor* arg_indices = ((THPIndexTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int64_t arg_k = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_k));
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_dim));
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        maybeThrowBackCompatKeepdimWarn("kthvalue");
        Py_UNBLOCK_THREADS;
        THTensor_(kthvalue)(LIBRARY_STATE arg_values, arg_indices, arg_self, arg_k, arg_dim, false);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPIndexTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_k) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_k))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_values = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THIndexTensor* arg_indices = ((THPIndexTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int64_t arg_k = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_k));
      
      PyThreadState *_save = NULL;
      try {
        int64_t __last_dim = THTensor_(nDimension)(LIBRARY_STATE ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata)-1;
        maybeThrowBackCompatKeepdimWarn("kthvalue");
        
        Py_UNBLOCK_THREADS;
        THTensor_(kthvalue)(LIBRARY_STATE arg_values, arg_indices, arg_self, arg_k, __last_dim, false);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 4 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_k) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_k)) &&
          (__tuplecount > 2 || __kw_dim) && THPUtils_checkLong((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_dim)) &&
          (__tuplecount > 3 || __kw_keepdim) && PyBool_Check((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_keepdim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _values_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_values_guard.get()) return NULL;
      THPTensor* values = _values_guard.get();
      
      #if IS_CUDA
      THCPLongTensorPtr _indices_guard((THCPLongTensor*) THCPLongTensor_NewEmpty());
      if (!_indices_guard.get()) return NULL;
      THCPLongTensor* indices = _indices_guard.get();
      
      #else
      THPLongTensorPtr _indices_guard((THPLongTensor*) THPLongTensor_NewEmpty());
      if (!_indices_guard.get()) return NULL;
      THPLongTensor* indices = _indices_guard.get();
      
      #endif
      
      
      THTensor* arg_values = ((THPTensor*)values)->cdata;
      THIndexTensor* arg_indices = ((THPIndexTensor*)indices)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int64_t arg_k = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_k));
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_dim));
      bool arg_keepdim = ((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_keepdim) == Py_True ? true : false);
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(kthvalue)(LIBRARY_STATE arg_values, arg_indices, arg_self, arg_k, arg_dim, arg_keepdim);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, values, indices);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 3 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_k) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_k)) &&
          (__tuplecount > 2 || __kw_keepdim) && PyBool_Check((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_keepdim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _values_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_values_guard.get()) return NULL;
      THPTensor* values = _values_guard.get();
      
      #if IS_CUDA
      THCPLongTensorPtr _indices_guard((THCPLongTensor*) THCPLongTensor_NewEmpty());
      if (!_indices_guard.get()) return NULL;
      THCPLongTensor* indices = _indices_guard.get();
      
      #else
      THPLongTensorPtr _indices_guard((THPLongTensor*) THPLongTensor_NewEmpty());
      if (!_indices_guard.get()) return NULL;
      THPLongTensor* indices = _indices_guard.get();
      
      #endif
      
      
      THTensor* arg_values = ((THPTensor*)values)->cdata;
      THIndexTensor* arg_indices = ((THPIndexTensor*)indices)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int64_t arg_k = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_k));
      bool arg_keepdim = ((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_keepdim) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        int64_t __last_dim = THTensor_(nDimension)(LIBRARY_STATE ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata)-1;
        Py_UNBLOCK_THREADS;
        THTensor_(kthvalue)(LIBRARY_STATE arg_values, arg_indices, arg_self, arg_k, __last_dim, arg_keepdim);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, values, indices);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 3 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_k) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_k)) &&
          (__tuplecount > 2 || __kw_dim) && THPUtils_checkLong((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_dim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _values_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_values_guard.get()) return NULL;
      THPTensor* values = _values_guard.get();
      
      #if IS_CUDA
      THCPLongTensorPtr _indices_guard((THCPLongTensor*) THCPLongTensor_NewEmpty());
      if (!_indices_guard.get()) return NULL;
      THCPLongTensor* indices = _indices_guard.get();
      
      #else
      THPLongTensorPtr _indices_guard((THPLongTensor*) THPLongTensor_NewEmpty());
      if (!_indices_guard.get()) return NULL;
      THPLongTensor* indices = _indices_guard.get();
      
      #endif
      
      
      THTensor* arg_values = ((THPTensor*)values)->cdata;
      THIndexTensor* arg_indices = ((THPIndexTensor*)indices)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int64_t arg_k = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_k));
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_dim));
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        maybeThrowBackCompatKeepdimWarn("kthvalue");
        Py_UNBLOCK_THREADS;
        THTensor_(kthvalue)(LIBRARY_STATE arg_values, arg_indices, arg_self, arg_k, arg_dim, false);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, values, indices);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_k) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_k))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _values_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_values_guard.get()) return NULL;
      THPTensor* values = _values_guard.get();
      
      #if IS_CUDA
      THCPLongTensorPtr _indices_guard((THCPLongTensor*) THCPLongTensor_NewEmpty());
      if (!_indices_guard.get()) return NULL;
      THCPLongTensor* indices = _indices_guard.get();
      
      #else
      THPLongTensorPtr _indices_guard((THPLongTensor*) THPLongTensor_NewEmpty());
      if (!_indices_guard.get()) return NULL;
      THPLongTensor* indices = _indices_guard.get();
      
      #endif
      
      
      THTensor* arg_values = ((THPTensor*)values)->cdata;
      THIndexTensor* arg_indices = ((THPIndexTensor*)indices)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int64_t arg_k = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_k));
      
      PyThreadState *_save = NULL;
      try {
        int64_t __last_dim = THTensor_(nDimension)(LIBRARY_STATE ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata)-1;
        maybeThrowBackCompatKeepdimWarn("kthvalue");
        
        Py_UNBLOCK_THREADS;
        THTensor_(kthvalue)(LIBRARY_STATE arg_values, arg_indices, arg_self, arg_k, __last_dim, false);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, values, indices);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.kthvalue", 4, "(" THPTensorStr " source, int k, #tuple[" THPTensorStr ", " THPModuleStr "LongTensor] out)", "(" THPTensorStr " source, int k, int dim, #tuple[" THPTensorStr ", " THPModuleStr "LongTensor] out)", "(" THPTensorStr " source, int k, bool keepdim, #tuple[" THPTensorStr ", " THPModuleStr "LongTensor] out)", "(" THPTensorStr " source, int k, int dim, bool keepdim, #tuple[" THPTensorStr ", " THPModuleStr "LongTensor] out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_(mode)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_dim = NULL;
    PyObject *__kw_keepdim = NULL;
    if (kwargs) {
      __kw_dim = PyDict_GetItemString(kwargs, "dim");
      __kw_keepdim = PyDict_GetItemString(kwargs, "keepdim");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPIndexTensorClass &&
          (__tuplecount > 0 || __kw_dim) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim)) &&
          (__tuplecount > 1 || __kw_keepdim) && PyBool_Check((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_keepdim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_values = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THIndexTensor* arg_indices = ((THPIndexTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim));
      bool arg_keepdim = ((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_keepdim) == Py_True ? true : false);
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(mode)(LIBRARY_STATE arg_values, arg_indices, arg_self, arg_dim, arg_keepdim);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPIndexTensorClass &&
          (__tuplecount > 0 || __kw_keepdim) && PyBool_Check((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_keepdim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_values = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THIndexTensor* arg_indices = ((THPIndexTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      bool arg_keepdim = ((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_keepdim) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        int64_t __last_dim = THTensor_(nDimension)(LIBRARY_STATE ((THPTensor*)self)->cdata)-1;
        Py_UNBLOCK_THREADS;
        THTensor_(mode)(LIBRARY_STATE arg_values, arg_indices, arg_self, __last_dim, arg_keepdim);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPIndexTensorClass &&
          (__tuplecount > 0 || __kw_dim) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_values = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THIndexTensor* arg_indices = ((THPIndexTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim));
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        maybeThrowBackCompatKeepdimWarn("mode");
        Py_UNBLOCK_THREADS;
        THTensor_(mode)(LIBRARY_STATE arg_values, arg_indices, arg_self, arg_dim, false);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 1 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPIndexTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_values = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THIndexTensor* arg_indices = ((THPIndexTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        int64_t __last_dim = THTensor_(nDimension)(LIBRARY_STATE ((THPTensor*)self)->cdata)-1;
        maybeThrowBackCompatKeepdimWarn("mode");
        
        Py_UNBLOCK_THREADS;
        THTensor_(mode)(LIBRARY_STATE arg_values, arg_indices, arg_self, __last_dim, false);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_dim) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim)) &&
          (__tuplecount > 1 || __kw_keepdim) && PyBool_Check((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_keepdim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _values_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_values_guard.get()) return NULL;
      THPTensor* values = _values_guard.get();
      
      #if IS_CUDA
      THCPLongTensorPtr _indices_guard((THCPLongTensor*) THCPLongTensor_NewEmpty());
      if (!_indices_guard.get()) return NULL;
      THCPLongTensor* indices = _indices_guard.get();
      
      #else
      THPLongTensorPtr _indices_guard((THPLongTensor*) THPLongTensor_NewEmpty());
      if (!_indices_guard.get()) return NULL;
      THPLongTensor* indices = _indices_guard.get();
      
      #endif
      
      
      THTensor* arg_values = ((THPTensor*)values)->cdata;
      THIndexTensor* arg_indices = ((THPIndexTensor*)indices)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim));
      bool arg_keepdim = ((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_keepdim) == Py_True ? true : false);
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(mode)(LIBRARY_STATE arg_values, arg_indices, arg_self, arg_dim, arg_keepdim);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, values, indices);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_keepdim) && PyBool_Check((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_keepdim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _values_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_values_guard.get()) return NULL;
      THPTensor* values = _values_guard.get();
      
      #if IS_CUDA
      THCPLongTensorPtr _indices_guard((THCPLongTensor*) THCPLongTensor_NewEmpty());
      if (!_indices_guard.get()) return NULL;
      THCPLongTensor* indices = _indices_guard.get();
      
      #else
      THPLongTensorPtr _indices_guard((THPLongTensor*) THPLongTensor_NewEmpty());
      if (!_indices_guard.get()) return NULL;
      THPLongTensor* indices = _indices_guard.get();
      
      #endif
      
      
      THTensor* arg_values = ((THPTensor*)values)->cdata;
      THIndexTensor* arg_indices = ((THPIndexTensor*)indices)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      bool arg_keepdim = ((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_keepdim) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        int64_t __last_dim = THTensor_(nDimension)(LIBRARY_STATE ((THPTensor*)self)->cdata)-1;
        Py_UNBLOCK_THREADS;
        THTensor_(mode)(LIBRARY_STATE arg_values, arg_indices, arg_self, __last_dim, arg_keepdim);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, values, indices);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_dim) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _values_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_values_guard.get()) return NULL;
      THPTensor* values = _values_guard.get();
      
      #if IS_CUDA
      THCPLongTensorPtr _indices_guard((THCPLongTensor*) THCPLongTensor_NewEmpty());
      if (!_indices_guard.get()) return NULL;
      THCPLongTensor* indices = _indices_guard.get();
      
      #else
      THPLongTensorPtr _indices_guard((THPLongTensor*) THPLongTensor_NewEmpty());
      if (!_indices_guard.get()) return NULL;
      THPLongTensor* indices = _indices_guard.get();
      
      #endif
      
      
      THTensor* arg_values = ((THPTensor*)values)->cdata;
      THIndexTensor* arg_indices = ((THPIndexTensor*)indices)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim));
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        maybeThrowBackCompatKeepdimWarn("mode");
        Py_UNBLOCK_THREADS;
        THTensor_(mode)(LIBRARY_STATE arg_values, arg_indices, arg_self, arg_dim, false);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, values, indices);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _values_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_values_guard.get()) return NULL;
      THPTensor* values = _values_guard.get();
      
      #if IS_CUDA
      THCPLongTensorPtr _indices_guard((THCPLongTensor*) THCPLongTensor_NewEmpty());
      if (!_indices_guard.get()) return NULL;
      THCPLongTensor* indices = _indices_guard.get();
      
      #else
      THPLongTensorPtr _indices_guard((THPLongTensor*) THPLongTensor_NewEmpty());
      if (!_indices_guard.get()) return NULL;
      THPLongTensor* indices = _indices_guard.get();
      
      #endif
      
      
      THTensor* arg_values = ((THPTensor*)values)->cdata;
      THIndexTensor* arg_indices = ((THPIndexTensor*)indices)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        int64_t __last_dim = THTensor_(nDimension)(LIBRARY_STATE ((THPTensor*)self)->cdata)-1;
        maybeThrowBackCompatKeepdimWarn("mode");
        
        Py_UNBLOCK_THREADS;
        THTensor_(mode)(LIBRARY_STATE arg_values, arg_indices, arg_self, __last_dim, false);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, values, indices);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "mode", 4, "(#tuple[" THPTensorStr ", " THPModuleStr "LongTensor] out)", "(int dim, #tuple[" THPTensorStr ", " THPModuleStr "LongTensor] out)", "(bool keepdim, #tuple[" THPTensorStr ", " THPModuleStr "LongTensor] out)", "(int dim, bool keepdim, #tuple[" THPTensorStr ", " THPModuleStr "LongTensor] out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_stateless_(mode)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    PyObject *__kw_dim = NULL;
    PyObject *__kw_keepdim = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
      __kw_dim = PyDict_GetItemString(kwargs, "dim");
      __kw_keepdim = PyDict_GetItemString(kwargs, "keepdim");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 4 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPIndexTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_dim) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim)) &&
          (__tuplecount > 2 || __kw_keepdim) && PyBool_Check((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_keepdim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_values = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THIndexTensor* arg_indices = ((THPIndexTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim));
      bool arg_keepdim = ((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_keepdim) == Py_True ? true : false);
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(mode)(LIBRARY_STATE arg_values, arg_indices, arg_self, arg_dim, arg_keepdim);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPIndexTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_keepdim) && PyBool_Check((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_keepdim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_values = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THIndexTensor* arg_indices = ((THPIndexTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      bool arg_keepdim = ((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_keepdim) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        int64_t __last_dim = THTensor_(nDimension)(LIBRARY_STATE ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata)-1;
        Py_UNBLOCK_THREADS;
        THTensor_(mode)(LIBRARY_STATE arg_values, arg_indices, arg_self, __last_dim, arg_keepdim);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPIndexTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_dim) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_values = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THIndexTensor* arg_indices = ((THPIndexTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim));
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        maybeThrowBackCompatKeepdimWarn("mode");
        Py_UNBLOCK_THREADS;
        THTensor_(mode)(LIBRARY_STATE arg_values, arg_indices, arg_self, arg_dim, false);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPIndexTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_values = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THIndexTensor* arg_indices = ((THPIndexTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        int64_t __last_dim = THTensor_(nDimension)(LIBRARY_STATE ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata)-1;
        maybeThrowBackCompatKeepdimWarn("mode");
        
        Py_UNBLOCK_THREADS;
        THTensor_(mode)(LIBRARY_STATE arg_values, arg_indices, arg_self, __last_dim, false);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 3 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_dim) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim)) &&
          (__tuplecount > 2 || __kw_keepdim) && PyBool_Check((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_keepdim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _values_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_values_guard.get()) return NULL;
      THPTensor* values = _values_guard.get();
      
      #if IS_CUDA
      THCPLongTensorPtr _indices_guard((THCPLongTensor*) THCPLongTensor_NewEmpty());
      if (!_indices_guard.get()) return NULL;
      THCPLongTensor* indices = _indices_guard.get();
      
      #else
      THPLongTensorPtr _indices_guard((THPLongTensor*) THPLongTensor_NewEmpty());
      if (!_indices_guard.get()) return NULL;
      THPLongTensor* indices = _indices_guard.get();
      
      #endif
      
      
      THTensor* arg_values = ((THPTensor*)values)->cdata;
      THIndexTensor* arg_indices = ((THPIndexTensor*)indices)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim));
      bool arg_keepdim = ((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_keepdim) == Py_True ? true : false);
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(mode)(LIBRARY_STATE arg_values, arg_indices, arg_self, arg_dim, arg_keepdim);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, values, indices);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_keepdim) && PyBool_Check((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_keepdim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _values_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_values_guard.get()) return NULL;
      THPTensor* values = _values_guard.get();
      
      #if IS_CUDA
      THCPLongTensorPtr _indices_guard((THCPLongTensor*) THCPLongTensor_NewEmpty());
      if (!_indices_guard.get()) return NULL;
      THCPLongTensor* indices = _indices_guard.get();
      
      #else
      THPLongTensorPtr _indices_guard((THPLongTensor*) THPLongTensor_NewEmpty());
      if (!_indices_guard.get()) return NULL;
      THPLongTensor* indices = _indices_guard.get();
      
      #endif
      
      
      THTensor* arg_values = ((THPTensor*)values)->cdata;
      THIndexTensor* arg_indices = ((THPIndexTensor*)indices)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      bool arg_keepdim = ((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_keepdim) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        int64_t __last_dim = THTensor_(nDimension)(LIBRARY_STATE ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata)-1;
        Py_UNBLOCK_THREADS;
        THTensor_(mode)(LIBRARY_STATE arg_values, arg_indices, arg_self, __last_dim, arg_keepdim);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, values, indices);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_dim) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _values_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_values_guard.get()) return NULL;
      THPTensor* values = _values_guard.get();
      
      #if IS_CUDA
      THCPLongTensorPtr _indices_guard((THCPLongTensor*) THCPLongTensor_NewEmpty());
      if (!_indices_guard.get()) return NULL;
      THCPLongTensor* indices = _indices_guard.get();
      
      #else
      THPLongTensorPtr _indices_guard((THPLongTensor*) THPLongTensor_NewEmpty());
      if (!_indices_guard.get()) return NULL;
      THPLongTensor* indices = _indices_guard.get();
      
      #endif
      
      
      THTensor* arg_values = ((THPTensor*)values)->cdata;
      THIndexTensor* arg_indices = ((THPIndexTensor*)indices)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim));
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        maybeThrowBackCompatKeepdimWarn("mode");
        Py_UNBLOCK_THREADS;
        THTensor_(mode)(LIBRARY_STATE arg_values, arg_indices, arg_self, arg_dim, false);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, values, indices);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _values_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_values_guard.get()) return NULL;
      THPTensor* values = _values_guard.get();
      
      #if IS_CUDA
      THCPLongTensorPtr _indices_guard((THCPLongTensor*) THCPLongTensor_NewEmpty());
      if (!_indices_guard.get()) return NULL;
      THCPLongTensor* indices = _indices_guard.get();
      
      #else
      THPLongTensorPtr _indices_guard((THPLongTensor*) THPLongTensor_NewEmpty());
      if (!_indices_guard.get()) return NULL;
      THPLongTensor* indices = _indices_guard.get();
      
      #endif
      
      
      THTensor* arg_values = ((THPTensor*)values)->cdata;
      THIndexTensor* arg_indices = ((THPIndexTensor*)indices)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        int64_t __last_dim = THTensor_(nDimension)(LIBRARY_STATE ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata)-1;
        maybeThrowBackCompatKeepdimWarn("mode");
        
        Py_UNBLOCK_THREADS;
        THTensor_(mode)(LIBRARY_STATE arg_values, arg_indices, arg_self, __last_dim, false);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, values, indices);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.mode", 4, "(" THPTensorStr " source, #tuple[" THPTensorStr ", " THPModuleStr "LongTensor] out)", "(" THPTensorStr " source, int dim, #tuple[" THPTensorStr ", " THPModuleStr "LongTensor] out)", "(" THPTensorStr " source, bool keepdim, #tuple[" THPTensorStr ", " THPModuleStr "LongTensor] out)", "(" THPTensorStr " source, int dim, bool keepdim, #tuple[" THPTensorStr ", " THPModuleStr "LongTensor] out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_(median)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_dim = NULL;
    PyObject *__kw_keepdim = NULL;
    if (kwargs) {
      __kw_dim = PyDict_GetItemString(kwargs, "dim");
      __kw_keepdim = PyDict_GetItemString(kwargs, "keepdim");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPIndexTensorClass &&
          (__tuplecount > 0 || __kw_dim) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim)) &&
          (__tuplecount > 1 || __kw_keepdim) && PyBool_Check((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_keepdim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_values = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THIndexTensor* arg_indices = ((THPIndexTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim));
      bool arg_keepdim = ((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_keepdim) == Py_True ? true : false);
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(median)(LIBRARY_STATE arg_values, arg_indices, arg_self, arg_dim, arg_keepdim);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPIndexTensorClass &&
          (__tuplecount > 0 || __kw_keepdim) && PyBool_Check((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_keepdim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_values = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THIndexTensor* arg_indices = ((THPIndexTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      bool arg_keepdim = ((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_keepdim) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        int64_t __last_dim = THTensor_(nDimension)(LIBRARY_STATE ((THPTensor*)self)->cdata)-1;
        Py_UNBLOCK_THREADS;
        THTensor_(median)(LIBRARY_STATE arg_values, arg_indices, arg_self, __last_dim, arg_keepdim);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPIndexTensorClass &&
          (__tuplecount > 0 || __kw_dim) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_values = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THIndexTensor* arg_indices = ((THPIndexTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim));
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        maybeThrowBackCompatKeepdimWarn("median");
        Py_UNBLOCK_THREADS;
        THTensor_(median)(LIBRARY_STATE arg_values, arg_indices, arg_self, arg_dim, false);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_dim) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim)) &&
          (__tuplecount > 1 || __kw_keepdim) && PyBool_Check((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_keepdim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _values_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_values_guard.get()) return NULL;
      THPTensor* values = _values_guard.get();
      
      #if IS_CUDA
      THCPLongTensorPtr _indices_guard((THCPLongTensor*) THCPLongTensor_NewEmpty());
      if (!_indices_guard.get()) return NULL;
      THCPLongTensor* indices = _indices_guard.get();
      
      #else
      THPLongTensorPtr _indices_guard((THPLongTensor*) THPLongTensor_NewEmpty());
      if (!_indices_guard.get()) return NULL;
      THPLongTensor* indices = _indices_guard.get();
      
      #endif
      
      
      THTensor* arg_values = ((THPTensor*)values)->cdata;
      THIndexTensor* arg_indices = ((THPIndexTensor*)indices)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim));
      bool arg_keepdim = ((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_keepdim) == Py_True ? true : false);
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(median)(LIBRARY_STATE arg_values, arg_indices, arg_self, arg_dim, arg_keepdim);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, values, indices);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_keepdim) && PyBool_Check((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_keepdim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _values_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_values_guard.get()) return NULL;
      THPTensor* values = _values_guard.get();
      
      #if IS_CUDA
      THCPLongTensorPtr _indices_guard((THCPLongTensor*) THCPLongTensor_NewEmpty());
      if (!_indices_guard.get()) return NULL;
      THCPLongTensor* indices = _indices_guard.get();
      
      #else
      THPLongTensorPtr _indices_guard((THPLongTensor*) THPLongTensor_NewEmpty());
      if (!_indices_guard.get()) return NULL;
      THPLongTensor* indices = _indices_guard.get();
      
      #endif
      
      
      THTensor* arg_values = ((THPTensor*)values)->cdata;
      THIndexTensor* arg_indices = ((THPIndexTensor*)indices)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      bool arg_keepdim = ((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_keepdim) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        int64_t __last_dim = THTensor_(nDimension)(LIBRARY_STATE ((THPTensor*)self)->cdata)-1;
        Py_UNBLOCK_THREADS;
        THTensor_(median)(LIBRARY_STATE arg_values, arg_indices, arg_self, __last_dim, arg_keepdim);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, values, indices);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_dim) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _values_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_values_guard.get()) return NULL;
      THPTensor* values = _values_guard.get();
      
      #if IS_CUDA
      THCPLongTensorPtr _indices_guard((THCPLongTensor*) THCPLongTensor_NewEmpty());
      if (!_indices_guard.get()) return NULL;
      THCPLongTensor* indices = _indices_guard.get();
      
      #else
      THPLongTensorPtr _indices_guard((THPLongTensor*) THPLongTensor_NewEmpty());
      if (!_indices_guard.get()) return NULL;
      THPLongTensor* indices = _indices_guard.get();
      
      #endif
      
      
      THTensor* arg_values = ((THPTensor*)values)->cdata;
      THIndexTensor* arg_indices = ((THPIndexTensor*)indices)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim));
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        maybeThrowBackCompatKeepdimWarn("median");
        Py_UNBLOCK_THREADS;
        THTensor_(median)(LIBRARY_STATE arg_values, arg_indices, arg_self, arg_dim, false);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, values, indices);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        real __result = THTensor_(medianall)(LIBRARY_STATE arg_self);
        Py_BLOCK_THREADS;
        return THPUtils_(newReal)(__result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "median", 4, "no arguments", "(int dim, #tuple[" THPTensorStr ", " THPModuleStr "LongTensor] out)", "(bool keepdim, #tuple[" THPTensorStr ", " THPModuleStr "LongTensor] out)", "(int dim, bool keepdim, #tuple[" THPTensorStr ", " THPModuleStr "LongTensor] out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_stateless_(median)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    PyObject *__kw_dim = NULL;
    PyObject *__kw_keepdim = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
      __kw_dim = PyDict_GetItemString(kwargs, "dim");
      __kw_keepdim = PyDict_GetItemString(kwargs, "keepdim");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 4 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPIndexTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_dim) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim)) &&
          (__tuplecount > 2 || __kw_keepdim) && PyBool_Check((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_keepdim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_values = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THIndexTensor* arg_indices = ((THPIndexTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim));
      bool arg_keepdim = ((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_keepdim) == Py_True ? true : false);
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(median)(LIBRARY_STATE arg_values, arg_indices, arg_self, arg_dim, arg_keepdim);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPIndexTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_keepdim) && PyBool_Check((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_keepdim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_values = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THIndexTensor* arg_indices = ((THPIndexTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      bool arg_keepdim = ((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_keepdim) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        int64_t __last_dim = THTensor_(nDimension)(LIBRARY_STATE ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata)-1;
        Py_UNBLOCK_THREADS;
        THTensor_(median)(LIBRARY_STATE arg_values, arg_indices, arg_self, __last_dim, arg_keepdim);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPIndexTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_dim) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_values = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THIndexTensor* arg_indices = ((THPIndexTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim));
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        maybeThrowBackCompatKeepdimWarn("median");
        Py_UNBLOCK_THREADS;
        THTensor_(median)(LIBRARY_STATE arg_values, arg_indices, arg_self, arg_dim, false);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 3 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_dim) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim)) &&
          (__tuplecount > 2 || __kw_keepdim) && PyBool_Check((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_keepdim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _values_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_values_guard.get()) return NULL;
      THPTensor* values = _values_guard.get();
      
      #if IS_CUDA
      THCPLongTensorPtr _indices_guard((THCPLongTensor*) THCPLongTensor_NewEmpty());
      if (!_indices_guard.get()) return NULL;
      THCPLongTensor* indices = _indices_guard.get();
      
      #else
      THPLongTensorPtr _indices_guard((THPLongTensor*) THPLongTensor_NewEmpty());
      if (!_indices_guard.get()) return NULL;
      THPLongTensor* indices = _indices_guard.get();
      
      #endif
      
      
      THTensor* arg_values = ((THPTensor*)values)->cdata;
      THIndexTensor* arg_indices = ((THPIndexTensor*)indices)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim));
      bool arg_keepdim = ((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_keepdim) == Py_True ? true : false);
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(median)(LIBRARY_STATE arg_values, arg_indices, arg_self, arg_dim, arg_keepdim);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, values, indices);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_keepdim) && PyBool_Check((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_keepdim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _values_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_values_guard.get()) return NULL;
      THPTensor* values = _values_guard.get();
      
      #if IS_CUDA
      THCPLongTensorPtr _indices_guard((THCPLongTensor*) THCPLongTensor_NewEmpty());
      if (!_indices_guard.get()) return NULL;
      THCPLongTensor* indices = _indices_guard.get();
      
      #else
      THPLongTensorPtr _indices_guard((THPLongTensor*) THPLongTensor_NewEmpty());
      if (!_indices_guard.get()) return NULL;
      THPLongTensor* indices = _indices_guard.get();
      
      #endif
      
      
      THTensor* arg_values = ((THPTensor*)values)->cdata;
      THIndexTensor* arg_indices = ((THPIndexTensor*)indices)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      bool arg_keepdim = ((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_keepdim) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        int64_t __last_dim = THTensor_(nDimension)(LIBRARY_STATE ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata)-1;
        Py_UNBLOCK_THREADS;
        THTensor_(median)(LIBRARY_STATE arg_values, arg_indices, arg_self, __last_dim, arg_keepdim);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, values, indices);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_dim) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _values_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_values_guard.get()) return NULL;
      THPTensor* values = _values_guard.get();
      
      #if IS_CUDA
      THCPLongTensorPtr _indices_guard((THCPLongTensor*) THCPLongTensor_NewEmpty());
      if (!_indices_guard.get()) return NULL;
      THCPLongTensor* indices = _indices_guard.get();
      
      #else
      THPLongTensorPtr _indices_guard((THPLongTensor*) THPLongTensor_NewEmpty());
      if (!_indices_guard.get()) return NULL;
      THPLongTensor* indices = _indices_guard.get();
      
      #endif
      
      
      THTensor* arg_values = ((THPTensor*)values)->cdata;
      THIndexTensor* arg_indices = ((THPIndexTensor*)indices)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim));
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        maybeThrowBackCompatKeepdimWarn("median");
        Py_UNBLOCK_THREADS;
        THTensor_(median)(LIBRARY_STATE arg_values, arg_indices, arg_self, arg_dim, false);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, values, indices);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        real __result = THTensor_(medianall)(LIBRARY_STATE arg_self);
        Py_BLOCK_THREADS;
        return THPUtils_(newReal)(__result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.median", 4, "(" THPTensorStr " source)", "(" THPTensorStr " source, int dim, #tuple[" THPTensorStr ", " THPModuleStr "LongTensor] out)", "(" THPTensorStr " source, bool keepdim, #tuple[" THPTensorStr ", " THPModuleStr "LongTensor] out)", "(" THPTensorStr " source, int dim, bool keepdim, #tuple[" THPTensorStr ", " THPModuleStr "LongTensor] out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_(sort)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_dim = NULL;
    PyObject *__kw_descending = NULL;
    if (kwargs) {
      __kw_dim = PyDict_GetItemString(kwargs, "dim");
      __kw_descending = PyDict_GetItemString(kwargs, "descending");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPIndexTensorClass &&
          (__tuplecount > 0 || __kw_dim) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim)) &&
          (__tuplecount > 1 || __kw_descending) && PyBool_Check((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_descending))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_values = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THIndexTensor* arg_indices = ((THPIndexTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim));
      bool arg_descending = ((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_descending) == Py_True ? true : false);
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(sort)(LIBRARY_STATE arg_values, arg_indices, arg_self, arg_dim, arg_descending);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPIndexTensorClass &&
          __kw_descending && PyBool_Check(__kw_descending)) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_values = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THIndexTensor* arg_indices = ((THPIndexTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      bool arg_descending = (__kw_descending == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        int64_t __last_dim = THTensor_(nDimension)(LIBRARY_STATE ((THPTensor*)self)->cdata)-1;
        Py_UNBLOCK_THREADS;
        THTensor_(sort)(LIBRARY_STATE arg_values, arg_indices, arg_self, __last_dim, arg_descending);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPIndexTensorClass &&
          (__tuplecount > 0 || __kw_dim) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_values = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THIndexTensor* arg_indices = ((THPIndexTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim));
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(sort)(LIBRARY_STATE arg_values, arg_indices, arg_self, arg_dim, false);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 1 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPIndexTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_values = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THIndexTensor* arg_indices = ((THPIndexTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        int64_t __last_dim = THTensor_(nDimension)(LIBRARY_STATE ((THPTensor*)self)->cdata)-1;
        Py_UNBLOCK_THREADS;
        THTensor_(sort)(LIBRARY_STATE arg_values, arg_indices, arg_self, __last_dim, false);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_dim) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim)) &&
          (__tuplecount > 1 || __kw_descending) && PyBool_Check((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_descending))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _values_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_values_guard.get()) return NULL;
      THPTensor* values = _values_guard.get();
      
      #if IS_CUDA
      THCPLongTensorPtr _indices_guard((THCPLongTensor*) THCPLongTensor_NewEmpty());
      if (!_indices_guard.get()) return NULL;
      THCPLongTensor* indices = _indices_guard.get();
      
      #else
      THPLongTensorPtr _indices_guard((THPLongTensor*) THPLongTensor_NewEmpty());
      if (!_indices_guard.get()) return NULL;
      THPLongTensor* indices = _indices_guard.get();
      
      #endif
      
      
      THTensor* arg_values = ((THPTensor*)values)->cdata;
      THIndexTensor* arg_indices = ((THPIndexTensor*)indices)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim));
      bool arg_descending = ((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_descending) == Py_True ? true : false);
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(sort)(LIBRARY_STATE arg_values, arg_indices, arg_self, arg_dim, arg_descending);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, values, indices);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          __kw_descending && PyBool_Check(__kw_descending)) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _values_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_values_guard.get()) return NULL;
      THPTensor* values = _values_guard.get();
      
      #if IS_CUDA
      THCPLongTensorPtr _indices_guard((THCPLongTensor*) THCPLongTensor_NewEmpty());
      if (!_indices_guard.get()) return NULL;
      THCPLongTensor* indices = _indices_guard.get();
      
      #else
      THPLongTensorPtr _indices_guard((THPLongTensor*) THPLongTensor_NewEmpty());
      if (!_indices_guard.get()) return NULL;
      THPLongTensor* indices = _indices_guard.get();
      
      #endif
      
      
      THTensor* arg_values = ((THPTensor*)values)->cdata;
      THIndexTensor* arg_indices = ((THPIndexTensor*)indices)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      bool arg_descending = (__kw_descending == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        int64_t __last_dim = THTensor_(nDimension)(LIBRARY_STATE ((THPTensor*)self)->cdata)-1;
        Py_UNBLOCK_THREADS;
        THTensor_(sort)(LIBRARY_STATE arg_values, arg_indices, arg_self, __last_dim, arg_descending);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, values, indices);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_dim) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _values_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_values_guard.get()) return NULL;
      THPTensor* values = _values_guard.get();
      
      #if IS_CUDA
      THCPLongTensorPtr _indices_guard((THCPLongTensor*) THCPLongTensor_NewEmpty());
      if (!_indices_guard.get()) return NULL;
      THCPLongTensor* indices = _indices_guard.get();
      
      #else
      THPLongTensorPtr _indices_guard((THPLongTensor*) THPLongTensor_NewEmpty());
      if (!_indices_guard.get()) return NULL;
      THPLongTensor* indices = _indices_guard.get();
      
      #endif
      
      
      THTensor* arg_values = ((THPTensor*)values)->cdata;
      THIndexTensor* arg_indices = ((THPIndexTensor*)indices)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim));
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(sort)(LIBRARY_STATE arg_values, arg_indices, arg_self, arg_dim, false);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, values, indices);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _values_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_values_guard.get()) return NULL;
      THPTensor* values = _values_guard.get();
      
      #if IS_CUDA
      THCPLongTensorPtr _indices_guard((THCPLongTensor*) THCPLongTensor_NewEmpty());
      if (!_indices_guard.get()) return NULL;
      THCPLongTensor* indices = _indices_guard.get();
      
      #else
      THPLongTensorPtr _indices_guard((THPLongTensor*) THPLongTensor_NewEmpty());
      if (!_indices_guard.get()) return NULL;
      THPLongTensor* indices = _indices_guard.get();
      
      #endif
      
      
      THTensor* arg_values = ((THPTensor*)values)->cdata;
      THIndexTensor* arg_indices = ((THPIndexTensor*)indices)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        int64_t __last_dim = THTensor_(nDimension)(LIBRARY_STATE ((THPTensor*)self)->cdata)-1;
        Py_UNBLOCK_THREADS;
        THTensor_(sort)(LIBRARY_STATE arg_values, arg_indices, arg_self, __last_dim, false);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, values, indices);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "sort", 4, "(#tuple[" THPTensorStr ", " THPModuleStr "LongTensor] out)", "(int dim, #tuple[" THPTensorStr ", " THPModuleStr "LongTensor] out)", "(bool descending, #tuple[" THPTensorStr ", " THPModuleStr "LongTensor] out)", "(int dim, bool descending, #tuple[" THPTensorStr ", " THPModuleStr "LongTensor] out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_stateless_(sort)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    PyObject *__kw_dim = NULL;
    PyObject *__kw_descending = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
      __kw_dim = PyDict_GetItemString(kwargs, "dim");
      __kw_descending = PyDict_GetItemString(kwargs, "descending");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 4 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPIndexTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_dim) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim)) &&
          (__tuplecount > 2 || __kw_descending) && PyBool_Check((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_descending))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_values = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THIndexTensor* arg_indices = ((THPIndexTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim));
      bool arg_descending = ((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_descending) == Py_True ? true : false);
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(sort)(LIBRARY_STATE arg_values, arg_indices, arg_self, arg_dim, arg_descending);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPIndexTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          __kw_descending && PyBool_Check(__kw_descending)) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_values = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THIndexTensor* arg_indices = ((THPIndexTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      bool arg_descending = (__kw_descending == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        int64_t __last_dim = THTensor_(nDimension)(LIBRARY_STATE ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata)-1;
        Py_UNBLOCK_THREADS;
        THTensor_(sort)(LIBRARY_STATE arg_values, arg_indices, arg_self, __last_dim, arg_descending);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPIndexTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_dim) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_values = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THIndexTensor* arg_indices = ((THPIndexTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim));
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(sort)(LIBRARY_STATE arg_values, arg_indices, arg_self, arg_dim, false);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPIndexTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_values = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THIndexTensor* arg_indices = ((THPIndexTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        int64_t __last_dim = THTensor_(nDimension)(LIBRARY_STATE ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata)-1;
        Py_UNBLOCK_THREADS;
        THTensor_(sort)(LIBRARY_STATE arg_values, arg_indices, arg_self, __last_dim, false);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 3 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_dim) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim)) &&
          (__tuplecount > 2 || __kw_descending) && PyBool_Check((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_descending))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _values_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_values_guard.get()) return NULL;
      THPTensor* values = _values_guard.get();
      
      #if IS_CUDA
      THCPLongTensorPtr _indices_guard((THCPLongTensor*) THCPLongTensor_NewEmpty());
      if (!_indices_guard.get()) return NULL;
      THCPLongTensor* indices = _indices_guard.get();
      
      #else
      THPLongTensorPtr _indices_guard((THPLongTensor*) THPLongTensor_NewEmpty());
      if (!_indices_guard.get()) return NULL;
      THPLongTensor* indices = _indices_guard.get();
      
      #endif
      
      
      THTensor* arg_values = ((THPTensor*)values)->cdata;
      THIndexTensor* arg_indices = ((THPIndexTensor*)indices)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim));
      bool arg_descending = ((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_descending) == Py_True ? true : false);
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(sort)(LIBRARY_STATE arg_values, arg_indices, arg_self, arg_dim, arg_descending);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, values, indices);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          __kw_descending && PyBool_Check(__kw_descending)) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _values_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_values_guard.get()) return NULL;
      THPTensor* values = _values_guard.get();
      
      #if IS_CUDA
      THCPLongTensorPtr _indices_guard((THCPLongTensor*) THCPLongTensor_NewEmpty());
      if (!_indices_guard.get()) return NULL;
      THCPLongTensor* indices = _indices_guard.get();
      
      #else
      THPLongTensorPtr _indices_guard((THPLongTensor*) THPLongTensor_NewEmpty());
      if (!_indices_guard.get()) return NULL;
      THPLongTensor* indices = _indices_guard.get();
      
      #endif
      
      
      THTensor* arg_values = ((THPTensor*)values)->cdata;
      THIndexTensor* arg_indices = ((THPIndexTensor*)indices)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      bool arg_descending = (__kw_descending == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        int64_t __last_dim = THTensor_(nDimension)(LIBRARY_STATE ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata)-1;
        Py_UNBLOCK_THREADS;
        THTensor_(sort)(LIBRARY_STATE arg_values, arg_indices, arg_self, __last_dim, arg_descending);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, values, indices);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_dim) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _values_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_values_guard.get()) return NULL;
      THPTensor* values = _values_guard.get();
      
      #if IS_CUDA
      THCPLongTensorPtr _indices_guard((THCPLongTensor*) THCPLongTensor_NewEmpty());
      if (!_indices_guard.get()) return NULL;
      THCPLongTensor* indices = _indices_guard.get();
      
      #else
      THPLongTensorPtr _indices_guard((THPLongTensor*) THPLongTensor_NewEmpty());
      if (!_indices_guard.get()) return NULL;
      THPLongTensor* indices = _indices_guard.get();
      
      #endif
      
      
      THTensor* arg_values = ((THPTensor*)values)->cdata;
      THIndexTensor* arg_indices = ((THPIndexTensor*)indices)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim));
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(sort)(LIBRARY_STATE arg_values, arg_indices, arg_self, arg_dim, false);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, values, indices);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _values_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_values_guard.get()) return NULL;
      THPTensor* values = _values_guard.get();
      
      #if IS_CUDA
      THCPLongTensorPtr _indices_guard((THCPLongTensor*) THCPLongTensor_NewEmpty());
      if (!_indices_guard.get()) return NULL;
      THCPLongTensor* indices = _indices_guard.get();
      
      #else
      THPLongTensorPtr _indices_guard((THPLongTensor*) THPLongTensor_NewEmpty());
      if (!_indices_guard.get()) return NULL;
      THPLongTensor* indices = _indices_guard.get();
      
      #endif
      
      
      THTensor* arg_values = ((THPTensor*)values)->cdata;
      THIndexTensor* arg_indices = ((THPIndexTensor*)indices)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        int64_t __last_dim = THTensor_(nDimension)(LIBRARY_STATE ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata)-1;
        Py_UNBLOCK_THREADS;
        THTensor_(sort)(LIBRARY_STATE arg_values, arg_indices, arg_self, __last_dim, false);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, values, indices);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.sort", 4, "(" THPTensorStr " source, #tuple[" THPTensorStr ", " THPModuleStr "LongTensor] out)", "(" THPTensorStr " source, int dim, #tuple[" THPTensorStr ", " THPModuleStr "LongTensor] out)", "(" THPTensorStr " source, bool descending, #tuple[" THPTensorStr ", " THPModuleStr "LongTensor] out)", "(" THPTensorStr " source, int dim, bool descending, #tuple[" THPTensorStr ", " THPModuleStr "LongTensor] out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_(topk)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_k = NULL;
    PyObject *__kw_dim = NULL;
    PyObject *__kw_largest = NULL;
    PyObject *__kw_sorted = NULL;
    if (kwargs) {
      __kw_k = PyDict_GetItemString(kwargs, "k");
      __kw_dim = PyDict_GetItemString(kwargs, "dim");
      __kw_largest = PyDict_GetItemString(kwargs, "largest");
      __kw_sorted = PyDict_GetItemString(kwargs, "sorted");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 5 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPIndexTensorClass &&
          (__tuplecount > 0 || __kw_k) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_k)) &&
          (__tuplecount > 1 || __kw_dim) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim)) &&
          (__tuplecount > 2 || __kw_largest) && PyBool_Check((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_largest)) &&
          (__tuplecount > 3 || __kw_sorted) && PyBool_Check((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_sorted))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_values = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THIndexTensor* arg_indices = ((THPIndexTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_k = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_k));
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim));
      bool arg_largest = ((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_largest) == Py_True ? true : false);
      bool arg_sorted = ((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_sorted) == Py_True ? true : false);
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(topk)(LIBRARY_STATE arg_values, arg_indices, arg_self, arg_k, arg_dim, arg_largest, arg_sorted);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 4 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPIndexTensorClass &&
          (__tuplecount > 0 || __kw_k) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_k)) &&
          __kw_largest && PyBool_Check(__kw_largest) &&
          __kw_sorted && PyBool_Check(__kw_sorted)) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_values = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THIndexTensor* arg_indices = ((THPIndexTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_k = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_k));
      bool arg_largest = (__kw_largest == Py_True ? true : false);
      bool arg_sorted = (__kw_sorted == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        int64_t __last_dim = THTensor_(nDimension)(LIBRARY_STATE ((THPTensor*)self)->cdata)-1;
        Py_UNBLOCK_THREADS;
        THTensor_(topk)(LIBRARY_STATE arg_values, arg_indices, arg_self, arg_k, __last_dim, arg_largest, arg_sorted);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 4 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPIndexTensorClass &&
          (__tuplecount > 0 || __kw_k) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_k)) &&
          (__tuplecount > 1 || __kw_dim) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim)) &&
          (__tuplecount > 2 || __kw_largest) && PyBool_Check((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_largest))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_values = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THIndexTensor* arg_indices = ((THPIndexTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_k = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_k));
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim));
      bool arg_largest = ((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_largest) == Py_True ? true : false);
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(topk)(LIBRARY_STATE arg_values, arg_indices, arg_self, arg_k, arg_dim, arg_largest, true);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 4 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPIndexTensorClass &&
          (__tuplecount > 0 || __kw_k) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_k)) &&
          (__tuplecount > 1 || __kw_dim) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim)) &&
          __kw_sorted && PyBool_Check(__kw_sorted)) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_values = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THIndexTensor* arg_indices = ((THPIndexTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_k = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_k));
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim));
      bool arg_sorted = (__kw_sorted == Py_True ? true : false);
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(topk)(LIBRARY_STATE arg_values, arg_indices, arg_self, arg_k, arg_dim, true, arg_sorted);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPIndexTensorClass &&
          (__tuplecount > 0 || __kw_k) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_k)) &&
          __kw_largest && PyBool_Check(__kw_largest)) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_values = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THIndexTensor* arg_indices = ((THPIndexTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_k = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_k));
      bool arg_largest = (__kw_largest == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        int64_t __last_dim = THTensor_(nDimension)(LIBRARY_STATE ((THPTensor*)self)->cdata)-1;
        Py_UNBLOCK_THREADS;
        THTensor_(topk)(LIBRARY_STATE arg_values, arg_indices, arg_self, arg_k, __last_dim, arg_largest, true);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPIndexTensorClass &&
          (__tuplecount > 0 || __kw_k) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_k)) &&
          __kw_sorted && PyBool_Check(__kw_sorted)) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_values = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THIndexTensor* arg_indices = ((THPIndexTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_k = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_k));
      bool arg_sorted = (__kw_sorted == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        int64_t __last_dim = THTensor_(nDimension)(LIBRARY_STATE ((THPTensor*)self)->cdata)-1;
        Py_UNBLOCK_THREADS;
        THTensor_(topk)(LIBRARY_STATE arg_values, arg_indices, arg_self, arg_k, __last_dim, true, arg_sorted);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 4 &&
          (__tuplecount > 0 || __kw_k) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_k)) &&
          (__tuplecount > 1 || __kw_dim) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim)) &&
          (__tuplecount > 2 || __kw_largest) && PyBool_Check((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_largest)) &&
          (__tuplecount > 3 || __kw_sorted) && PyBool_Check((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_sorted))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _values_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_values_guard.get()) return NULL;
      THPTensor* values = _values_guard.get();
      
      #if IS_CUDA
      THCPLongTensorPtr _indices_guard((THCPLongTensor*) THCPLongTensor_NewEmpty());
      if (!_indices_guard.get()) return NULL;
      THCPLongTensor* indices = _indices_guard.get();
      
      #else
      THPLongTensorPtr _indices_guard((THPLongTensor*) THPLongTensor_NewEmpty());
      if (!_indices_guard.get()) return NULL;
      THPLongTensor* indices = _indices_guard.get();
      
      #endif
      
      
      THTensor* arg_values = ((THPTensor*)values)->cdata;
      THIndexTensor* arg_indices = ((THPIndexTensor*)indices)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_k = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_k));
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim));
      bool arg_largest = ((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_largest) == Py_True ? true : false);
      bool arg_sorted = ((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_sorted) == Py_True ? true : false);
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(topk)(LIBRARY_STATE arg_values, arg_indices, arg_self, arg_k, arg_dim, arg_largest, arg_sorted);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, values, indices);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPIndexTensorClass &&
          (__tuplecount > 0 || __kw_k) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_k)) &&
          (__tuplecount > 1 || __kw_dim) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_values = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THIndexTensor* arg_indices = ((THPIndexTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_k = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_k));
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim));
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(topk)(LIBRARY_STATE arg_values, arg_indices, arg_self, arg_k, arg_dim, true, true);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 3 &&
          (__tuplecount > 0 || __kw_k) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_k)) &&
          __kw_largest && PyBool_Check(__kw_largest) &&
          __kw_sorted && PyBool_Check(__kw_sorted)) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _values_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_values_guard.get()) return NULL;
      THPTensor* values = _values_guard.get();
      
      #if IS_CUDA
      THCPLongTensorPtr _indices_guard((THCPLongTensor*) THCPLongTensor_NewEmpty());
      if (!_indices_guard.get()) return NULL;
      THCPLongTensor* indices = _indices_guard.get();
      
      #else
      THPLongTensorPtr _indices_guard((THPLongTensor*) THPLongTensor_NewEmpty());
      if (!_indices_guard.get()) return NULL;
      THPLongTensor* indices = _indices_guard.get();
      
      #endif
      
      
      THTensor* arg_values = ((THPTensor*)values)->cdata;
      THIndexTensor* arg_indices = ((THPIndexTensor*)indices)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_k = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_k));
      bool arg_largest = (__kw_largest == Py_True ? true : false);
      bool arg_sorted = (__kw_sorted == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        int64_t __last_dim = THTensor_(nDimension)(LIBRARY_STATE ((THPTensor*)self)->cdata)-1;
        Py_UNBLOCK_THREADS;
        THTensor_(topk)(LIBRARY_STATE arg_values, arg_indices, arg_self, arg_k, __last_dim, arg_largest, arg_sorted);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, values, indices);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPIndexTensorClass &&
          (__tuplecount > 0 || __kw_k) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_k))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_values = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THIndexTensor* arg_indices = ((THPIndexTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_k = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_k));
      
      PyThreadState *_save = NULL;
      try {
        int64_t __last_dim = THTensor_(nDimension)(LIBRARY_STATE ((THPTensor*)self)->cdata)-1;
        Py_UNBLOCK_THREADS;
        THTensor_(topk)(LIBRARY_STATE arg_values, arg_indices, arg_self, arg_k, __last_dim, true, true);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 3 &&
          (__tuplecount > 0 || __kw_k) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_k)) &&
          (__tuplecount > 1 || __kw_dim) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim)) &&
          (__tuplecount > 2 || __kw_largest) && PyBool_Check((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_largest))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _values_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_values_guard.get()) return NULL;
      THPTensor* values = _values_guard.get();
      
      #if IS_CUDA
      THCPLongTensorPtr _indices_guard((THCPLongTensor*) THCPLongTensor_NewEmpty());
      if (!_indices_guard.get()) return NULL;
      THCPLongTensor* indices = _indices_guard.get();
      
      #else
      THPLongTensorPtr _indices_guard((THPLongTensor*) THPLongTensor_NewEmpty());
      if (!_indices_guard.get()) return NULL;
      THPLongTensor* indices = _indices_guard.get();
      
      #endif
      
      
      THTensor* arg_values = ((THPTensor*)values)->cdata;
      THIndexTensor* arg_indices = ((THPIndexTensor*)indices)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_k = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_k));
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim));
      bool arg_largest = ((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_largest) == Py_True ? true : false);
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(topk)(LIBRARY_STATE arg_values, arg_indices, arg_self, arg_k, arg_dim, arg_largest, true);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, values, indices);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 3 &&
          (__tuplecount > 0 || __kw_k) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_k)) &&
          (__tuplecount > 1 || __kw_dim) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim)) &&
          __kw_sorted && PyBool_Check(__kw_sorted)) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _values_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_values_guard.get()) return NULL;
      THPTensor* values = _values_guard.get();
      
      #if IS_CUDA
      THCPLongTensorPtr _indices_guard((THCPLongTensor*) THCPLongTensor_NewEmpty());
      if (!_indices_guard.get()) return NULL;
      THCPLongTensor* indices = _indices_guard.get();
      
      #else
      THPLongTensorPtr _indices_guard((THPLongTensor*) THPLongTensor_NewEmpty());
      if (!_indices_guard.get()) return NULL;
      THPLongTensor* indices = _indices_guard.get();
      
      #endif
      
      
      THTensor* arg_values = ((THPTensor*)values)->cdata;
      THIndexTensor* arg_indices = ((THPIndexTensor*)indices)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_k = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_k));
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim));
      bool arg_sorted = (__kw_sorted == Py_True ? true : false);
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(topk)(LIBRARY_STATE arg_values, arg_indices, arg_self, arg_k, arg_dim, true, arg_sorted);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, values, indices);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_k) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_k)) &&
          __kw_largest && PyBool_Check(__kw_largest)) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _values_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_values_guard.get()) return NULL;
      THPTensor* values = _values_guard.get();
      
      #if IS_CUDA
      THCPLongTensorPtr _indices_guard((THCPLongTensor*) THCPLongTensor_NewEmpty());
      if (!_indices_guard.get()) return NULL;
      THCPLongTensor* indices = _indices_guard.get();
      
      #else
      THPLongTensorPtr _indices_guard((THPLongTensor*) THPLongTensor_NewEmpty());
      if (!_indices_guard.get()) return NULL;
      THPLongTensor* indices = _indices_guard.get();
      
      #endif
      
      
      THTensor* arg_values = ((THPTensor*)values)->cdata;
      THIndexTensor* arg_indices = ((THPIndexTensor*)indices)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_k = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_k));
      bool arg_largest = (__kw_largest == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        int64_t __last_dim = THTensor_(nDimension)(LIBRARY_STATE ((THPTensor*)self)->cdata)-1;
        Py_UNBLOCK_THREADS;
        THTensor_(topk)(LIBRARY_STATE arg_values, arg_indices, arg_self, arg_k, __last_dim, arg_largest, true);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, values, indices);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_k) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_k)) &&
          __kw_sorted && PyBool_Check(__kw_sorted)) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _values_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_values_guard.get()) return NULL;
      THPTensor* values = _values_guard.get();
      
      #if IS_CUDA
      THCPLongTensorPtr _indices_guard((THCPLongTensor*) THCPLongTensor_NewEmpty());
      if (!_indices_guard.get()) return NULL;
      THCPLongTensor* indices = _indices_guard.get();
      
      #else
      THPLongTensorPtr _indices_guard((THPLongTensor*) THPLongTensor_NewEmpty());
      if (!_indices_guard.get()) return NULL;
      THPLongTensor* indices = _indices_guard.get();
      
      #endif
      
      
      THTensor* arg_values = ((THPTensor*)values)->cdata;
      THIndexTensor* arg_indices = ((THPIndexTensor*)indices)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_k = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_k));
      bool arg_sorted = (__kw_sorted == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        int64_t __last_dim = THTensor_(nDimension)(LIBRARY_STATE ((THPTensor*)self)->cdata)-1;
        Py_UNBLOCK_THREADS;
        THTensor_(topk)(LIBRARY_STATE arg_values, arg_indices, arg_self, arg_k, __last_dim, true, arg_sorted);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, values, indices);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_k) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_k)) &&
          (__tuplecount > 1 || __kw_dim) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _values_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_values_guard.get()) return NULL;
      THPTensor* values = _values_guard.get();
      
      #if IS_CUDA
      THCPLongTensorPtr _indices_guard((THCPLongTensor*) THCPLongTensor_NewEmpty());
      if (!_indices_guard.get()) return NULL;
      THCPLongTensor* indices = _indices_guard.get();
      
      #else
      THPLongTensorPtr _indices_guard((THPLongTensor*) THPLongTensor_NewEmpty());
      if (!_indices_guard.get()) return NULL;
      THPLongTensor* indices = _indices_guard.get();
      
      #endif
      
      
      THTensor* arg_values = ((THPTensor*)values)->cdata;
      THIndexTensor* arg_indices = ((THPIndexTensor*)indices)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_k = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_k));
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim));
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(topk)(LIBRARY_STATE arg_values, arg_indices, arg_self, arg_k, arg_dim, true, true);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, values, indices);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_k) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_k))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _values_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_values_guard.get()) return NULL;
      THPTensor* values = _values_guard.get();
      
      #if IS_CUDA
      THCPLongTensorPtr _indices_guard((THCPLongTensor*) THCPLongTensor_NewEmpty());
      if (!_indices_guard.get()) return NULL;
      THCPLongTensor* indices = _indices_guard.get();
      
      #else
      THPLongTensorPtr _indices_guard((THPLongTensor*) THPLongTensor_NewEmpty());
      if (!_indices_guard.get()) return NULL;
      THPLongTensor* indices = _indices_guard.get();
      
      #endif
      
      
      THTensor* arg_values = ((THPTensor*)values)->cdata;
      THIndexTensor* arg_indices = ((THPIndexTensor*)indices)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_k = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_k));
      
      PyThreadState *_save = NULL;
      try {
        int64_t __last_dim = THTensor_(nDimension)(LIBRARY_STATE ((THPTensor*)self)->cdata)-1;
        Py_UNBLOCK_THREADS;
        THTensor_(topk)(LIBRARY_STATE arg_values, arg_indices, arg_self, arg_k, __last_dim, true, true);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, values, indices);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "topk", 8, "(int k, #tuple[" THPTensorStr ", " THPModuleStr "LongTensor] out)", "(int k, int dim, #tuple[" THPTensorStr ", " THPModuleStr "LongTensor] out)", "(int k, bool sorted, #tuple[" THPTensorStr ", " THPModuleStr "LongTensor] out)", "(int k, bool largest, #tuple[" THPTensorStr ", " THPModuleStr "LongTensor] out)", "(int k, int dim, bool sorted, #tuple[" THPTensorStr ", " THPModuleStr "LongTensor] out)", "(int k, int dim, bool largest, #tuple[" THPTensorStr ", " THPModuleStr "LongTensor] out)", "(int k, bool largest, bool sorted, #tuple[" THPTensorStr ", " THPModuleStr "LongTensor] out)", "(int k, int dim, bool largest, bool sorted, #tuple[" THPTensorStr ", " THPModuleStr "LongTensor] out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF)
PyObject * THPTensor_stateless_(topk)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    PyObject *__kw_k = NULL;
    PyObject *__kw_dim = NULL;
    PyObject *__kw_largest = NULL;
    PyObject *__kw_sorted = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
      __kw_k = PyDict_GetItemString(kwargs, "k");
      __kw_dim = PyDict_GetItemString(kwargs, "dim");
      __kw_largest = PyDict_GetItemString(kwargs, "largest");
      __kw_sorted = PyDict_GetItemString(kwargs, "sorted");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 6 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPIndexTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_k) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_k)) &&
          (__tuplecount > 2 || __kw_dim) && THPUtils_checkLong((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_dim)) &&
          (__tuplecount > 3 || __kw_largest) && PyBool_Check((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_largest)) &&
          (__tuplecount > 4 || __kw_sorted) && PyBool_Check((__tuplecount > 4 ? PyTuple_GET_ITEM(args, 4) : __kw_sorted))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_values = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THIndexTensor* arg_indices = ((THPIndexTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int64_t arg_k = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_k));
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_dim));
      bool arg_largest = ((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_largest) == Py_True ? true : false);
      bool arg_sorted = ((__tuplecount > 4 ? PyTuple_GET_ITEM(args, 4) : __kw_sorted) == Py_True ? true : false);
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(topk)(LIBRARY_STATE arg_values, arg_indices, arg_self, arg_k, arg_dim, arg_largest, arg_sorted);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 5 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPIndexTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_k) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_k)) &&
          __kw_largest && PyBool_Check(__kw_largest) &&
          __kw_sorted && PyBool_Check(__kw_sorted)) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_values = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THIndexTensor* arg_indices = ((THPIndexTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int64_t arg_k = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_k));
      bool arg_largest = (__kw_largest == Py_True ? true : false);
      bool arg_sorted = (__kw_sorted == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        int64_t __last_dim = THTensor_(nDimension)(LIBRARY_STATE ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata)-1;
        Py_UNBLOCK_THREADS;
        THTensor_(topk)(LIBRARY_STATE arg_values, arg_indices, arg_self, arg_k, __last_dim, arg_largest, arg_sorted);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 5 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPIndexTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_k) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_k)) &&
          (__tuplecount > 2 || __kw_dim) && THPUtils_checkLong((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_dim)) &&
          (__tuplecount > 3 || __kw_largest) && PyBool_Check((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_largest))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_values = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THIndexTensor* arg_indices = ((THPIndexTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int64_t arg_k = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_k));
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_dim));
      bool arg_largest = ((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_largest) == Py_True ? true : false);
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(topk)(LIBRARY_STATE arg_values, arg_indices, arg_self, arg_k, arg_dim, arg_largest, true);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 5 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPIndexTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_k) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_k)) &&
          (__tuplecount > 2 || __kw_dim) && THPUtils_checkLong((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_dim)) &&
          __kw_sorted && PyBool_Check(__kw_sorted)) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_values = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THIndexTensor* arg_indices = ((THPIndexTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int64_t arg_k = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_k));
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_dim));
      bool arg_sorted = (__kw_sorted == Py_True ? true : false);
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(topk)(LIBRARY_STATE arg_values, arg_indices, arg_self, arg_k, arg_dim, true, arg_sorted);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 4 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPIndexTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_k) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_k)) &&
          __kw_largest && PyBool_Check(__kw_largest)) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_values = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THIndexTensor* arg_indices = ((THPIndexTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int64_t arg_k = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_k));
      bool arg_largest = (__kw_largest == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        int64_t __last_dim = THTensor_(nDimension)(LIBRARY_STATE ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata)-1;
        Py_UNBLOCK_THREADS;
        THTensor_(topk)(LIBRARY_STATE arg_values, arg_indices, arg_self, arg_k, __last_dim, arg_largest, true);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 4 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPIndexTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_k) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_k)) &&
          __kw_sorted && PyBool_Check(__kw_sorted)) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_values = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THIndexTensor* arg_indices = ((THPIndexTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int64_t arg_k = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_k));
      bool arg_sorted = (__kw_sorted == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        int64_t __last_dim = THTensor_(nDimension)(LIBRARY_STATE ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata)-1;
        Py_UNBLOCK_THREADS;
        THTensor_(topk)(LIBRARY_STATE arg_values, arg_indices, arg_self, arg_k, __last_dim, true, arg_sorted);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 5 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_k) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_k)) &&
          (__tuplecount > 2 || __kw_dim) && THPUtils_checkLong((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_dim)) &&
          (__tuplecount > 3 || __kw_largest) && PyBool_Check((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_largest)) &&
          (__tuplecount > 4 || __kw_sorted) && PyBool_Check((__tuplecount > 4 ? PyTuple_GET_ITEM(args, 4) : __kw_sorted))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _values_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_values_guard.get()) return NULL;
      THPTensor* values = _values_guard.get();
      
      #if IS_CUDA
      THCPLongTensorPtr _indices_guard((THCPLongTensor*) THCPLongTensor_NewEmpty());
      if (!_indices_guard.get()) return NULL;
      THCPLongTensor* indices = _indices_guard.get();
      
      #else
      THPLongTensorPtr _indices_guard((THPLongTensor*) THPLongTensor_NewEmpty());
      if (!_indices_guard.get()) return NULL;
      THPLongTensor* indices = _indices_guard.get();
      
      #endif
      
      
      THTensor* arg_values = ((THPTensor*)values)->cdata;
      THIndexTensor* arg_indices = ((THPIndexTensor*)indices)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int64_t arg_k = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_k));
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_dim));
      bool arg_largest = ((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_largest) == Py_True ? true : false);
      bool arg_sorted = ((__tuplecount > 4 ? PyTuple_GET_ITEM(args, 4) : __kw_sorted) == Py_True ? true : false);
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(topk)(LIBRARY_STATE arg_values, arg_indices, arg_self, arg_k, arg_dim, arg_largest, arg_sorted);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, values, indices);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 4 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPIndexTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_k) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_k)) &&
          (__tuplecount > 2 || __kw_dim) && THPUtils_checkLong((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_dim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_values = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THIndexTensor* arg_indices = ((THPIndexTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int64_t arg_k = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_k));
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_dim));
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(topk)(LIBRARY_STATE arg_values, arg_indices, arg_self, arg_k, arg_dim, true, true);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 4 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_k) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_k)) &&
          __kw_largest && PyBool_Check(__kw_largest) &&
          __kw_sorted && PyBool_Check(__kw_sorted)) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _values_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_values_guard.get()) return NULL;
      THPTensor* values = _values_guard.get();
      
      #if IS_CUDA
      THCPLongTensorPtr _indices_guard((THCPLongTensor*) THCPLongTensor_NewEmpty());
      if (!_indices_guard.get()) return NULL;
      THCPLongTensor* indices = _indices_guard.get();
      
      #else
      THPLongTensorPtr _indices_guard((THPLongTensor*) THPLongTensor_NewEmpty());
      if (!_indices_guard.get()) return NULL;
      THPLongTensor* indices = _indices_guard.get();
      
      #endif
      
      
      THTensor* arg_values = ((THPTensor*)values)->cdata;
      THIndexTensor* arg_indices = ((THPIndexTensor*)indices)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int64_t arg_k = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_k));
      bool arg_largest = (__kw_largest == Py_True ? true : false);
      bool arg_sorted = (__kw_sorted == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        int64_t __last_dim = THTensor_(nDimension)(LIBRARY_STATE ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata)-1;
        Py_UNBLOCK_THREADS;
        THTensor_(topk)(LIBRARY_STATE arg_values, arg_indices, arg_self, arg_k, __last_dim, arg_largest, arg_sorted);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, values, indices);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          PyTuple_Check(___out) &&
          PyTuple_GET_SIZE(___out) == 2 &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 0)) == THPTensorClass &&
          (PyObject*)Py_TYPE(PyTuple_GET_ITEM(___out, 1)) == THPIndexTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_k) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_k))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_values = ((THPTensor*)PyTuple_GET_ITEM(___out, 0))->cdata;
      THIndexTensor* arg_indices = ((THPIndexTensor*)PyTuple_GET_ITEM(___out, 1))->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int64_t arg_k = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_k));
      
      PyThreadState *_save = NULL;
      try {
        int64_t __last_dim = THTensor_(nDimension)(LIBRARY_STATE ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata)-1;
        Py_UNBLOCK_THREADS;
        THTensor_(topk)(LIBRARY_STATE arg_values, arg_indices, arg_self, arg_k, __last_dim, true, true);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, PyTuple_GET_ITEM(___out, 0), PyTuple_GET_ITEM(___out, 1));
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 4 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_k) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_k)) &&
          (__tuplecount > 2 || __kw_dim) && THPUtils_checkLong((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_dim)) &&
          (__tuplecount > 3 || __kw_largest) && PyBool_Check((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_largest))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _values_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_values_guard.get()) return NULL;
      THPTensor* values = _values_guard.get();
      
      #if IS_CUDA
      THCPLongTensorPtr _indices_guard((THCPLongTensor*) THCPLongTensor_NewEmpty());
      if (!_indices_guard.get()) return NULL;
      THCPLongTensor* indices = _indices_guard.get();
      
      #else
      THPLongTensorPtr _indices_guard((THPLongTensor*) THPLongTensor_NewEmpty());
      if (!_indices_guard.get()) return NULL;
      THPLongTensor* indices = _indices_guard.get();
      
      #endif
      
      
      THTensor* arg_values = ((THPTensor*)values)->cdata;
      THIndexTensor* arg_indices = ((THPIndexTensor*)indices)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int64_t arg_k = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_k));
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_dim));
      bool arg_largest = ((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_largest) == Py_True ? true : false);
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(topk)(LIBRARY_STATE arg_values, arg_indices, arg_self, arg_k, arg_dim, arg_largest, true);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, values, indices);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 4 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_k) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_k)) &&
          (__tuplecount > 2 || __kw_dim) && THPUtils_checkLong((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_dim)) &&
          __kw_sorted && PyBool_Check(__kw_sorted)) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _values_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_values_guard.get()) return NULL;
      THPTensor* values = _values_guard.get();
      
      #if IS_CUDA
      THCPLongTensorPtr _indices_guard((THCPLongTensor*) THCPLongTensor_NewEmpty());
      if (!_indices_guard.get()) return NULL;
      THCPLongTensor* indices = _indices_guard.get();
      
      #else
      THPLongTensorPtr _indices_guard((THPLongTensor*) THPLongTensor_NewEmpty());
      if (!_indices_guard.get()) return NULL;
      THPLongTensor* indices = _indices_guard.get();
      
      #endif
      
      
      THTensor* arg_values = ((THPTensor*)values)->cdata;
      THIndexTensor* arg_indices = ((THPIndexTensor*)indices)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int64_t arg_k = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_k));
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_dim));
      bool arg_sorted = (__kw_sorted == Py_True ? true : false);
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(topk)(LIBRARY_STATE arg_values, arg_indices, arg_self, arg_k, arg_dim, true, arg_sorted);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, values, indices);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 3 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_k) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_k)) &&
          __kw_largest && PyBool_Check(__kw_largest)) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _values_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_values_guard.get()) return NULL;
      THPTensor* values = _values_guard.get();
      
      #if IS_CUDA
      THCPLongTensorPtr _indices_guard((THCPLongTensor*) THCPLongTensor_NewEmpty());
      if (!_indices_guard.get()) return NULL;
      THCPLongTensor* indices = _indices_guard.get();
      
      #else
      THPLongTensorPtr _indices_guard((THPLongTensor*) THPLongTensor_NewEmpty());
      if (!_indices_guard.get()) return NULL;
      THPLongTensor* indices = _indices_guard.get();
      
      #endif
      
      
      THTensor* arg_values = ((THPTensor*)values)->cdata;
      THIndexTensor* arg_indices = ((THPIndexTensor*)indices)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int64_t arg_k = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_k));
      bool arg_largest = (__kw_largest == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        int64_t __last_dim = THTensor_(nDimension)(LIBRARY_STATE ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata)-1;
        Py_UNBLOCK_THREADS;
        THTensor_(topk)(LIBRARY_STATE arg_values, arg_indices, arg_self, arg_k, __last_dim, arg_largest, true);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, values, indices);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 3 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_k) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_k)) &&
          __kw_sorted && PyBool_Check(__kw_sorted)) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _values_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_values_guard.get()) return NULL;
      THPTensor* values = _values_guard.get();
      
      #if IS_CUDA
      THCPLongTensorPtr _indices_guard((THCPLongTensor*) THCPLongTensor_NewEmpty());
      if (!_indices_guard.get()) return NULL;
      THCPLongTensor* indices = _indices_guard.get();
      
      #else
      THPLongTensorPtr _indices_guard((THPLongTensor*) THPLongTensor_NewEmpty());
      if (!_indices_guard.get()) return NULL;
      THPLongTensor* indices = _indices_guard.get();
      
      #endif
      
      
      THTensor* arg_values = ((THPTensor*)values)->cdata;
      THIndexTensor* arg_indices = ((THPIndexTensor*)indices)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int64_t arg_k = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_k));
      bool arg_sorted = (__kw_sorted == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        int64_t __last_dim = THTensor_(nDimension)(LIBRARY_STATE ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata)-1;
        Py_UNBLOCK_THREADS;
        THTensor_(topk)(LIBRARY_STATE arg_values, arg_indices, arg_self, arg_k, __last_dim, true, arg_sorted);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, values, indices);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 3 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_k) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_k)) &&
          (__tuplecount > 2 || __kw_dim) && THPUtils_checkLong((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_dim))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _values_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_values_guard.get()) return NULL;
      THPTensor* values = _values_guard.get();
      
      #if IS_CUDA
      THCPLongTensorPtr _indices_guard((THCPLongTensor*) THCPLongTensor_NewEmpty());
      if (!_indices_guard.get()) return NULL;
      THCPLongTensor* indices = _indices_guard.get();
      
      #else
      THPLongTensorPtr _indices_guard((THPLongTensor*) THPLongTensor_NewEmpty());
      if (!_indices_guard.get()) return NULL;
      THPLongTensor* indices = _indices_guard.get();
      
      #endif
      
      
      THTensor* arg_values = ((THPTensor*)values)->cdata;
      THIndexTensor* arg_indices = ((THPIndexTensor*)indices)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int64_t arg_k = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_k));
      int64_t arg_dim = THPUtils_unpackLong((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_dim));
      
      THPUtils_assert(arg_self->nDimension > 0,
          "dimension specified as %d, but tensor has no dimensions", arg_dim);
      THPUtils_assert(arg_dim >= -(arg_self->nDimension) && arg_dim < (arg_self->nDimension),
          "dimension out of range (expected to be in range of [%d, %d], but got %d)",
          -(arg_self->nDimension), (arg_self->nDimension)-1, arg_dim);
      if (arg_dim < 0) arg_dim += (arg_self->nDimension);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(topk)(LIBRARY_STATE arg_values, arg_indices, arg_self, arg_k, arg_dim, true, true);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, values, indices);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_k) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_k))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _values_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_values_guard.get()) return NULL;
      THPTensor* values = _values_guard.get();
      
      #if IS_CUDA
      THCPLongTensorPtr _indices_guard((THCPLongTensor*) THCPLongTensor_NewEmpty());
      if (!_indices_guard.get()) return NULL;
      THCPLongTensor* indices = _indices_guard.get();
      
      #else
      THPLongTensorPtr _indices_guard((THPLongTensor*) THPLongTensor_NewEmpty());
      if (!_indices_guard.get()) return NULL;
      THPLongTensor* indices = _indices_guard.get();
      
      #endif
      
      
      THTensor* arg_values = ((THPTensor*)values)->cdata;
      THIndexTensor* arg_indices = ((THPIndexTensor*)indices)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int64_t arg_k = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_k));
      
      PyThreadState *_save = NULL;
      try {
        int64_t __last_dim = THTensor_(nDimension)(LIBRARY_STATE ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata)-1;
        Py_UNBLOCK_THREADS;
        THTensor_(topk)(LIBRARY_STATE arg_values, arg_indices, arg_self, arg_k, __last_dim, true, true);
        Py_BLOCK_THREADS;
        return PyTuple_Pack(2, values, indices);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.topk", 8, "(" THPTensorStr " source, int k, #tuple[" THPTensorStr ", " THPModuleStr "LongTensor] out)", "(" THPTensorStr " source, int k, int dim, #tuple[" THPTensorStr ", " THPModuleStr "LongTensor] out)", "(" THPTensorStr " source, int k, bool sorted, #tuple[" THPTensorStr ", " THPModuleStr "LongTensor] out)", "(" THPTensorStr " source, int k, bool largest, #tuple[" THPTensorStr ", " THPModuleStr "LongTensor] out)", "(" THPTensorStr " source, int k, int dim, bool sorted, #tuple[" THPTensorStr ", " THPModuleStr "LongTensor] out)", "(" THPTensorStr " source, int k, int dim, bool largest, #tuple[" THPTensorStr ", " THPModuleStr "LongTensor] out)", "(" THPTensorStr " source, int k, bool largest, bool sorted, #tuple[" THPTensorStr ", " THPModuleStr "LongTensor] out)", "(" THPTensorStr " source, int k, int dim, bool largest, bool sorted, #tuple[" THPTensorStr ", " THPModuleStr "LongTensor] out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_BYTE) || CUDA_BYTE)
PyObject * THPTensor_(all)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    
    if (kwargs) {
      
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        bool __result = THTensor_(logicalall)(LIBRARY_STATE arg_self);
        Py_BLOCK_THREADS;
        return PyBool_FromLong(__result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "all", 1, "no arguments");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_BYTE) || CUDA_BYTE)
PyObject * THPTensor_(any)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    
    if (kwargs) {
      
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        bool __result = THTensor_(logicalany)(LIBRARY_STATE arg_self);
        Py_BLOCK_THREADS;
        return PyBool_FromLong(__result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "any", 1, "no arguments");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF) && (!IS_DISTRIBUTED && (!IS_CUDA))
PyObject * THPTensor_stateless_(randperm)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_generator = NULL;
    PyObject *__kw_n = NULL;
    if (kwargs) {
      __kw_generator = PyDict_GetItemString(kwargs, "generator");
      __kw_n = PyDict_GetItemString(kwargs, "n");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    #if !IS_DISTRIBUTED
          
    if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          __kw_generator && (PyObject*)Py_TYPE(__kw_generator) == THPGeneratorClass &&
          (__tuplecount > 0 || __kw_n) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_n))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THGenerator* arg_generator = THPGenerator_TH_CData(__kw_generator);
      int64_t arg_n = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_n));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(randperm)(LIBRARY_STATE arg_result, arg_generator, arg_n);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #else
    if (false) {
    #endif
#if !IS_DISTRIBUTED
          
    } else if (___out == NULL &&
          __argcount == 2 &&
          __kw_generator && (PyObject*)Py_TYPE(__kw_generator) == THPGeneratorClass &&
          (__tuplecount > 0 || __kw_n) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_n))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THGenerator* arg_generator = THPGenerator_TH_CData(__kw_generator);
      int64_t arg_n = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_n));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(randperm)(LIBRARY_STATE arg_result, arg_generator, arg_n);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif
#if !IS_DISTRIBUTED
          
    } else if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_n) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_n))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      int64_t arg_n = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_n));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(randperm)(LIBRARY_STATE arg_result, THPGenerator_TH_CData(THPDefaultGenerator), arg_n);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif
#if !IS_DISTRIBUTED
          
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_n) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_n))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      int64_t arg_n = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_n));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(randperm)(LIBRARY_STATE arg_result, THPGenerator_TH_CData(THPDefaultGenerator), arg_n);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif

    }

    THPUtils_invalidArguments(args, kwargs, "torch.randperm", 2, "(int n, #" THPTensorStr " out)", "(torch.Generator generator, int n, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF) && (!IS_DISTRIBUTED && (!IS_CUDA))
PyObject * THPTensor_(random_)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_generator = NULL;
    PyObject *__kw_from = NULL;
    PyObject *__kw_to = NULL;
    if (kwargs) {
      __kw_generator = PyDict_GetItemString(kwargs, "generator");
      __kw_from = PyDict_GetItemString(kwargs, "from");
      __kw_to = PyDict_GetItemString(kwargs, "to");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    #if !IS_DISTRIBUTED
          
    if (__argcount == 3 &&
          __kw_generator && (PyObject*)Py_TYPE(__kw_generator) == THPGeneratorClass &&
          (__tuplecount > 0 || __kw_from) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_from)) &&
          (__tuplecount > 1 || __kw_to) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_to))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THGenerator* arg_generator = THPGenerator_TH_CData(__kw_generator);
      int64_t arg_from = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_from));
      int64_t arg_to = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_to));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(clampedRandom)(LIBRARY_STATE arg_self, arg_generator, arg_from, arg_to);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #else
    if (false) {
    #endif
#if !IS_DISTRIBUTED
          
    } else if (__argcount == 2 &&
          __kw_generator && (PyObject*)Py_TYPE(__kw_generator) == THPGeneratorClass &&
          (__tuplecount > 0 || __kw_to) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_to))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THGenerator* arg_generator = THPGenerator_TH_CData(__kw_generator);
      int64_t arg_to = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_to));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cappedRandom)(LIBRARY_STATE arg_self, arg_generator, arg_to);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif
#if !IS_DISTRIBUTED
          
    } else if (__argcount == 2 &&
          (__tuplecount > 0 || __kw_from) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_from)) &&
          (__tuplecount > 1 || __kw_to) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_to))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_from = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_from));
      int64_t arg_to = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_to));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(clampedRandom)(LIBRARY_STATE arg_self, THPGenerator_TH_CData(THPDefaultGenerator), arg_from, arg_to);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif
#if !IS_DISTRIBUTED
          
    } else if (__argcount == 1 &&
          __kw_generator && (PyObject*)Py_TYPE(__kw_generator) == THPGeneratorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THGenerator* arg_generator = THPGenerator_TH_CData(__kw_generator);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(random)(LIBRARY_STATE arg_self, arg_generator);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif
#if !IS_DISTRIBUTED
          
    } else if (__argcount == 1 &&
          (__tuplecount > 0 || __kw_to) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_to))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_to = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_to));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cappedRandom)(LIBRARY_STATE arg_self, THPGenerator_TH_CData(THPDefaultGenerator), arg_to);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif
#if !IS_DISTRIBUTED
          
    } else if (__argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(random)(LIBRARY_STATE arg_self, THPGenerator_TH_CData(THPDefaultGenerator));
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif

    }

    THPUtils_invalidArguments(args, kwargs, "random_", 6, "(int to)", "no arguments", "(int from, int to)", "(torch.Generator generator)", "(torch.Generator generator, int to)", "(torch.Generator generator, int from, int to)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF) && (!IS_DISTRIBUTED && (IS_CUDA))
PyObject * THPTensor_(random_)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_from = NULL;
    PyObject *__kw_to = NULL;
    if (kwargs) {
      __kw_from = PyDict_GetItemString(kwargs, "from");
      __kw_to = PyDict_GetItemString(kwargs, "to");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    #if !IS_DISTRIBUTED
          
    if (__argcount == 2 &&
          (__tuplecount > 0 || __kw_from) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_from)) &&
          (__tuplecount > 1 || __kw_to) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_to))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_from = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_from));
      int64_t arg_to = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_to));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(clampedRandom)(LIBRARY_STATE arg_self, arg_from, arg_to);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #else
    if (false) {
    #endif
#if !IS_DISTRIBUTED
          
    } else if (__argcount == 2 &&
          (__tuplecount > 0 || __kw_from) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_from)) &&
          (__tuplecount > 1 || __kw_to) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_to))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_from = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_from));
      int64_t arg_to = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_to));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(clampedRandom)(LIBRARY_STATE arg_self, arg_from, arg_to);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif
#if !IS_DISTRIBUTED
          
    } else if (__argcount == 1 &&
          (__tuplecount > 0 || __kw_to) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_to))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_to = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_to));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cappedRandom)(LIBRARY_STATE arg_self, arg_to);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif
#if !IS_DISTRIBUTED
          
    } else if (__argcount == 1 &&
          (__tuplecount > 0 || __kw_to) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_to))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int64_t arg_to = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_to));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cappedRandom)(LIBRARY_STATE arg_self, arg_to);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif
#if !IS_DISTRIBUTED
          
    } else if (__argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(random)(LIBRARY_STATE arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif
#if !IS_DISTRIBUTED
          
    } else if (__argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(random)(LIBRARY_STATE arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif

    }

    THPUtils_invalidArguments(args, kwargs, "random_", 3, "(int to)", "no arguments", "(int from, int to)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT))
PyObject * THPTensor_(multinomial)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_generator = NULL;
    PyObject *__kw_num_samples = NULL;
    PyObject *__kw_replacement = NULL;
    if (kwargs) {
      __kw_generator = PyDict_GetItemString(kwargs, "generator");
      __kw_num_samples = PyDict_GetItemString(kwargs, "num_samples");
      __kw_replacement = PyDict_GetItemString(kwargs, "replacement");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 4 &&
          (PyObject*)Py_TYPE(___out) == THPIndexTensorClass &&
          __kw_generator && (PyObject*)Py_TYPE(__kw_generator) == THPGeneratorClass &&
          (__tuplecount > 0 || __kw_num_samples) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_num_samples)) &&
          (__tuplecount > 1 || __kw_replacement) && PyBool_Check((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_replacement))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THIndexTensor* arg_result = ((THPIndexTensor*)___out)->cdata;
      THGenerator* arg_generator = THPGenerator_TH_CData(__kw_generator);
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int arg_num_samples = ((int) THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_num_samples)));
      bool arg_replacement = ((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_replacement) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(multinomial)(LIBRARY_STATE arg_result, arg_generator, arg_self, arg_num_samples, arg_replacement);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 3 &&
          __kw_generator && (PyObject*)Py_TYPE(__kw_generator) == THPGeneratorClass &&
          (__tuplecount > 0 || __kw_num_samples) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_num_samples)) &&
          (__tuplecount > 1 || __kw_replacement) && PyBool_Check((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_replacement))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      #if IS_CUDA
      THCPLongTensorPtr _result_guard((THCPLongTensor*) THCPLongTensor_NewEmpty());
      if (!_result_guard.get()) return NULL;
      THCPLongTensor* result = _result_guard.get();
      
      #else
      THPLongTensorPtr _result_guard((THPLongTensor*) THPLongTensor_NewEmpty());
      if (!_result_guard.get()) return NULL;
      THPLongTensor* result = _result_guard.get();
      
      #endif
      
      
      THIndexTensor* arg_result = ((THPIndexTensor*)result)->cdata;
      THGenerator* arg_generator = THPGenerator_TH_CData(__kw_generator);
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int arg_num_samples = ((int) THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_num_samples)));
      bool arg_replacement = ((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_replacement) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(multinomial)(LIBRARY_STATE arg_result, arg_generator, arg_self, arg_num_samples, arg_replacement);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPIndexTensorClass &&
          __kw_generator && (PyObject*)Py_TYPE(__kw_generator) == THPGeneratorClass &&
          (__tuplecount > 0 || __kw_num_samples) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_num_samples))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THIndexTensor* arg_result = ((THPIndexTensor*)___out)->cdata;
      THGenerator* arg_generator = THPGenerator_TH_CData(__kw_generator);
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int arg_num_samples = ((int) THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_num_samples)));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(multinomial)(LIBRARY_STATE arg_result, arg_generator, arg_self, arg_num_samples, false);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPIndexTensorClass &&
          (__tuplecount > 0 || __kw_num_samples) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_num_samples)) &&
          (__tuplecount > 1 || __kw_replacement) && PyBool_Check((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_replacement))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THIndexTensor* arg_result = ((THPIndexTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int arg_num_samples = ((int) THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_num_samples)));
      bool arg_replacement = ((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_replacement) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(multinomial)(LIBRARY_STATE arg_result, THPGenerator_TH_CData(THPDefaultGenerator), arg_self, arg_num_samples, arg_replacement);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          __kw_generator && (PyObject*)Py_TYPE(__kw_generator) == THPGeneratorClass &&
          (__tuplecount > 0 || __kw_num_samples) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_num_samples))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      #if IS_CUDA
      THCPLongTensorPtr _result_guard((THCPLongTensor*) THCPLongTensor_NewEmpty());
      if (!_result_guard.get()) return NULL;
      THCPLongTensor* result = _result_guard.get();
      
      #else
      THPLongTensorPtr _result_guard((THPLongTensor*) THPLongTensor_NewEmpty());
      if (!_result_guard.get()) return NULL;
      THPLongTensor* result = _result_guard.get();
      
      #endif
      
      
      THIndexTensor* arg_result = ((THPIndexTensor*)result)->cdata;
      THGenerator* arg_generator = THPGenerator_TH_CData(__kw_generator);
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int arg_num_samples = ((int) THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_num_samples)));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(multinomial)(LIBRARY_STATE arg_result, arg_generator, arg_self, arg_num_samples, false);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_num_samples) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_num_samples)) &&
          (__tuplecount > 1 || __kw_replacement) && PyBool_Check((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_replacement))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      #if IS_CUDA
      THCPLongTensorPtr _result_guard((THCPLongTensor*) THCPLongTensor_NewEmpty());
      if (!_result_guard.get()) return NULL;
      THCPLongTensor* result = _result_guard.get();
      
      #else
      THPLongTensorPtr _result_guard((THPLongTensor*) THPLongTensor_NewEmpty());
      if (!_result_guard.get()) return NULL;
      THPLongTensor* result = _result_guard.get();
      
      #endif
      
      
      THIndexTensor* arg_result = ((THPIndexTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int arg_num_samples = ((int) THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_num_samples)));
      bool arg_replacement = ((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_replacement) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(multinomial)(LIBRARY_STATE arg_result, THPGenerator_TH_CData(THPDefaultGenerator), arg_self, arg_num_samples, arg_replacement);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPIndexTensorClass &&
          (__tuplecount > 0 || __kw_num_samples) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_num_samples))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THIndexTensor* arg_result = ((THPIndexTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int arg_num_samples = ((int) THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_num_samples)));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(multinomial)(LIBRARY_STATE arg_result, THPGenerator_TH_CData(THPDefaultGenerator), arg_self, arg_num_samples, false);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_num_samples) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_num_samples))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      #if IS_CUDA
      THCPLongTensorPtr _result_guard((THCPLongTensor*) THCPLongTensor_NewEmpty());
      if (!_result_guard.get()) return NULL;
      THCPLongTensor* result = _result_guard.get();
      
      #else
      THPLongTensorPtr _result_guard((THPLongTensor*) THPLongTensor_NewEmpty());
      if (!_result_guard.get()) return NULL;
      THPLongTensor* result = _result_guard.get();
      
      #endif
      
      
      THIndexTensor* arg_result = ((THPIndexTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int arg_num_samples = ((int) THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_num_samples)));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(multinomial)(LIBRARY_STATE arg_result, THPGenerator_TH_CData(THPDefaultGenerator), arg_self, arg_num_samples, false);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "multinomial", 4, "(int num_samples, #" THPModuleStr "LongTensor out)", "(int num_samples, bool replacement, #" THPModuleStr "LongTensor out)", "(torch.Generator generator, int num_samples, #" THPModuleStr "LongTensor out)", "(torch.Generator generator, int num_samples, bool replacement, #" THPModuleStr "LongTensor out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF) && (CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_(multinomial)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_num_samples = NULL;
    PyObject *__kw_replacement = NULL;
    if (kwargs) {
      __kw_num_samples = PyDict_GetItemString(kwargs, "num_samples");
      __kw_replacement = PyDict_GetItemString(kwargs, "replacement");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPIndexTensorClass &&
          (__tuplecount > 0 || __kw_num_samples) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_num_samples)) &&
          (__tuplecount > 1 || __kw_replacement) && PyBool_Check((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_replacement))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THIndexTensor* arg_result = ((THPIndexTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int arg_num_samples = ((int) THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_num_samples)));
      bool arg_replacement = ((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_replacement) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(multinomial)(LIBRARY_STATE arg_result, arg_self, arg_num_samples, arg_replacement);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPIndexTensorClass &&
          (__tuplecount > 0 || __kw_num_samples) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_num_samples)) &&
          (__tuplecount > 1 || __kw_replacement) && PyBool_Check((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_replacement))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THIndexTensor* arg_result = ((THPIndexTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int arg_num_samples = ((int) THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_num_samples)));
      bool arg_replacement = ((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_replacement) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(multinomial)(LIBRARY_STATE arg_result, arg_self, arg_num_samples, arg_replacement);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_num_samples) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_num_samples)) &&
          (__tuplecount > 1 || __kw_replacement) && PyBool_Check((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_replacement))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      #if IS_CUDA
      THCPLongTensorPtr _result_guard((THCPLongTensor*) THCPLongTensor_NewEmpty());
      if (!_result_guard.get()) return NULL;
      THCPLongTensor* result = _result_guard.get();
      
      #else
      THPLongTensorPtr _result_guard((THPLongTensor*) THPLongTensor_NewEmpty());
      if (!_result_guard.get()) return NULL;
      THPLongTensor* result = _result_guard.get();
      
      #endif
      
      
      THIndexTensor* arg_result = ((THPIndexTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int arg_num_samples = ((int) THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_num_samples)));
      bool arg_replacement = ((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_replacement) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(multinomial)(LIBRARY_STATE arg_result, arg_self, arg_num_samples, arg_replacement);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPIndexTensorClass &&
          (__tuplecount > 0 || __kw_num_samples) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_num_samples))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THIndexTensor* arg_result = ((THPIndexTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int arg_num_samples = ((int) THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_num_samples)));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(multinomial)(LIBRARY_STATE arg_result, arg_self, arg_num_samples, false);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_num_samples) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_num_samples)) &&
          (__tuplecount > 1 || __kw_replacement) && PyBool_Check((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_replacement))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      #if IS_CUDA
      THCPLongTensorPtr _result_guard((THCPLongTensor*) THCPLongTensor_NewEmpty());
      if (!_result_guard.get()) return NULL;
      THCPLongTensor* result = _result_guard.get();
      
      #else
      THPLongTensorPtr _result_guard((THPLongTensor*) THPLongTensor_NewEmpty());
      if (!_result_guard.get()) return NULL;
      THPLongTensor* result = _result_guard.get();
      
      #endif
      
      
      THIndexTensor* arg_result = ((THPIndexTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int arg_num_samples = ((int) THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_num_samples)));
      bool arg_replacement = ((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_replacement) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(multinomial)(LIBRARY_STATE arg_result, arg_self, arg_num_samples, arg_replacement);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPIndexTensorClass &&
          (__tuplecount > 0 || __kw_num_samples) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_num_samples))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THIndexTensor* arg_result = ((THPIndexTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int arg_num_samples = ((int) THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_num_samples)));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(multinomial)(LIBRARY_STATE arg_result, arg_self, arg_num_samples, false);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_num_samples) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_num_samples))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      #if IS_CUDA
      THCPLongTensorPtr _result_guard((THCPLongTensor*) THCPLongTensor_NewEmpty());
      if (!_result_guard.get()) return NULL;
      THCPLongTensor* result = _result_guard.get();
      
      #else
      THPLongTensorPtr _result_guard((THPLongTensor*) THPLongTensor_NewEmpty());
      if (!_result_guard.get()) return NULL;
      THPLongTensor* result = _result_guard.get();
      
      #endif
      
      
      THIndexTensor* arg_result = ((THPIndexTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int arg_num_samples = ((int) THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_num_samples)));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(multinomial)(LIBRARY_STATE arg_result, arg_self, arg_num_samples, false);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_num_samples) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_num_samples))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      #if IS_CUDA
      THCPLongTensorPtr _result_guard((THCPLongTensor*) THCPLongTensor_NewEmpty());
      if (!_result_guard.get()) return NULL;
      THCPLongTensor* result = _result_guard.get();
      
      #else
      THPLongTensorPtr _result_guard((THPLongTensor*) THPLongTensor_NewEmpty());
      if (!_result_guard.get()) return NULL;
      THPLongTensor* result = _result_guard.get();
      
      #endif
      
      
      THIndexTensor* arg_result = ((THPIndexTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      int arg_num_samples = ((int) THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_num_samples)));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(multinomial)(LIBRARY_STATE arg_result, arg_self, arg_num_samples, false);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "multinomial", 2, "(int num_samples, #" THPModuleStr "LongTensor out)", "(int num_samples, bool replacement, #" THPModuleStr "LongTensor out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT))
PyObject * THPTensor_stateless_(multinomial)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_generator = NULL;
    PyObject *__kw_source = NULL;
    PyObject *__kw_num_samples = NULL;
    PyObject *__kw_replacement = NULL;
    if (kwargs) {
      __kw_generator = PyDict_GetItemString(kwargs, "generator");
      __kw_source = PyDict_GetItemString(kwargs, "source");
      __kw_num_samples = PyDict_GetItemString(kwargs, "num_samples");
      __kw_replacement = PyDict_GetItemString(kwargs, "replacement");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 5 &&
          (PyObject*)Py_TYPE(___out) == THPIndexTensorClass &&
          __kw_generator && (PyObject*)Py_TYPE(__kw_generator) == THPGeneratorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_num_samples) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_num_samples)) &&
          (__tuplecount > 2 || __kw_replacement) && PyBool_Check((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_replacement))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THIndexTensor* arg_result = ((THPIndexTensor*)___out)->cdata;
      THGenerator* arg_generator = THPGenerator_TH_CData(__kw_generator);
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int arg_num_samples = ((int) THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_num_samples)));
      bool arg_replacement = ((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_replacement) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(multinomial)(LIBRARY_STATE arg_result, arg_generator, arg_self, arg_num_samples, arg_replacement);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 4 &&
          __kw_generator && (PyObject*)Py_TYPE(__kw_generator) == THPGeneratorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_num_samples) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_num_samples)) &&
          (__tuplecount > 2 || __kw_replacement) && PyBool_Check((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_replacement))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      #if IS_CUDA
      THCPLongTensorPtr _result_guard((THCPLongTensor*) THCPLongTensor_NewEmpty());
      if (!_result_guard.get()) return NULL;
      THCPLongTensor* result = _result_guard.get();
      
      #else
      THPLongTensorPtr _result_guard((THPLongTensor*) THPLongTensor_NewEmpty());
      if (!_result_guard.get()) return NULL;
      THPLongTensor* result = _result_guard.get();
      
      #endif
      
      
      THIndexTensor* arg_result = ((THPIndexTensor*)result)->cdata;
      THGenerator* arg_generator = THPGenerator_TH_CData(__kw_generator);
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int arg_num_samples = ((int) THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_num_samples)));
      bool arg_replacement = ((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_replacement) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(multinomial)(LIBRARY_STATE arg_result, arg_generator, arg_self, arg_num_samples, arg_replacement);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 4 &&
          (PyObject*)Py_TYPE(___out) == THPIndexTensorClass &&
          __kw_generator && (PyObject*)Py_TYPE(__kw_generator) == THPGeneratorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_num_samples) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_num_samples))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THIndexTensor* arg_result = ((THPIndexTensor*)___out)->cdata;
      THGenerator* arg_generator = THPGenerator_TH_CData(__kw_generator);
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int arg_num_samples = ((int) THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_num_samples)));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(multinomial)(LIBRARY_STATE arg_result, arg_generator, arg_self, arg_num_samples, false);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 4 &&
          (PyObject*)Py_TYPE(___out) == THPIndexTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_num_samples) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_num_samples)) &&
          (__tuplecount > 2 || __kw_replacement) && PyBool_Check((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_replacement))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THIndexTensor* arg_result = ((THPIndexTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int arg_num_samples = ((int) THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_num_samples)));
      bool arg_replacement = ((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_replacement) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(multinomial)(LIBRARY_STATE arg_result, THPGenerator_TH_CData(THPDefaultGenerator), arg_self, arg_num_samples, arg_replacement);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 3 &&
          __kw_generator && (PyObject*)Py_TYPE(__kw_generator) == THPGeneratorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_num_samples) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_num_samples))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      #if IS_CUDA
      THCPLongTensorPtr _result_guard((THCPLongTensor*) THCPLongTensor_NewEmpty());
      if (!_result_guard.get()) return NULL;
      THCPLongTensor* result = _result_guard.get();
      
      #else
      THPLongTensorPtr _result_guard((THPLongTensor*) THPLongTensor_NewEmpty());
      if (!_result_guard.get()) return NULL;
      THPLongTensor* result = _result_guard.get();
      
      #endif
      
      
      THIndexTensor* arg_result = ((THPIndexTensor*)result)->cdata;
      THGenerator* arg_generator = THPGenerator_TH_CData(__kw_generator);
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int arg_num_samples = ((int) THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_num_samples)));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(multinomial)(LIBRARY_STATE arg_result, arg_generator, arg_self, arg_num_samples, false);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 3 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_num_samples) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_num_samples)) &&
          (__tuplecount > 2 || __kw_replacement) && PyBool_Check((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_replacement))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      #if IS_CUDA
      THCPLongTensorPtr _result_guard((THCPLongTensor*) THCPLongTensor_NewEmpty());
      if (!_result_guard.get()) return NULL;
      THCPLongTensor* result = _result_guard.get();
      
      #else
      THPLongTensorPtr _result_guard((THPLongTensor*) THPLongTensor_NewEmpty());
      if (!_result_guard.get()) return NULL;
      THPLongTensor* result = _result_guard.get();
      
      #endif
      
      
      THIndexTensor* arg_result = ((THPIndexTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int arg_num_samples = ((int) THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_num_samples)));
      bool arg_replacement = ((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_replacement) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(multinomial)(LIBRARY_STATE arg_result, THPGenerator_TH_CData(THPDefaultGenerator), arg_self, arg_num_samples, arg_replacement);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPIndexTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_num_samples) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_num_samples))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THIndexTensor* arg_result = ((THPIndexTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int arg_num_samples = ((int) THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_num_samples)));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(multinomial)(LIBRARY_STATE arg_result, THPGenerator_TH_CData(THPDefaultGenerator), arg_self, arg_num_samples, false);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_num_samples) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_num_samples))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      #if IS_CUDA
      THCPLongTensorPtr _result_guard((THCPLongTensor*) THCPLongTensor_NewEmpty());
      if (!_result_guard.get()) return NULL;
      THCPLongTensor* result = _result_guard.get();
      
      #else
      THPLongTensorPtr _result_guard((THPLongTensor*) THPLongTensor_NewEmpty());
      if (!_result_guard.get()) return NULL;
      THPLongTensor* result = _result_guard.get();
      
      #endif
      
      
      THIndexTensor* arg_result = ((THPIndexTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int arg_num_samples = ((int) THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_num_samples)));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(multinomial)(LIBRARY_STATE arg_result, THPGenerator_TH_CData(THPDefaultGenerator), arg_self, arg_num_samples, false);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.multinomial", 4, "(" THPTensorStr " source, int num_samples, #" THPModuleStr "LongTensor out)", "(" THPTensorStr " source, int num_samples, bool replacement, #" THPModuleStr "LongTensor out)", "(torch.Generator generator, " THPTensorStr " source, int num_samples, #" THPModuleStr "LongTensor out)", "(torch.Generator generator, " THPTensorStr " source, int num_samples, bool replacement, #" THPModuleStr "LongTensor out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF) && (CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_stateless_(multinomial)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    PyObject *__kw_num_samples = NULL;
    PyObject *__kw_replacement = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
      __kw_num_samples = PyDict_GetItemString(kwargs, "num_samples");
      __kw_replacement = PyDict_GetItemString(kwargs, "replacement");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 4 &&
          (PyObject*)Py_TYPE(___out) == THPIndexTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_num_samples) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_num_samples)) &&
          (__tuplecount > 2 || __kw_replacement) && PyBool_Check((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_replacement))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THIndexTensor* arg_result = ((THPIndexTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int arg_num_samples = ((int) THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_num_samples)));
      bool arg_replacement = ((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_replacement) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(multinomial)(LIBRARY_STATE arg_result, arg_self, arg_num_samples, arg_replacement);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 4 &&
          (PyObject*)Py_TYPE(___out) == THPIndexTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_num_samples) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_num_samples)) &&
          (__tuplecount > 2 || __kw_replacement) && PyBool_Check((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_replacement))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THIndexTensor* arg_result = ((THPIndexTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int arg_num_samples = ((int) THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_num_samples)));
      bool arg_replacement = ((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_replacement) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(multinomial)(LIBRARY_STATE arg_result, arg_self, arg_num_samples, arg_replacement);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 3 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_num_samples) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_num_samples)) &&
          (__tuplecount > 2 || __kw_replacement) && PyBool_Check((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_replacement))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      #if IS_CUDA
      THCPLongTensorPtr _result_guard((THCPLongTensor*) THCPLongTensor_NewEmpty());
      if (!_result_guard.get()) return NULL;
      THCPLongTensor* result = _result_guard.get();
      
      #else
      THPLongTensorPtr _result_guard((THPLongTensor*) THPLongTensor_NewEmpty());
      if (!_result_guard.get()) return NULL;
      THPLongTensor* result = _result_guard.get();
      
      #endif
      
      
      THIndexTensor* arg_result = ((THPIndexTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int arg_num_samples = ((int) THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_num_samples)));
      bool arg_replacement = ((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_replacement) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(multinomial)(LIBRARY_STATE arg_result, arg_self, arg_num_samples, arg_replacement);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPIndexTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_num_samples) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_num_samples))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THIndexTensor* arg_result = ((THPIndexTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int arg_num_samples = ((int) THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_num_samples)));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(multinomial)(LIBRARY_STATE arg_result, arg_self, arg_num_samples, false);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 3 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_num_samples) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_num_samples)) &&
          (__tuplecount > 2 || __kw_replacement) && PyBool_Check((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_replacement))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      #if IS_CUDA
      THCPLongTensorPtr _result_guard((THCPLongTensor*) THCPLongTensor_NewEmpty());
      if (!_result_guard.get()) return NULL;
      THCPLongTensor* result = _result_guard.get();
      
      #else
      THPLongTensorPtr _result_guard((THPLongTensor*) THPLongTensor_NewEmpty());
      if (!_result_guard.get()) return NULL;
      THPLongTensor* result = _result_guard.get();
      
      #endif
      
      
      THIndexTensor* arg_result = ((THPIndexTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int arg_num_samples = ((int) THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_num_samples)));
      bool arg_replacement = ((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_replacement) == Py_True ? true : false);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(multinomial)(LIBRARY_STATE arg_result, arg_self, arg_num_samples, arg_replacement);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPIndexTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_num_samples) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_num_samples))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THIndexTensor* arg_result = ((THPIndexTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int arg_num_samples = ((int) THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_num_samples)));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(multinomial)(LIBRARY_STATE arg_result, arg_self, arg_num_samples, false);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_num_samples) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_num_samples))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      #if IS_CUDA
      THCPLongTensorPtr _result_guard((THCPLongTensor*) THCPLongTensor_NewEmpty());
      if (!_result_guard.get()) return NULL;
      THCPLongTensor* result = _result_guard.get();
      
      #else
      THPLongTensorPtr _result_guard((THPLongTensor*) THPLongTensor_NewEmpty());
      if (!_result_guard.get()) return NULL;
      THPLongTensor* result = _result_guard.get();
      
      #endif
      
      
      THIndexTensor* arg_result = ((THPIndexTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int arg_num_samples = ((int) THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_num_samples)));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(multinomial)(LIBRARY_STATE arg_result, arg_self, arg_num_samples, false);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_num_samples) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_num_samples))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      #if IS_CUDA
      THCPLongTensorPtr _result_guard((THCPLongTensor*) THCPLongTensor_NewEmpty());
      if (!_result_guard.get()) return NULL;
      THCPLongTensor* result = _result_guard.get();
      
      #else
      THPLongTensorPtr _result_guard((THPLongTensor*) THPLongTensor_NewEmpty());
      if (!_result_guard.get()) return NULL;
      THPLongTensor* result = _result_guard.get();
      
      #endif
      
      
      THIndexTensor* arg_result = ((THPIndexTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      int arg_num_samples = ((int) THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_num_samples)));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(multinomial)(LIBRARY_STATE arg_result, arg_self, arg_num_samples, false);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.multinomial", 2, "(" THPTensorStr " source, int num_samples, #" THPModuleStr "LongTensor out)", "(" THPTensorStr " source, int num_samples, bool replacement, #" THPModuleStr "LongTensor out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT))
PyObject * THPTensor_(uniform_)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_generator = NULL;
    PyObject *__kw_from = NULL;
    PyObject *__kw_to = NULL;
    if (kwargs) {
      __kw_generator = PyDict_GetItemString(kwargs, "generator");
      __kw_from = PyDict_GetItemString(kwargs, "from");
      __kw_to = PyDict_GetItemString(kwargs, "to");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 3 &&
          __kw_generator && (PyObject*)Py_TYPE(__kw_generator) == THPGeneratorClass &&
          (__tuplecount > 0 || __kw_from) && THPDoubleUtils_checkReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_from)) &&
          (__tuplecount > 1 || __kw_to) && THPDoubleUtils_checkReal((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_to))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THGenerator* arg_generator = THPGenerator_TH_CData(__kw_generator);
      double arg_from = THPDoubleUtils_unpackReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_from));
      double arg_to = THPDoubleUtils_unpackReal((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_to));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(uniform)(LIBRARY_STATE arg_self, arg_generator, arg_from, arg_to);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 2 &&
          __kw_generator && (PyObject*)Py_TYPE(__kw_generator) == THPGeneratorClass &&
          (__tuplecount > 0 || __kw_from) && THPDoubleUtils_checkReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_from))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THGenerator* arg_generator = THPGenerator_TH_CData(__kw_generator);
      double arg_from = THPDoubleUtils_unpackReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_from));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(uniform)(LIBRARY_STATE arg_self, arg_generator, arg_from, 1);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 2 &&
          __kw_generator && (PyObject*)Py_TYPE(__kw_generator) == THPGeneratorClass &&
          __kw_to && THPDoubleUtils_checkReal(__kw_to)) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THGenerator* arg_generator = THPGenerator_TH_CData(__kw_generator);
      double arg_to = THPDoubleUtils_unpackReal(__kw_to);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(uniform)(LIBRARY_STATE arg_self, arg_generator, 0, arg_to);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 2 &&
          (__tuplecount > 0 || __kw_from) && THPDoubleUtils_checkReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_from)) &&
          (__tuplecount > 1 || __kw_to) && THPDoubleUtils_checkReal((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_to))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      double arg_from = THPDoubleUtils_unpackReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_from));
      double arg_to = THPDoubleUtils_unpackReal((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_to));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(uniform)(LIBRARY_STATE arg_self, THPGenerator_TH_CData(THPDefaultGenerator), arg_from, arg_to);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 1 &&
          __kw_generator && (PyObject*)Py_TYPE(__kw_generator) == THPGeneratorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THGenerator* arg_generator = THPGenerator_TH_CData(__kw_generator);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(uniform)(LIBRARY_STATE arg_self, arg_generator, 0, 1);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 1 &&
          (__tuplecount > 0 || __kw_from) && THPDoubleUtils_checkReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_from))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      double arg_from = THPDoubleUtils_unpackReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_from));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(uniform)(LIBRARY_STATE arg_self, THPGenerator_TH_CData(THPDefaultGenerator), arg_from, 1);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 1 &&
          __kw_to && THPDoubleUtils_checkReal(__kw_to)) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      double arg_to = THPDoubleUtils_unpackReal(__kw_to);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(uniform)(LIBRARY_STATE arg_self, THPGenerator_TH_CData(THPDefaultGenerator), 0, arg_to);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(uniform)(LIBRARY_STATE arg_self, THPGenerator_TH_CData(THPDefaultGenerator), 0, 1);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "uniform_", 8, "(float to)", "(float from)", "no arguments", "(float from, float to)", "(torch.Generator generator)", "(torch.Generator generator, float to)", "(torch.Generator generator, float from)", "(torch.Generator generator, float from, float to)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF) && (CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_(uniform_)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_from = NULL;
    PyObject *__kw_to = NULL;
    if (kwargs) {
      __kw_from = PyDict_GetItemString(kwargs, "from");
      __kw_to = PyDict_GetItemString(kwargs, "to");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 2 &&
          (__tuplecount > 0 || __kw_from) && THPDoubleUtils_checkReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_from)) &&
          (__tuplecount > 1 || __kw_to) && THPDoubleUtils_checkReal((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_to))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      double arg_from = THPDoubleUtils_unpackReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_from));
      double arg_to = THPDoubleUtils_unpackReal((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_to));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(uniform)(LIBRARY_STATE arg_self, arg_from, arg_to);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 2 &&
          (__tuplecount > 0 || __kw_from) && THPDoubleUtils_checkReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_from)) &&
          (__tuplecount > 1 || __kw_to) && THPDoubleUtils_checkReal((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_to))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      double arg_from = THPDoubleUtils_unpackReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_from));
      double arg_to = THPDoubleUtils_unpackReal((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_to));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(uniform)(LIBRARY_STATE arg_self, arg_from, arg_to);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 1 &&
          (__tuplecount > 0 || __kw_from) && THPDoubleUtils_checkReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_from))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      double arg_from = THPDoubleUtils_unpackReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_from));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(uniform)(LIBRARY_STATE arg_self, arg_from, 1);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 1 &&
          __kw_to && THPDoubleUtils_checkReal(__kw_to)) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      double arg_to = THPDoubleUtils_unpackReal(__kw_to);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(uniform)(LIBRARY_STATE arg_self, 0, arg_to);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 1 &&
          (__tuplecount > 0 || __kw_from) && THPDoubleUtils_checkReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_from))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      double arg_from = THPDoubleUtils_unpackReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_from));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(uniform)(LIBRARY_STATE arg_self, arg_from, 1);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 1 &&
          __kw_to && THPDoubleUtils_checkReal(__kw_to)) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      double arg_to = THPDoubleUtils_unpackReal(__kw_to);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(uniform)(LIBRARY_STATE arg_self, 0, arg_to);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(uniform)(LIBRARY_STATE arg_self, 0, 1);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(uniform)(LIBRARY_STATE arg_self, 0, 1);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "uniform_", 4, "(float to)", "(float from)", "no arguments", "(float from, float to)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT))
PyObject * THPTensor_stateless_(normal)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_generator = NULL;
    PyObject *__kw_means = NULL;
    PyObject *__kw_std = NULL;
    PyObject *__kw_mean = NULL;
    if (kwargs) {
      __kw_generator = PyDict_GetItemString(kwargs, "generator");
      __kw_means = PyDict_GetItemString(kwargs, "means");
      __kw_std = PyDict_GetItemString(kwargs, "std");
      __kw_mean = PyDict_GetItemString(kwargs, "mean");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 4 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          __kw_generator && (PyObject*)Py_TYPE(__kw_generator) == THPGeneratorClass &&
          (__tuplecount > 0 || __kw_means) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_means)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_std) && THPDoubleUtils_checkReal((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_std))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_output = ((THPTensor*)___out)->cdata;
      THGenerator* arg_generator = THPGenerator_TH_CData(__kw_generator);
      THTensor* arg_means = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_means))->cdata;
      double arg_std = THPDoubleUtils_unpackReal((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_std));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(normal_means)(LIBRARY_STATE arg_output, arg_generator, arg_means, arg_std);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 4 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          __kw_generator && (PyObject*)Py_TYPE(__kw_generator) == THPGeneratorClass &&
          (__tuplecount > 0 || __kw_mean) && THPDoubleUtils_checkReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mean)) &&
          (__tuplecount > 1 || __kw_std) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_std)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_output = ((THPTensor*)___out)->cdata;
      THGenerator* arg_generator = THPGenerator_TH_CData(__kw_generator);
      double arg_mean = THPDoubleUtils_unpackReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mean));
      THTensor* arg_std = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_std))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(normal_stddevs)(LIBRARY_STATE arg_output, arg_generator, arg_mean, arg_std);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 4 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          __kw_generator && (PyObject*)Py_TYPE(__kw_generator) == THPGeneratorClass &&
          (__tuplecount > 0 || __kw_means) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_means)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_std) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_std)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_output = ((THPTensor*)___out)->cdata;
      THGenerator* arg_generator = THPGenerator_TH_CData(__kw_generator);
      THTensor* arg_means = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_means))->cdata;
      THTensor* arg_std = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_std))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(normal_means_stddevs)(LIBRARY_STATE arg_output, arg_generator, arg_means, arg_std);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 3 &&
          __kw_generator && (PyObject*)Py_TYPE(__kw_generator) == THPGeneratorClass &&
          (__tuplecount > 0 || __kw_means) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_means)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_std) && THPDoubleUtils_checkReal((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_std))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _output_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_output_guard.get()) return NULL;
      THPTensor* output = _output_guard.get();
      
      
      THTensor* arg_output = ((THPTensor*)output)->cdata;
      THGenerator* arg_generator = THPGenerator_TH_CData(__kw_generator);
      THTensor* arg_means = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_means))->cdata;
      double arg_std = THPDoubleUtils_unpackReal((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_std));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(normal_means)(LIBRARY_STATE arg_output, arg_generator, arg_means, arg_std);
        Py_BLOCK_THREADS;
        Py_INCREF(output);
        return (PyObject*)(output);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          __kw_generator && (PyObject*)Py_TYPE(__kw_generator) == THPGeneratorClass &&
          (__tuplecount > 0 || __kw_means) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_means)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_output = ((THPTensor*)___out)->cdata;
      THGenerator* arg_generator = THPGenerator_TH_CData(__kw_generator);
      THTensor* arg_means = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_means))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(normal_means)(LIBRARY_STATE arg_output, arg_generator, arg_means, 1);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_means) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_means)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_std) && THPDoubleUtils_checkReal((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_std))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_output = ((THPTensor*)___out)->cdata;
      THTensor* arg_means = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_means))->cdata;
      double arg_std = THPDoubleUtils_unpackReal((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_std));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(normal_means)(LIBRARY_STATE arg_output, THPGenerator_TH_CData(THPDefaultGenerator), arg_means, arg_std);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 3 &&
          __kw_generator && (PyObject*)Py_TYPE(__kw_generator) == THPGeneratorClass &&
          (__tuplecount > 0 || __kw_mean) && THPDoubleUtils_checkReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mean)) &&
          (__tuplecount > 1 || __kw_std) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_std)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _output_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_output_guard.get()) return NULL;
      THPTensor* output = _output_guard.get();
      
      
      THTensor* arg_output = ((THPTensor*)output)->cdata;
      THGenerator* arg_generator = THPGenerator_TH_CData(__kw_generator);
      double arg_mean = THPDoubleUtils_unpackReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mean));
      THTensor* arg_std = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_std))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(normal_stddevs)(LIBRARY_STATE arg_output, arg_generator, arg_mean, arg_std);
        Py_BLOCK_THREADS;
        Py_INCREF(output);
        return (PyObject*)(output);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          __kw_generator && (PyObject*)Py_TYPE(__kw_generator) == THPGeneratorClass &&
          __kw_std && (PyObject*)Py_TYPE(__kw_std) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_output = ((THPTensor*)___out)->cdata;
      THGenerator* arg_generator = THPGenerator_TH_CData(__kw_generator);
      THTensor* arg_std = ((THPTensor*)__kw_std)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(normal_stddevs)(LIBRARY_STATE arg_output, arg_generator, 0, arg_std);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_mean) && THPDoubleUtils_checkReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mean)) &&
          (__tuplecount > 1 || __kw_std) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_std)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_output = ((THPTensor*)___out)->cdata;
      double arg_mean = THPDoubleUtils_unpackReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mean));
      THTensor* arg_std = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_std))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(normal_stddevs)(LIBRARY_STATE arg_output, THPGenerator_TH_CData(THPDefaultGenerator), arg_mean, arg_std);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 3 &&
          __kw_generator && (PyObject*)Py_TYPE(__kw_generator) == THPGeneratorClass &&
          (__tuplecount > 0 || __kw_means) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_means)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_std) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_std)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _output_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_output_guard.get()) return NULL;
      THPTensor* output = _output_guard.get();
      
      
      THTensor* arg_output = ((THPTensor*)output)->cdata;
      THGenerator* arg_generator = THPGenerator_TH_CData(__kw_generator);
      THTensor* arg_means = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_means))->cdata;
      THTensor* arg_std = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_std))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(normal_means_stddevs)(LIBRARY_STATE arg_output, arg_generator, arg_means, arg_std);
        Py_BLOCK_THREADS;
        Py_INCREF(output);
        return (PyObject*)(output);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_means) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_means)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_std) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_std)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_output = ((THPTensor*)___out)->cdata;
      THTensor* arg_means = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_means))->cdata;
      THTensor* arg_std = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_std))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(normal_means_stddevs)(LIBRARY_STATE arg_output, THPGenerator_TH_CData(THPDefaultGenerator), arg_means, arg_std);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          __kw_generator && (PyObject*)Py_TYPE(__kw_generator) == THPGeneratorClass &&
          (__tuplecount > 0 || __kw_means) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_means)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _output_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_output_guard.get()) return NULL;
      THPTensor* output = _output_guard.get();
      
      
      THTensor* arg_output = ((THPTensor*)output)->cdata;
      THGenerator* arg_generator = THPGenerator_TH_CData(__kw_generator);
      THTensor* arg_means = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_means))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(normal_means)(LIBRARY_STATE arg_output, arg_generator, arg_means, 1);
        Py_BLOCK_THREADS;
        Py_INCREF(output);
        return (PyObject*)(output);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_means) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_means)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_std) && THPDoubleUtils_checkReal((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_std))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _output_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_output_guard.get()) return NULL;
      THPTensor* output = _output_guard.get();
      
      
      THTensor* arg_output = ((THPTensor*)output)->cdata;
      THTensor* arg_means = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_means))->cdata;
      double arg_std = THPDoubleUtils_unpackReal((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_std));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(normal_means)(LIBRARY_STATE arg_output, THPGenerator_TH_CData(THPDefaultGenerator), arg_means, arg_std);
        Py_BLOCK_THREADS;
        Py_INCREF(output);
        return (PyObject*)(output);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_means) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_means)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_output = ((THPTensor*)___out)->cdata;
      THTensor* arg_means = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_means))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(normal_means)(LIBRARY_STATE arg_output, THPGenerator_TH_CData(THPDefaultGenerator), arg_means, 1);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          __kw_generator && (PyObject*)Py_TYPE(__kw_generator) == THPGeneratorClass &&
          __kw_std && (PyObject*)Py_TYPE(__kw_std) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _output_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_output_guard.get()) return NULL;
      THPTensor* output = _output_guard.get();
      
      
      THTensor* arg_output = ((THPTensor*)output)->cdata;
      THGenerator* arg_generator = THPGenerator_TH_CData(__kw_generator);
      THTensor* arg_std = ((THPTensor*)__kw_std)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(normal_stddevs)(LIBRARY_STATE arg_output, arg_generator, 0, arg_std);
        Py_BLOCK_THREADS;
        Py_INCREF(output);
        return (PyObject*)(output);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_mean) && THPDoubleUtils_checkReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mean)) &&
          (__tuplecount > 1 || __kw_std) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_std)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _output_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_output_guard.get()) return NULL;
      THPTensor* output = _output_guard.get();
      
      
      THTensor* arg_output = ((THPTensor*)output)->cdata;
      double arg_mean = THPDoubleUtils_unpackReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mean));
      THTensor* arg_std = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_std))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(normal_stddevs)(LIBRARY_STATE arg_output, THPGenerator_TH_CData(THPDefaultGenerator), arg_mean, arg_std);
        Py_BLOCK_THREADS;
        Py_INCREF(output);
        return (PyObject*)(output);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          __kw_std && (PyObject*)Py_TYPE(__kw_std) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_output = ((THPTensor*)___out)->cdata;
      THTensor* arg_std = ((THPTensor*)__kw_std)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(normal_stddevs)(LIBRARY_STATE arg_output, THPGenerator_TH_CData(THPDefaultGenerator), 0, arg_std);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_means) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_means)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_std) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_std)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _output_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_output_guard.get()) return NULL;
      THPTensor* output = _output_guard.get();
      
      
      THTensor* arg_output = ((THPTensor*)output)->cdata;
      THTensor* arg_means = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_means))->cdata;
      THTensor* arg_std = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_std))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(normal_means_stddevs)(LIBRARY_STATE arg_output, THPGenerator_TH_CData(THPDefaultGenerator), arg_means, arg_std);
        Py_BLOCK_THREADS;
        Py_INCREF(output);
        return (PyObject*)(output);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_means) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_means)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _output_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_output_guard.get()) return NULL;
      THPTensor* output = _output_guard.get();
      
      
      THTensor* arg_output = ((THPTensor*)output)->cdata;
      THTensor* arg_means = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_means))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(normal_means)(LIBRARY_STATE arg_output, THPGenerator_TH_CData(THPDefaultGenerator), arg_means, 1);
        Py_BLOCK_THREADS;
        Py_INCREF(output);
        return (PyObject*)(output);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          __kw_std && (PyObject*)Py_TYPE(__kw_std) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _output_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_output_guard.get()) return NULL;
      THPTensor* output = _output_guard.get();
      
      
      THTensor* arg_output = ((THPTensor*)output)->cdata;
      THTensor* arg_std = ((THPTensor*)__kw_std)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(normal_stddevs)(LIBRARY_STATE arg_output, THPGenerator_TH_CData(THPDefaultGenerator), 0, arg_std);
        Py_BLOCK_THREADS;
        Py_INCREF(output);
        return (PyObject*)(output);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.normal", 10, "(" THPTensorStr " std, #" THPTensorStr " out)", "(" THPTensorStr " means, #" THPTensorStr " out)", "(float mean, " THPTensorStr " std, #" THPTensorStr " out)", "(" THPTensorStr " means, float std, #" THPTensorStr " out)", "(" THPTensorStr " means, " THPTensorStr " std, #" THPTensorStr " out)", "(torch.Generator generator, " THPTensorStr " std, #" THPTensorStr " out)", "(torch.Generator generator, " THPTensorStr " means, #" THPTensorStr " out)", "(torch.Generator generator, float mean, " THPTensorStr " std, #" THPTensorStr " out)", "(torch.Generator generator, " THPTensorStr " means, float std, #" THPTensorStr " out)", "(torch.Generator generator, " THPTensorStr " means, " THPTensorStr " std, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF) && (CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_stateless_(normal)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_means = NULL;
    PyObject *__kw_std = NULL;
    PyObject *__kw_mean = NULL;
    if (kwargs) {
      __kw_means = PyDict_GetItemString(kwargs, "means");
      __kw_std = PyDict_GetItemString(kwargs, "std");
      __kw_mean = PyDict_GetItemString(kwargs, "mean");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_means) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_means)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_std) && THPDoubleUtils_checkReal((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_std))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_output = ((THPTensor*)___out)->cdata;
      THTensor* arg_means = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_means))->cdata;
      double arg_std = THPDoubleUtils_unpackReal((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_std));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(normal_means)(LIBRARY_STATE arg_output, arg_means, arg_std);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_means) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_means)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_std) && THPDoubleUtils_checkReal((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_std))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_output = ((THPTensor*)___out)->cdata;
      THTensor* arg_means = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_means))->cdata;
      double arg_std = THPDoubleUtils_unpackReal((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_std));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(normal_means)(LIBRARY_STATE arg_output, arg_means, arg_std);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_mean) && THPDoubleUtils_checkReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mean)) &&
          (__tuplecount > 1 || __kw_std) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_std)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_output = ((THPTensor*)___out)->cdata;
      double arg_mean = THPDoubleUtils_unpackReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mean));
      THTensor* arg_std = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_std))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(normal_stddevs)(LIBRARY_STATE arg_output, arg_mean, arg_std);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_mean) && THPDoubleUtils_checkReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mean)) &&
          (__tuplecount > 1 || __kw_std) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_std)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_output = ((THPTensor*)___out)->cdata;
      double arg_mean = THPDoubleUtils_unpackReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mean));
      THTensor* arg_std = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_std))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(normal_stddevs)(LIBRARY_STATE arg_output, arg_mean, arg_std);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_means) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_means)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_std) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_std)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_output = ((THPTensor*)___out)->cdata;
      THTensor* arg_means = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_means))->cdata;
      THTensor* arg_std = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_std))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(normal_means_stddevs)(LIBRARY_STATE arg_output, arg_means, arg_std);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_means) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_means)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_std) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_std)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_output = ((THPTensor*)___out)->cdata;
      THTensor* arg_means = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_means))->cdata;
      THTensor* arg_std = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_std))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(normal_means_stddevs)(LIBRARY_STATE arg_output, arg_means, arg_std);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_means) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_means)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_std) && THPDoubleUtils_checkReal((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_std))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _output_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_output_guard.get()) return NULL;
      THPTensor* output = _output_guard.get();
      
      
      THTensor* arg_output = ((THPTensor*)output)->cdata;
      THTensor* arg_means = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_means))->cdata;
      double arg_std = THPDoubleUtils_unpackReal((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_std));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(normal_means)(LIBRARY_STATE arg_output, arg_means, arg_std);
        Py_BLOCK_THREADS;
        Py_INCREF(output);
        return (PyObject*)(output);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_means) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_means)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_output = ((THPTensor*)___out)->cdata;
      THTensor* arg_means = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_means))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(normal_means)(LIBRARY_STATE arg_output, arg_means, 1);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_means) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_means)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_std) && THPDoubleUtils_checkReal((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_std))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _output_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_output_guard.get()) return NULL;
      THPTensor* output = _output_guard.get();
      
      
      THTensor* arg_output = ((THPTensor*)output)->cdata;
      THTensor* arg_means = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_means))->cdata;
      double arg_std = THPDoubleUtils_unpackReal((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_std));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(normal_means)(LIBRARY_STATE arg_output, arg_means, arg_std);
        Py_BLOCK_THREADS;
        Py_INCREF(output);
        return (PyObject*)(output);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_means) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_means)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_output = ((THPTensor*)___out)->cdata;
      THTensor* arg_means = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_means))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(normal_means)(LIBRARY_STATE arg_output, arg_means, 1);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_mean) && THPDoubleUtils_checkReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mean)) &&
          (__tuplecount > 1 || __kw_std) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_std)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _output_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_output_guard.get()) return NULL;
      THPTensor* output = _output_guard.get();
      
      
      THTensor* arg_output = ((THPTensor*)output)->cdata;
      double arg_mean = THPDoubleUtils_unpackReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mean));
      THTensor* arg_std = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_std))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(normal_stddevs)(LIBRARY_STATE arg_output, arg_mean, arg_std);
        Py_BLOCK_THREADS;
        Py_INCREF(output);
        return (PyObject*)(output);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          __kw_std && (PyObject*)Py_TYPE(__kw_std) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_output = ((THPTensor*)___out)->cdata;
      THTensor* arg_std = ((THPTensor*)__kw_std)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(normal_stddevs)(LIBRARY_STATE arg_output, 0, arg_std);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_mean) && THPDoubleUtils_checkReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mean)) &&
          (__tuplecount > 1 || __kw_std) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_std)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _output_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_output_guard.get()) return NULL;
      THPTensor* output = _output_guard.get();
      
      
      THTensor* arg_output = ((THPTensor*)output)->cdata;
      double arg_mean = THPDoubleUtils_unpackReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mean));
      THTensor* arg_std = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_std))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(normal_stddevs)(LIBRARY_STATE arg_output, arg_mean, arg_std);
        Py_BLOCK_THREADS;
        Py_INCREF(output);
        return (PyObject*)(output);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          __kw_std && (PyObject*)Py_TYPE(__kw_std) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_output = ((THPTensor*)___out)->cdata;
      THTensor* arg_std = ((THPTensor*)__kw_std)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(normal_stddevs)(LIBRARY_STATE arg_output, 0, arg_std);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_means) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_means)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_std) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_std)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _output_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_output_guard.get()) return NULL;
      THPTensor* output = _output_guard.get();
      
      
      THTensor* arg_output = ((THPTensor*)output)->cdata;
      THTensor* arg_means = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_means))->cdata;
      THTensor* arg_std = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_std))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(normal_means_stddevs)(LIBRARY_STATE arg_output, arg_means, arg_std);
        Py_BLOCK_THREADS;
        Py_INCREF(output);
        return (PyObject*)(output);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_means) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_means)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_std) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_std)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _output_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_output_guard.get()) return NULL;
      THPTensor* output = _output_guard.get();
      
      
      THTensor* arg_output = ((THPTensor*)output)->cdata;
      THTensor* arg_means = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_means))->cdata;
      THTensor* arg_std = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_std))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(normal_means_stddevs)(LIBRARY_STATE arg_output, arg_means, arg_std);
        Py_BLOCK_THREADS;
        Py_INCREF(output);
        return (PyObject*)(output);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_means) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_means)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _output_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_output_guard.get()) return NULL;
      THPTensor* output = _output_guard.get();
      
      
      THTensor* arg_output = ((THPTensor*)output)->cdata;
      THTensor* arg_means = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_means))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(normal_means)(LIBRARY_STATE arg_output, arg_means, 1);
        Py_BLOCK_THREADS;
        Py_INCREF(output);
        return (PyObject*)(output);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_means) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_means)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _output_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_output_guard.get()) return NULL;
      THPTensor* output = _output_guard.get();
      
      
      THTensor* arg_output = ((THPTensor*)output)->cdata;
      THTensor* arg_means = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_means))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(normal_means)(LIBRARY_STATE arg_output, arg_means, 1);
        Py_BLOCK_THREADS;
        Py_INCREF(output);
        return (PyObject*)(output);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          __kw_std && (PyObject*)Py_TYPE(__kw_std) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _output_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_output_guard.get()) return NULL;
      THPTensor* output = _output_guard.get();
      
      
      THTensor* arg_output = ((THPTensor*)output)->cdata;
      THTensor* arg_std = ((THPTensor*)__kw_std)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(normal_stddevs)(LIBRARY_STATE arg_output, 0, arg_std);
        Py_BLOCK_THREADS;
        Py_INCREF(output);
        return (PyObject*)(output);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          __kw_std && (PyObject*)Py_TYPE(__kw_std) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _output_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_output_guard.get()) return NULL;
      THPTensor* output = _output_guard.get();
      
      
      THTensor* arg_output = ((THPTensor*)output)->cdata;
      THTensor* arg_std = ((THPTensor*)__kw_std)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(normal_stddevs)(LIBRARY_STATE arg_output, 0, arg_std);
        Py_BLOCK_THREADS;
        Py_INCREF(output);
        return (PyObject*)(output);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.normal", 5, "(" THPTensorStr " std, #" THPTensorStr " out)", "(" THPTensorStr " means, #" THPTensorStr " out)", "(float mean, " THPTensorStr " std, #" THPTensorStr " out)", "(" THPTensorStr " means, float std, #" THPTensorStr " out)", "(" THPTensorStr " means, " THPTensorStr " std, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT))
PyObject * THPTensor_(normal_)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_generator = NULL;
    PyObject *__kw_mean = NULL;
    PyObject *__kw_std = NULL;
    if (kwargs) {
      __kw_generator = PyDict_GetItemString(kwargs, "generator");
      __kw_mean = PyDict_GetItemString(kwargs, "mean");
      __kw_std = PyDict_GetItemString(kwargs, "std");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 3 &&
          __kw_generator && (PyObject*)Py_TYPE(__kw_generator) == THPGeneratorClass &&
          (__tuplecount > 0 || __kw_mean) && THPDoubleUtils_checkReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mean)) &&
          (__tuplecount > 1 || __kw_std) && THPDoubleUtils_checkReal((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_std))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THGenerator* arg_generator = THPGenerator_TH_CData(__kw_generator);
      double arg_mean = THPDoubleUtils_unpackReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mean));
      double arg_std = THPDoubleUtils_unpackReal((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_std));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(normal)(LIBRARY_STATE arg_self, arg_generator, arg_mean, arg_std);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 2 &&
          __kw_generator && (PyObject*)Py_TYPE(__kw_generator) == THPGeneratorClass &&
          (__tuplecount > 0 || __kw_mean) && THPDoubleUtils_checkReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mean))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THGenerator* arg_generator = THPGenerator_TH_CData(__kw_generator);
      double arg_mean = THPDoubleUtils_unpackReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mean));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(normal)(LIBRARY_STATE arg_self, arg_generator, arg_mean, 1);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 2 &&
          __kw_generator && (PyObject*)Py_TYPE(__kw_generator) == THPGeneratorClass &&
          __kw_std && THPDoubleUtils_checkReal(__kw_std)) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THGenerator* arg_generator = THPGenerator_TH_CData(__kw_generator);
      double arg_std = THPDoubleUtils_unpackReal(__kw_std);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(normal)(LIBRARY_STATE arg_self, arg_generator, 0, arg_std);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 2 &&
          (__tuplecount > 0 || __kw_mean) && THPDoubleUtils_checkReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mean)) &&
          (__tuplecount > 1 || __kw_std) && THPDoubleUtils_checkReal((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_std))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      double arg_mean = THPDoubleUtils_unpackReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mean));
      double arg_std = THPDoubleUtils_unpackReal((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_std));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(normal)(LIBRARY_STATE arg_self, THPGenerator_TH_CData(THPDefaultGenerator), arg_mean, arg_std);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 1 &&
          __kw_generator && (PyObject*)Py_TYPE(__kw_generator) == THPGeneratorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THGenerator* arg_generator = THPGenerator_TH_CData(__kw_generator);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(normal)(LIBRARY_STATE arg_self, arg_generator, 0, 1);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 1 &&
          (__tuplecount > 0 || __kw_mean) && THPDoubleUtils_checkReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mean))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      double arg_mean = THPDoubleUtils_unpackReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mean));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(normal)(LIBRARY_STATE arg_self, THPGenerator_TH_CData(THPDefaultGenerator), arg_mean, 1);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 1 &&
          __kw_std && THPDoubleUtils_checkReal(__kw_std)) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      double arg_std = THPDoubleUtils_unpackReal(__kw_std);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(normal)(LIBRARY_STATE arg_self, THPGenerator_TH_CData(THPDefaultGenerator), 0, arg_std);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(normal)(LIBRARY_STATE arg_self, THPGenerator_TH_CData(THPDefaultGenerator), 0, 1);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "normal_", 8, "(float std)", "(float mean)", "no arguments", "(float mean, float std)", "(torch.Generator generator)", "(torch.Generator generator, float std)", "(torch.Generator generator, float mean)", "(torch.Generator generator, float mean, float std)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF) && (CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_(normal_)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_mean = NULL;
    PyObject *__kw_std = NULL;
    if (kwargs) {
      __kw_mean = PyDict_GetItemString(kwargs, "mean");
      __kw_std = PyDict_GetItemString(kwargs, "std");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 2 &&
          (__tuplecount > 0 || __kw_mean) && THPDoubleUtils_checkReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mean)) &&
          (__tuplecount > 1 || __kw_std) && THPDoubleUtils_checkReal((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_std))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      double arg_mean = THPDoubleUtils_unpackReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mean));
      double arg_std = THPDoubleUtils_unpackReal((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_std));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(normal)(LIBRARY_STATE arg_self, arg_mean, arg_std);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 2 &&
          (__tuplecount > 0 || __kw_mean) && THPDoubleUtils_checkReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mean)) &&
          (__tuplecount > 1 || __kw_std) && THPDoubleUtils_checkReal((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_std))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      double arg_mean = THPDoubleUtils_unpackReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mean));
      double arg_std = THPDoubleUtils_unpackReal((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_std));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(normal)(LIBRARY_STATE arg_self, arg_mean, arg_std);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 1 &&
          (__tuplecount > 0 || __kw_mean) && THPDoubleUtils_checkReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mean))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      double arg_mean = THPDoubleUtils_unpackReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mean));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(normal)(LIBRARY_STATE arg_self, arg_mean, 1);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 1 &&
          __kw_std && THPDoubleUtils_checkReal(__kw_std)) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      double arg_std = THPDoubleUtils_unpackReal(__kw_std);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(normal)(LIBRARY_STATE arg_self, 0, arg_std);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 1 &&
          (__tuplecount > 0 || __kw_mean) && THPDoubleUtils_checkReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mean))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      double arg_mean = THPDoubleUtils_unpackReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mean));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(normal)(LIBRARY_STATE arg_self, arg_mean, 1);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 1 &&
          __kw_std && THPDoubleUtils_checkReal(__kw_std)) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      double arg_std = THPDoubleUtils_unpackReal(__kw_std);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(normal)(LIBRARY_STATE arg_self, 0, arg_std);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(normal)(LIBRARY_STATE arg_self, 0, 1);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(normal)(LIBRARY_STATE arg_self, 0, 1);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "normal_", 4, "(float std)", "(float mean)", "no arguments", "(float mean, float std)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT))
PyObject * THPTensor_(cauchy_)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_generator = NULL;
    PyObject *__kw_median = NULL;
    PyObject *__kw_sigma = NULL;
    if (kwargs) {
      __kw_generator = PyDict_GetItemString(kwargs, "generator");
      __kw_median = PyDict_GetItemString(kwargs, "median");
      __kw_sigma = PyDict_GetItemString(kwargs, "sigma");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 3 &&
          __kw_generator && (PyObject*)Py_TYPE(__kw_generator) == THPGeneratorClass &&
          (__tuplecount > 0 || __kw_median) && THPDoubleUtils_checkReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_median)) &&
          (__tuplecount > 1 || __kw_sigma) && THPDoubleUtils_checkReal((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_sigma))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THGenerator* arg_generator = THPGenerator_TH_CData(__kw_generator);
      double arg_median = THPDoubleUtils_unpackReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_median));
      double arg_sigma = THPDoubleUtils_unpackReal((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_sigma));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cauchy)(LIBRARY_STATE arg_self, arg_generator, arg_median, arg_sigma);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 2 &&
          __kw_generator && (PyObject*)Py_TYPE(__kw_generator) == THPGeneratorClass &&
          (__tuplecount > 0 || __kw_median) && THPDoubleUtils_checkReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_median))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THGenerator* arg_generator = THPGenerator_TH_CData(__kw_generator);
      double arg_median = THPDoubleUtils_unpackReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_median));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cauchy)(LIBRARY_STATE arg_self, arg_generator, arg_median, 1);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 2 &&
          __kw_generator && (PyObject*)Py_TYPE(__kw_generator) == THPGeneratorClass &&
          __kw_sigma && THPDoubleUtils_checkReal(__kw_sigma)) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THGenerator* arg_generator = THPGenerator_TH_CData(__kw_generator);
      double arg_sigma = THPDoubleUtils_unpackReal(__kw_sigma);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cauchy)(LIBRARY_STATE arg_self, arg_generator, 0, arg_sigma);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 2 &&
          (__tuplecount > 0 || __kw_median) && THPDoubleUtils_checkReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_median)) &&
          (__tuplecount > 1 || __kw_sigma) && THPDoubleUtils_checkReal((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_sigma))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      double arg_median = THPDoubleUtils_unpackReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_median));
      double arg_sigma = THPDoubleUtils_unpackReal((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_sigma));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cauchy)(LIBRARY_STATE arg_self, THPGenerator_TH_CData(THPDefaultGenerator), arg_median, arg_sigma);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 1 &&
          __kw_generator && (PyObject*)Py_TYPE(__kw_generator) == THPGeneratorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THGenerator* arg_generator = THPGenerator_TH_CData(__kw_generator);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cauchy)(LIBRARY_STATE arg_self, arg_generator, 0, 1);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 1 &&
          (__tuplecount > 0 || __kw_median) && THPDoubleUtils_checkReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_median))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      double arg_median = THPDoubleUtils_unpackReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_median));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cauchy)(LIBRARY_STATE arg_self, THPGenerator_TH_CData(THPDefaultGenerator), arg_median, 1);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 1 &&
          __kw_sigma && THPDoubleUtils_checkReal(__kw_sigma)) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      double arg_sigma = THPDoubleUtils_unpackReal(__kw_sigma);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cauchy)(LIBRARY_STATE arg_self, THPGenerator_TH_CData(THPDefaultGenerator), 0, arg_sigma);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cauchy)(LIBRARY_STATE arg_self, THPGenerator_TH_CData(THPDefaultGenerator), 0, 1);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "cauchy_", 8, "no arguments", "(float sigma)", "(float median)", "(float median, float sigma)", "(torch.Generator generator)", "(torch.Generator generator, float sigma)", "(torch.Generator generator, float median)", "(torch.Generator generator, float median, float sigma)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF) && (CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_(cauchy_)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_median = NULL;
    PyObject *__kw_sigma = NULL;
    if (kwargs) {
      __kw_median = PyDict_GetItemString(kwargs, "median");
      __kw_sigma = PyDict_GetItemString(kwargs, "sigma");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 2 &&
          (__tuplecount > 0 || __kw_median) && THPDoubleUtils_checkReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_median)) &&
          (__tuplecount > 1 || __kw_sigma) && THPDoubleUtils_checkReal((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_sigma))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      double arg_median = THPDoubleUtils_unpackReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_median));
      double arg_sigma = THPDoubleUtils_unpackReal((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_sigma));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cauchy)(LIBRARY_STATE arg_self, arg_median, arg_sigma);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 2 &&
          (__tuplecount > 0 || __kw_median) && THPDoubleUtils_checkReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_median)) &&
          (__tuplecount > 1 || __kw_sigma) && THPDoubleUtils_checkReal((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_sigma))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      double arg_median = THPDoubleUtils_unpackReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_median));
      double arg_sigma = THPDoubleUtils_unpackReal((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_sigma));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cauchy)(LIBRARY_STATE arg_self, arg_median, arg_sigma);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 1 &&
          (__tuplecount > 0 || __kw_median) && THPDoubleUtils_checkReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_median))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      double arg_median = THPDoubleUtils_unpackReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_median));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cauchy)(LIBRARY_STATE arg_self, arg_median, 1);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 1 &&
          __kw_sigma && THPDoubleUtils_checkReal(__kw_sigma)) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      double arg_sigma = THPDoubleUtils_unpackReal(__kw_sigma);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cauchy)(LIBRARY_STATE arg_self, 0, arg_sigma);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 1 &&
          (__tuplecount > 0 || __kw_median) && THPDoubleUtils_checkReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_median))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      double arg_median = THPDoubleUtils_unpackReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_median));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cauchy)(LIBRARY_STATE arg_self, arg_median, 1);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 1 &&
          __kw_sigma && THPDoubleUtils_checkReal(__kw_sigma)) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      double arg_sigma = THPDoubleUtils_unpackReal(__kw_sigma);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cauchy)(LIBRARY_STATE arg_self, 0, arg_sigma);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cauchy)(LIBRARY_STATE arg_self, 0, 1);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(cauchy)(LIBRARY_STATE arg_self, 0, 1);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "cauchy_", 4, "no arguments", "(float sigma)", "(float median)", "(float median, float sigma)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT))
PyObject * THPTensor_(logNormal_)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_generator = NULL;
    PyObject *__kw_mean = NULL;
    PyObject *__kw_std = NULL;
    if (kwargs) {
      __kw_generator = PyDict_GetItemString(kwargs, "generator");
      __kw_mean = PyDict_GetItemString(kwargs, "mean");
      __kw_std = PyDict_GetItemString(kwargs, "std");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 3 &&
          __kw_generator && (PyObject*)Py_TYPE(__kw_generator) == THPGeneratorClass &&
          (__tuplecount > 0 || __kw_mean) && THPDoubleUtils_checkReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mean)) &&
          (__tuplecount > 1 || __kw_std) && THPDoubleUtils_checkReal((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_std))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THGenerator* arg_generator = THPGenerator_TH_CData(__kw_generator);
      double arg_mean = THPDoubleUtils_unpackReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mean));
      double arg_std = THPDoubleUtils_unpackReal((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_std));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(logNormal)(LIBRARY_STATE arg_self, arg_generator, arg_mean, arg_std);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 2 &&
          __kw_generator && (PyObject*)Py_TYPE(__kw_generator) == THPGeneratorClass &&
          (__tuplecount > 0 || __kw_mean) && THPDoubleUtils_checkReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mean))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THGenerator* arg_generator = THPGenerator_TH_CData(__kw_generator);
      double arg_mean = THPDoubleUtils_unpackReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mean));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(logNormal)(LIBRARY_STATE arg_self, arg_generator, arg_mean, 2);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 2 &&
          __kw_generator && (PyObject*)Py_TYPE(__kw_generator) == THPGeneratorClass &&
          __kw_std && THPDoubleUtils_checkReal(__kw_std)) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THGenerator* arg_generator = THPGenerator_TH_CData(__kw_generator);
      double arg_std = THPDoubleUtils_unpackReal(__kw_std);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(logNormal)(LIBRARY_STATE arg_self, arg_generator, 1, arg_std);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 2 &&
          (__tuplecount > 0 || __kw_mean) && THPDoubleUtils_checkReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mean)) &&
          (__tuplecount > 1 || __kw_std) && THPDoubleUtils_checkReal((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_std))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      double arg_mean = THPDoubleUtils_unpackReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mean));
      double arg_std = THPDoubleUtils_unpackReal((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_std));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(logNormal)(LIBRARY_STATE arg_self, THPGenerator_TH_CData(THPDefaultGenerator), arg_mean, arg_std);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 1 &&
          __kw_generator && (PyObject*)Py_TYPE(__kw_generator) == THPGeneratorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THGenerator* arg_generator = THPGenerator_TH_CData(__kw_generator);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(logNormal)(LIBRARY_STATE arg_self, arg_generator, 1, 2);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 1 &&
          (__tuplecount > 0 || __kw_mean) && THPDoubleUtils_checkReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mean))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      double arg_mean = THPDoubleUtils_unpackReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mean));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(logNormal)(LIBRARY_STATE arg_self, THPGenerator_TH_CData(THPDefaultGenerator), arg_mean, 2);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 1 &&
          __kw_std && THPDoubleUtils_checkReal(__kw_std)) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      double arg_std = THPDoubleUtils_unpackReal(__kw_std);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(logNormal)(LIBRARY_STATE arg_self, THPGenerator_TH_CData(THPDefaultGenerator), 1, arg_std);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(logNormal)(LIBRARY_STATE arg_self, THPGenerator_TH_CData(THPDefaultGenerator), 1, 2);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "log_normal_", 8, "(float std)", "(float mean)", "no arguments", "(float mean, float std)", "(torch.Generator generator)", "(torch.Generator generator, float std)", "(torch.Generator generator, float mean)", "(torch.Generator generator, float mean, float std)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF) && (CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_(logNormal_)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_mean = NULL;
    PyObject *__kw_std = NULL;
    if (kwargs) {
      __kw_mean = PyDict_GetItemString(kwargs, "mean");
      __kw_std = PyDict_GetItemString(kwargs, "std");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 2 &&
          (__tuplecount > 0 || __kw_mean) && THPDoubleUtils_checkReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mean)) &&
          (__tuplecount > 1 || __kw_std) && THPDoubleUtils_checkReal((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_std))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      double arg_mean = THPDoubleUtils_unpackReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mean));
      double arg_std = THPDoubleUtils_unpackReal((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_std));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(logNormal)(LIBRARY_STATE arg_self, arg_mean, arg_std);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 2 &&
          (__tuplecount > 0 || __kw_mean) && THPDoubleUtils_checkReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mean)) &&
          (__tuplecount > 1 || __kw_std) && THPDoubleUtils_checkReal((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_std))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      double arg_mean = THPDoubleUtils_unpackReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mean));
      double arg_std = THPDoubleUtils_unpackReal((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_std));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(logNormal)(LIBRARY_STATE arg_self, arg_mean, arg_std);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 1 &&
          (__tuplecount > 0 || __kw_mean) && THPDoubleUtils_checkReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mean))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      double arg_mean = THPDoubleUtils_unpackReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mean));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(logNormal)(LIBRARY_STATE arg_self, arg_mean, 2);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 1 &&
          __kw_std && THPDoubleUtils_checkReal(__kw_std)) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      double arg_std = THPDoubleUtils_unpackReal(__kw_std);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(logNormal)(LIBRARY_STATE arg_self, 1, arg_std);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 1 &&
          (__tuplecount > 0 || __kw_mean) && THPDoubleUtils_checkReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mean))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      double arg_mean = THPDoubleUtils_unpackReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mean));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(logNormal)(LIBRARY_STATE arg_self, arg_mean, 2);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 1 &&
          __kw_std && THPDoubleUtils_checkReal(__kw_std)) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      double arg_std = THPDoubleUtils_unpackReal(__kw_std);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(logNormal)(LIBRARY_STATE arg_self, 1, arg_std);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(logNormal)(LIBRARY_STATE arg_self, 1, 2);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(logNormal)(LIBRARY_STATE arg_self, 1, 2);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "log_normal_", 4, "(float std)", "(float mean)", "no arguments", "(float mean, float std)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT))
PyObject * THPTensor_(exponential_)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_generator = NULL;
    PyObject *__kw_lambd = NULL;
    if (kwargs) {
      __kw_generator = PyDict_GetItemString(kwargs, "generator");
      __kw_lambd = PyDict_GetItemString(kwargs, "lambd");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 2 &&
          __kw_generator && (PyObject*)Py_TYPE(__kw_generator) == THPGeneratorClass &&
          (__tuplecount > 0 || __kw_lambd) && THPDoubleUtils_checkReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_lambd))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THGenerator* arg_generator = THPGenerator_TH_CData(__kw_generator);
      double arg_lambd = THPDoubleUtils_unpackReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_lambd));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(exponential)(LIBRARY_STATE arg_self, arg_generator, arg_lambd);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 1 &&
          __kw_generator && (PyObject*)Py_TYPE(__kw_generator) == THPGeneratorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THGenerator* arg_generator = THPGenerator_TH_CData(__kw_generator);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(exponential)(LIBRARY_STATE arg_self, arg_generator, 1);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 1 &&
          (__tuplecount > 0 || __kw_lambd) && THPDoubleUtils_checkReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_lambd))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      double arg_lambd = THPDoubleUtils_unpackReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_lambd));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(exponential)(LIBRARY_STATE arg_self, THPGenerator_TH_CData(THPDefaultGenerator), arg_lambd);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(exponential)(LIBRARY_STATE arg_self, THPGenerator_TH_CData(THPDefaultGenerator), 1);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "exponential_", 4, "no arguments", "(float lambd)", "(torch.Generator generator)", "(torch.Generator generator, float lambd)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF) && (CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_(exponential_)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_lambd = NULL;
    if (kwargs) {
      __kw_lambd = PyDict_GetItemString(kwargs, "lambd");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 1 &&
          (__tuplecount > 0 || __kw_lambd) && THPDoubleUtils_checkReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_lambd))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      double arg_lambd = THPDoubleUtils_unpackReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_lambd));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(exponential)(LIBRARY_STATE arg_self, arg_lambd);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 1 &&
          (__tuplecount > 0 || __kw_lambd) && THPDoubleUtils_checkReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_lambd))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      double arg_lambd = THPDoubleUtils_unpackReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_lambd));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(exponential)(LIBRARY_STATE arg_self, arg_lambd);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(exponential)(LIBRARY_STATE arg_self, 1);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(exponential)(LIBRARY_STATE arg_self, 1);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "exponential_", 2, "no arguments", "(float lambd)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT))
PyObject * THPTensor_stateless_(_standard_gamma)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_generator = NULL;
    PyObject *__kw_alpha = NULL;
    if (kwargs) {
      __kw_generator = PyDict_GetItemString(kwargs, "generator");
      __kw_alpha = PyDict_GetItemString(kwargs, "alpha");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          __kw_generator && (PyObject*)Py_TYPE(__kw_generator) == THPGeneratorClass &&
          (__tuplecount > 0 || __kw_alpha) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_alpha)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_output = ((THPTensor*)___out)->cdata;
      THGenerator* arg_generator = THPGenerator_TH_CData(__kw_generator);
      THTensor* arg_alpha = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_alpha))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(standard_gamma)(LIBRARY_STATE arg_output, arg_generator, arg_alpha);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          __kw_generator && (PyObject*)Py_TYPE(__kw_generator) == THPGeneratorClass &&
          (__tuplecount > 0 || __kw_alpha) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_alpha)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _output_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_output_guard.get()) return NULL;
      THPTensor* output = _output_guard.get();
      
      
      THTensor* arg_output = ((THPTensor*)output)->cdata;
      THGenerator* arg_generator = THPGenerator_TH_CData(__kw_generator);
      THTensor* arg_alpha = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_alpha))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(standard_gamma)(LIBRARY_STATE arg_output, arg_generator, arg_alpha);
        Py_BLOCK_THREADS;
        Py_INCREF(output);
        return (PyObject*)(output);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_alpha) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_alpha)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_output = ((THPTensor*)___out)->cdata;
      THTensor* arg_alpha = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_alpha))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(standard_gamma)(LIBRARY_STATE arg_output, THPGenerator_TH_CData(THPDefaultGenerator), arg_alpha);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_alpha) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_alpha)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _output_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_output_guard.get()) return NULL;
      THPTensor* output = _output_guard.get();
      
      
      THTensor* arg_output = ((THPTensor*)output)->cdata;
      THTensor* arg_alpha = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_alpha))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(standard_gamma)(LIBRARY_STATE arg_output, THPGenerator_TH_CData(THPDefaultGenerator), arg_alpha);
        Py_BLOCK_THREADS;
        Py_INCREF(output);
        return (PyObject*)(output);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch._standard_gamma", 2, "(" THPTensorStr " alpha, #" THPTensorStr " out)", "(torch.Generator generator, " THPTensorStr " alpha, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT))
PyObject * THPTensor_stateless_(_dirichlet_grad)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_x = NULL;
    PyObject *__kw_alpha = NULL;
    PyObject *__kw_total = NULL;
    if (kwargs) {
      __kw_x = PyDict_GetItemString(kwargs, "x");
      __kw_alpha = PyDict_GetItemString(kwargs, "alpha");
      __kw_total = PyDict_GetItemString(kwargs, "total");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 4 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_x) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_x)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_alpha) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_alpha)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_total) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_total)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_output = ((THPTensor*)___out)->cdata;
      THTensor* arg_x = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_x))->cdata;
      THTensor* arg_alpha = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_alpha))->cdata;
      THTensor* arg_total = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_total))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(dirichlet_grad)(LIBRARY_STATE arg_output, arg_x, arg_alpha, arg_total);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 3 &&
          (__tuplecount > 0 || __kw_x) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_x)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_alpha) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_alpha)) == THPTensorClass &&
          (__tuplecount > 2 || __kw_total) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_total)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _output_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_output_guard.get()) return NULL;
      THPTensor* output = _output_guard.get();
      
      
      THTensor* arg_output = ((THPTensor*)output)->cdata;
      THTensor* arg_x = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_x))->cdata;
      THTensor* arg_alpha = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_alpha))->cdata;
      THTensor* arg_total = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_total))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(dirichlet_grad)(LIBRARY_STATE arg_output, arg_x, arg_alpha, arg_total);
        Py_BLOCK_THREADS;
        Py_INCREF(output);
        return (PyObject*)(output);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch._dirichlet_grad", 1, "(" THPTensorStr " x, " THPTensorStr " alpha, " THPTensorStr " total, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT))
PyObject * THPTensor_stateless_(rand)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_generator = NULL;
    if (kwargs) {
      __kw_generator = PyDict_GetItemString(kwargs, "generator");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    THLongStoragePtr __size;
PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (__dictcount == 2 &&
          ___out != NULL &&
          __argcount >= 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          __kw_generator && (PyObject*)Py_TYPE(__kw_generator) == THPGeneratorClass &&
          THPUtils_tryUnpackLongVarArgs(args, 0, __size)) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THGenerator* arg_generator = THPGenerator_TH_CData(__kw_generator);
      THSize* arg_size = __size.get();
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(rand)(LIBRARY_STATE arg_result, arg_generator, arg_size);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__dictcount == 1 &&
          ___out == NULL &&
          __argcount >= 2 &&
          __kw_generator && (PyObject*)Py_TYPE(__kw_generator) == THPGeneratorClass &&
          THPUtils_tryUnpackLongVarArgs(args, 0, __size)) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THGenerator* arg_generator = THPGenerator_TH_CData(__kw_generator);
      THSize* arg_size = __size.get();
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(rand)(LIBRARY_STATE arg_result, arg_generator, arg_size);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__dictcount == 1 &&
          ___out != NULL &&
          __argcount >= 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          THPUtils_tryUnpackLongVarArgs(args, 0, __size)) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THSize* arg_size = __size.get();
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(rand)(LIBRARY_STATE arg_result, THPGenerator_TH_CData(THPDefaultGenerator), arg_size);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__dictcount == 0 &&
          ___out == NULL &&
          __argcount >= 1 &&
          THPUtils_tryUnpackLongVarArgs(args, 0, __size)) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THSize* arg_size = __size.get();
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(rand)(LIBRARY_STATE arg_result, THPGenerator_TH_CData(THPDefaultGenerator), arg_size);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.rand", 4, "(int ... size, #" THPTensorStr " out)", "(torch.Size size, #" THPTensorStr " out)", "(torch.Generator generator, int ... size, #" THPTensorStr " out)", "(torch.Generator generator, torch.Size size, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF) && (CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_stateless_(rand)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    
    if (kwargs) {
      
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    THLongStoragePtr __size;
PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (__dictcount == 1 &&
          ___out != NULL &&
          __argcount >= 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          THPUtils_tryUnpackLongVarArgs(args, 0, __size)) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THSize* arg_size = __size.get();
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(rand)(LIBRARY_STATE arg_result, arg_size);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__dictcount == 1 &&
          ___out != NULL &&
          __argcount >= 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          THPUtils_tryUnpackLongVarArgs(args, 0, __size)) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THSize* arg_size = __size.get();
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(rand)(LIBRARY_STATE arg_result, arg_size);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__dictcount == 0 &&
          ___out == NULL &&
          __argcount >= 1 &&
          THPUtils_tryUnpackLongVarArgs(args, 0, __size)) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THSize* arg_size = __size.get();
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(rand)(LIBRARY_STATE arg_result, arg_size);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__dictcount == 0 &&
          ___out == NULL &&
          __argcount >= 1 &&
          THPUtils_tryUnpackLongVarArgs(args, 0, __size)) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THSize* arg_size = __size.get();
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(rand)(LIBRARY_STATE arg_result, arg_size);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.rand", 2, "(int ... size, #" THPTensorStr " out)", "(torch.Size size, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT))
PyObject * THPTensor_stateless_(randn)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_generator = NULL;
    if (kwargs) {
      __kw_generator = PyDict_GetItemString(kwargs, "generator");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    THLongStoragePtr __size;
PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (__dictcount == 2 &&
          ___out != NULL &&
          __argcount >= 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          __kw_generator && (PyObject*)Py_TYPE(__kw_generator) == THPGeneratorClass &&
          THPUtils_tryUnpackLongVarArgs(args, 0, __size)) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THGenerator* arg_generator = THPGenerator_TH_CData(__kw_generator);
      THSize* arg_size = __size.get();
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(randn)(LIBRARY_STATE arg_result, arg_generator, arg_size);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__dictcount == 1 &&
          ___out == NULL &&
          __argcount >= 2 &&
          __kw_generator && (PyObject*)Py_TYPE(__kw_generator) == THPGeneratorClass &&
          THPUtils_tryUnpackLongVarArgs(args, 0, __size)) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THGenerator* arg_generator = THPGenerator_TH_CData(__kw_generator);
      THSize* arg_size = __size.get();
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(randn)(LIBRARY_STATE arg_result, arg_generator, arg_size);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__dictcount == 1 &&
          ___out != NULL &&
          __argcount >= 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          THPUtils_tryUnpackLongVarArgs(args, 0, __size)) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THSize* arg_size = __size.get();
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(randn)(LIBRARY_STATE arg_result, THPGenerator_TH_CData(THPDefaultGenerator), arg_size);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__dictcount == 0 &&
          ___out == NULL &&
          __argcount >= 1 &&
          THPUtils_tryUnpackLongVarArgs(args, 0, __size)) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THSize* arg_size = __size.get();
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(randn)(LIBRARY_STATE arg_result, THPGenerator_TH_CData(THPDefaultGenerator), arg_size);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.randn", 4, "(int ... size, #" THPTensorStr " out)", "(torch.Size size, #" THPTensorStr " out)", "(torch.Generator generator, int ... size, #" THPTensorStr " out)", "(torch.Generator generator, torch.Size size, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF) && (CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
PyObject * THPTensor_stateless_(randn)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    
    if (kwargs) {
      
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    THLongStoragePtr __size;
PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (__dictcount == 1 &&
          ___out != NULL &&
          __argcount >= 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          THPUtils_tryUnpackLongVarArgs(args, 0, __size)) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THSize* arg_size = __size.get();
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(randn)(LIBRARY_STATE arg_result, arg_size);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__dictcount == 1 &&
          ___out != NULL &&
          __argcount >= 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          THPUtils_tryUnpackLongVarArgs(args, 0, __size)) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THSize* arg_size = __size.get();
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(randn)(LIBRARY_STATE arg_result, arg_size);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__dictcount == 0 &&
          ___out == NULL &&
          __argcount >= 1 &&
          THPUtils_tryUnpackLongVarArgs(args, 0, __size)) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THSize* arg_size = __size.get();
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(randn)(LIBRARY_STATE arg_result, arg_size);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (__dictcount == 0 &&
          ___out == NULL &&
          __argcount >= 1 &&
          THPUtils_tryUnpackLongVarArgs(args, 0, __size)) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THSize* arg_size = __size.get();
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(randn)(LIBRARY_STATE arg_result, arg_size);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.randn", 2, "(int ... size, #" THPTensorStr " out)", "(torch.Size size, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF) && (!IS_DISTRIBUTED && (!IS_CUDA))
PyObject * THPTensor_(geometric_)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_generator = NULL;
    PyObject *__kw_p = NULL;
    if (kwargs) {
      __kw_generator = PyDict_GetItemString(kwargs, "generator");
      __kw_p = PyDict_GetItemString(kwargs, "p");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    #if !IS_DISTRIBUTED
          
    if (__argcount == 2 &&
          __kw_generator && (PyObject*)Py_TYPE(__kw_generator) == THPGeneratorClass &&
          (__tuplecount > 0 || __kw_p) && THPDoubleUtils_checkReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_p))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THGenerator* arg_generator = THPGenerator_TH_CData(__kw_generator);
      double arg_p = THPDoubleUtils_unpackReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_p));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(geometric)(LIBRARY_STATE arg_self, arg_generator, arg_p);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #else
    if (false) {
    #endif
#if !IS_DISTRIBUTED
          
    } else if (__argcount == 1 &&
          (__tuplecount > 0 || __kw_p) && THPDoubleUtils_checkReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_p))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      double arg_p = THPDoubleUtils_unpackReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_p));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(geometric)(LIBRARY_STATE arg_self, THPGenerator_TH_CData(THPDefaultGenerator), arg_p);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif

    }

    THPUtils_invalidArguments(args, kwargs, "geometric_", 2, "(float p)", "(torch.Generator generator, float p)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF) && (!IS_DISTRIBUTED && (IS_CUDA))
PyObject * THPTensor_(geometric_)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_p = NULL;
    if (kwargs) {
      __kw_p = PyDict_GetItemString(kwargs, "p");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    #if !IS_DISTRIBUTED
          
    if (__argcount == 1 &&
          (__tuplecount > 0 || __kw_p) && THPDoubleUtils_checkReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_p))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      double arg_p = THPDoubleUtils_unpackReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_p));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(geometric)(LIBRARY_STATE arg_self, arg_p);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #else
    if (false) {
    #endif
#if !IS_DISTRIBUTED
          
    } else if (__argcount == 1 &&
          (__tuplecount > 0 || __kw_p) && THPDoubleUtils_checkReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_p))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      double arg_p = THPDoubleUtils_unpackReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_p));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(geometric)(LIBRARY_STATE arg_self, arg_p);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif

    }

    THPUtils_invalidArguments(args, kwargs, "geometric_", 1, "(float p)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#define THDoubleTensor_BERNOULLI_TENSOR THDoubleTensor_bernoulli_DoubleTensor
#define THFloatTensor_BERNOULLI_TENSOR THFloatTensor_bernoulli_FloatTensor
#define THCudaDoubleTensor_BERNOULLI_TENSOR THCudaDoubleTensor_bernoulli_DoubleTensor
#define THCudaTensor_BERNOULLI_TENSOR THCudaTensor_bernoulli_FloatTensor

#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_FLOAT) || defined(TH_REAL_IS_DOUBLE))
PyObject * THPTensor_(bernoulli)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_generator = NULL;
    if (kwargs) {
      __kw_generator = PyDict_GetItemString(kwargs, "generator");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          __kw_generator && (PyObject*)Py_TYPE(__kw_generator) == THPGeneratorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_output = ((THPTensor*)___out)->cdata;
      THGenerator* arg_generator = THPGenerator_TH_CData(__kw_generator);
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        THTensor_(resizeAs)(LIBRARY_STATE ((THPTensor*)___out)->cdata, ((THPTensor*)self)->cdata);
        Py_UNBLOCK_THREADS;
        THTensor_(BERNOULLI_TENSOR)(LIBRARY_STATE arg_output, arg_generator, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          __kw_generator && (PyObject*)Py_TYPE(__kw_generator) == THPGeneratorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _output_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_output_guard.get()) return NULL;
      THPTensor* output = _output_guard.get();
      
      
      THTensor* arg_output = ((THPTensor*)output)->cdata;
      THGenerator* arg_generator = THPGenerator_TH_CData(__kw_generator);
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        THTensor_(resizeAs)(LIBRARY_STATE ((THPTensor*)output)->cdata, ((THPTensor*)self)->cdata);
        Py_UNBLOCK_THREADS;
        THTensor_(BERNOULLI_TENSOR)(LIBRARY_STATE arg_output, arg_generator, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(output);
        return (PyObject*)(output);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 1 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_output = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        THTensor_(resizeAs)(LIBRARY_STATE ((THPTensor*)___out)->cdata, ((THPTensor*)self)->cdata);
        Py_UNBLOCK_THREADS;
        THTensor_(BERNOULLI_TENSOR)(LIBRARY_STATE arg_output, THPGenerator_TH_CData(THPDefaultGenerator), arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _output_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_output_guard.get()) return NULL;
      THPTensor* output = _output_guard.get();
      
      
      THTensor* arg_output = ((THPTensor*)output)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        THTensor_(resizeAs)(LIBRARY_STATE ((THPTensor*)output)->cdata, ((THPTensor*)self)->cdata);
        Py_UNBLOCK_THREADS;
        THTensor_(BERNOULLI_TENSOR)(LIBRARY_STATE arg_output, THPGenerator_TH_CData(THPDefaultGenerator), arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(output);
        return (PyObject*)(output);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "bernoulli", 2, "(#" THPTensorStr " out)", "(torch.Generator generator, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF) && (CUDA_FLOAT || CUDA_DOUBLE)
PyObject * THPTensor_(bernoulli)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    
    if (kwargs) {
      
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 1 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_output = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        THTensor_(resizeAs)(LIBRARY_STATE ((THPTensor*)___out)->cdata, ((THPTensor*)self)->cdata);
        Py_UNBLOCK_THREADS;
        THTensor_(BERNOULLI_TENSOR)(LIBRARY_STATE arg_output, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 1 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_output = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        THTensor_(resizeAs)(LIBRARY_STATE ((THPTensor*)___out)->cdata, ((THPTensor*)self)->cdata);
        Py_UNBLOCK_THREADS;
        THTensor_(BERNOULLI_TENSOR)(LIBRARY_STATE arg_output, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _output_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_output_guard.get()) return NULL;
      THPTensor* output = _output_guard.get();
      
      
      THTensor* arg_output = ((THPTensor*)output)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        THTensor_(resizeAs)(LIBRARY_STATE ((THPTensor*)output)->cdata, ((THPTensor*)self)->cdata);
        Py_UNBLOCK_THREADS;
        THTensor_(BERNOULLI_TENSOR)(LIBRARY_STATE arg_output, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(output);
        return (PyObject*)(output);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _output_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_output_guard.get()) return NULL;
      THPTensor* output = _output_guard.get();
      
      
      THTensor* arg_output = ((THPTensor*)output)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        THTensor_(resizeAs)(LIBRARY_STATE ((THPTensor*)output)->cdata, ((THPTensor*)self)->cdata);
        Py_UNBLOCK_THREADS;
        THTensor_(BERNOULLI_TENSOR)(LIBRARY_STATE arg_output, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(output);
        return (PyObject*)(output);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "bernoulli", 1, "(#" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_FLOAT) || defined(TH_REAL_IS_DOUBLE))
PyObject * THPTensor_stateless_(bernoulli)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_generator = NULL;
    PyObject *__kw_source = NULL;
    if (kwargs) {
      __kw_generator = PyDict_GetItemString(kwargs, "generator");
      __kw_source = PyDict_GetItemString(kwargs, "source");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          __kw_generator && (PyObject*)Py_TYPE(__kw_generator) == THPGeneratorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_output = ((THPTensor*)___out)->cdata;
      THGenerator* arg_generator = THPGenerator_TH_CData(__kw_generator);
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        THTensor_(resizeAs)(LIBRARY_STATE ((THPTensor*)___out)->cdata, ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata);
        Py_UNBLOCK_THREADS;
        THTensor_(BERNOULLI_TENSOR)(LIBRARY_STATE arg_output, arg_generator, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 2 &&
          __kw_generator && (PyObject*)Py_TYPE(__kw_generator) == THPGeneratorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _output_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_output_guard.get()) return NULL;
      THPTensor* output = _output_guard.get();
      
      
      THTensor* arg_output = ((THPTensor*)output)->cdata;
      THGenerator* arg_generator = THPGenerator_TH_CData(__kw_generator);
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        THTensor_(resizeAs)(LIBRARY_STATE ((THPTensor*)output)->cdata, ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata);
        Py_UNBLOCK_THREADS;
        THTensor_(BERNOULLI_TENSOR)(LIBRARY_STATE arg_output, arg_generator, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(output);
        return (PyObject*)(output);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_output = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        THTensor_(resizeAs)(LIBRARY_STATE ((THPTensor*)___out)->cdata, ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata);
        Py_UNBLOCK_THREADS;
        THTensor_(BERNOULLI_TENSOR)(LIBRARY_STATE arg_output, THPGenerator_TH_CData(THPDefaultGenerator), arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _output_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_output_guard.get()) return NULL;
      THPTensor* output = _output_guard.get();
      
      
      THTensor* arg_output = ((THPTensor*)output)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        THTensor_(resizeAs)(LIBRARY_STATE ((THPTensor*)output)->cdata, ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata);
        Py_UNBLOCK_THREADS;
        THTensor_(BERNOULLI_TENSOR)(LIBRARY_STATE arg_output, THPGenerator_TH_CData(THPDefaultGenerator), arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(output);
        return (PyObject*)(output);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.bernoulli", 2, "(" THPTensorStr " source, #" THPTensorStr " out)", "(torch.Generator generator, " THPTensorStr " source, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF) && (CUDA_FLOAT || CUDA_DOUBLE)
PyObject * THPTensor_stateless_(bernoulli)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    
    if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_output = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        THTensor_(resizeAs)(LIBRARY_STATE ((THPTensor*)___out)->cdata, ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata);
        Py_UNBLOCK_THREADS;
        THTensor_(BERNOULLI_TENSOR)(LIBRARY_STATE arg_output, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_output = ((THPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        THTensor_(resizeAs)(LIBRARY_STATE ((THPTensor*)___out)->cdata, ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata);
        Py_UNBLOCK_THREADS;
        THTensor_(BERNOULLI_TENSOR)(LIBRARY_STATE arg_output, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _output_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_output_guard.get()) return NULL;
      THPTensor* output = _output_guard.get();
      
      
      THTensor* arg_output = ((THPTensor*)output)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        THTensor_(resizeAs)(LIBRARY_STATE ((THPTensor*)output)->cdata, ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata);
        Py_UNBLOCK_THREADS;
        THTensor_(BERNOULLI_TENSOR)(LIBRARY_STATE arg_output, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(output);
        return (PyObject*)(output);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _output_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_output_guard.get()) return NULL;
      THPTensor* output = _output_guard.get();
      
      
      THTensor* arg_output = ((THPTensor*)output)->cdata;
      THTensor* arg_self = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        THTensor_(resizeAs)(LIBRARY_STATE ((THPTensor*)output)->cdata, ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata);
        Py_UNBLOCK_THREADS;
        THTensor_(BERNOULLI_TENSOR)(LIBRARY_STATE arg_output, arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(output);
        return (PyObject*)(output);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "torch.bernoulli", 1, "(" THPTensorStr " source, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#undef THDoubleTensor_BERNOULLI_TENSOR
#undef THFloatTensor_BERNOULLI_TENSOR
#undef THCudaDoubleTensor_BERNOULLI_TENSOR
#undef THCudaTensor_BERNOULLI_TENSOR

#if !defined(TH_REAL_IS_HALF) && (!IS_DISTRIBUTED && (!IS_CUDA))
PyObject * THPTensor_(bernoulli_)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_generator = NULL;
    PyObject *__kw_p = NULL;
    PyObject *__kw_float_p = NULL;
    if (kwargs) {
      __kw_generator = PyDict_GetItemString(kwargs, "generator");
      __kw_p = PyDict_GetItemString(kwargs, "p");
      __kw_float_p = PyDict_GetItemString(kwargs, "float_p");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    #if !IS_DISTRIBUTED
          
    if (__argcount == 2 &&
          __kw_generator && (PyObject*)Py_TYPE(__kw_generator) == THPGeneratorClass &&
          (__tuplecount > 0 || __kw_p) && THPDoubleUtils_checkReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_p))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THGenerator* arg_generator = THPGenerator_TH_CData(__kw_generator);
      double arg_p = THPDoubleUtils_unpackReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_p));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(bernoulli)(LIBRARY_STATE arg_self, arg_generator, arg_p);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #else
    if (false) {
    #endif
#if !IS_DISTRIBUTED
          
    } else if (__argcount == 2 &&
          __kw_generator && (PyObject*)Py_TYPE(__kw_generator) == THPGeneratorClass &&
          (__tuplecount > 0 || __kw_float_p) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_float_p)) == THPFloatTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THGenerator* arg_generator = THPGenerator_TH_CData(__kw_generator);
      THFloatTensor* arg_float_p = ((THPFloatTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_float_p))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(bernoulli_FloatTensor)(LIBRARY_STATE arg_self, arg_generator, arg_float_p);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif
#if !IS_DISTRIBUTED
          
    } else if (__argcount == 2 &&
          __kw_generator && (PyObject*)Py_TYPE(__kw_generator) == THPGeneratorClass &&
          (__tuplecount > 0 || __kw_float_p) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_float_p)) == THPDoubleTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THGenerator* arg_generator = THPGenerator_TH_CData(__kw_generator);
      THDoubleTensor* arg_float_p = ((THPDoubleTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_float_p))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(bernoulli_DoubleTensor)(LIBRARY_STATE arg_self, arg_generator, arg_float_p);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif
#if !IS_DISTRIBUTED
          
    } else if (__argcount == 1 &&
          __kw_generator && (PyObject*)Py_TYPE(__kw_generator) == THPGeneratorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THGenerator* arg_generator = THPGenerator_TH_CData(__kw_generator);
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(bernoulli)(LIBRARY_STATE arg_self, arg_generator, 0.5);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif
#if !IS_DISTRIBUTED
          
    } else if (__argcount == 1 &&
          (__tuplecount > 0 || __kw_p) && THPDoubleUtils_checkReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_p))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      double arg_p = THPDoubleUtils_unpackReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_p));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(bernoulli)(LIBRARY_STATE arg_self, THPGenerator_TH_CData(THPDefaultGenerator), arg_p);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif
#if !IS_DISTRIBUTED
          
    } else if (__argcount == 1 &&
          (__tuplecount > 0 || __kw_float_p) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_float_p)) == THPFloatTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THFloatTensor* arg_float_p = ((THPFloatTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_float_p))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(bernoulli_FloatTensor)(LIBRARY_STATE arg_self, THPGenerator_TH_CData(THPDefaultGenerator), arg_float_p);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif
#if !IS_DISTRIBUTED
          
    } else if (__argcount == 1 &&
          (__tuplecount > 0 || __kw_float_p) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_float_p)) == THPDoubleTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THDoubleTensor* arg_float_p = ((THPDoubleTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_float_p))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(bernoulli_DoubleTensor)(LIBRARY_STATE arg_self, THPGenerator_TH_CData(THPDefaultGenerator), arg_float_p);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif
#if !IS_DISTRIBUTED
          
    } else if (__argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(bernoulli)(LIBRARY_STATE arg_self, THPGenerator_TH_CData(THPDefaultGenerator), 0.5);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif

    }

    THPUtils_invalidArguments(args, kwargs, "bernoulli_", 8, "(float p)", "no arguments", "(torch.Generator generator)", "(torch.Generator generator, float p)", "(" THPModuleStr "FloatTensor float_p)", "(" THPModuleStr "DoubleTensor float_p)", "(torch.Generator generator, " THPModuleStr "FloatTensor float_p)", "(torch.Generator generator, " THPModuleStr "DoubleTensor float_p)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF) && (!IS_DISTRIBUTED && (IS_CUDA))
PyObject * THPTensor_(bernoulli_)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_p = NULL;
    PyObject *__kw_float_p = NULL;
    if (kwargs) {
      __kw_p = PyDict_GetItemString(kwargs, "p");
      __kw_float_p = PyDict_GetItemString(kwargs, "float_p");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    #if !IS_DISTRIBUTED
          
    if (__argcount == 1 &&
          (__tuplecount > 0 || __kw_p) && THPDoubleUtils_checkReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_p))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      double arg_p = THPDoubleUtils_unpackReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_p));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(bernoulli)(LIBRARY_STATE arg_self, arg_p);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #else
    if (false) {
    #endif
#if !IS_DISTRIBUTED
          
    } else if (__argcount == 1 &&
          (__tuplecount > 0 || __kw_p) && THPDoubleUtils_checkReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_p))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      double arg_p = THPDoubleUtils_unpackReal((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_p));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(bernoulli)(LIBRARY_STATE arg_self, arg_p);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif
#if !IS_DISTRIBUTED
          
    } else if (__argcount == 1 &&
          (__tuplecount > 0 || __kw_float_p) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_float_p)) == THCPFloatTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THCudaTensor* arg_float_p = ((THCPFloatTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_float_p))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(bernoulli_FloatTensor)(LIBRARY_STATE arg_self, arg_float_p);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif
#if !IS_DISTRIBUTED
          
    } else if (__argcount == 1 &&
          (__tuplecount > 0 || __kw_float_p) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_float_p)) == THCPFloatTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THCudaTensor* arg_float_p = ((THCPFloatTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_float_p))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(bernoulli_FloatTensor)(LIBRARY_STATE arg_self, arg_float_p);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif
#if !IS_DISTRIBUTED
          
    } else if (__argcount == 1 &&
          (__tuplecount > 0 || __kw_float_p) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_float_p)) == THCPDoubleTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THCudaDoubleTensor* arg_float_p = ((THCPDoubleTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_float_p))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(bernoulli_DoubleTensor)(LIBRARY_STATE arg_self, arg_float_p);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif
#if !IS_DISTRIBUTED
          
    } else if (__argcount == 1 &&
          (__tuplecount > 0 || __kw_float_p) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_float_p)) == THCPDoubleTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THCudaDoubleTensor* arg_float_p = ((THCPDoubleTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_float_p))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(bernoulli_DoubleTensor)(LIBRARY_STATE arg_self, arg_float_p);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif
#if !IS_DISTRIBUTED
          
    } else if (__argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(bernoulli)(LIBRARY_STATE arg_self, 0.5);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif
#if !IS_DISTRIBUTED
          
    } else if (__argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(bernoulli)(LIBRARY_STATE arg_self, 0.5);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif

    }

    THPUtils_invalidArguments(args, kwargs, "bernoulli_", 4, "(float p)", "no arguments", "(torch.cuda.FloatTensor float_p)", "(torch.cuda.DoubleTensor float_p)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF) && (IS_CUDA)
PyObject * THPTensor_(getDevice)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    
    if (kwargs) {
      
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    
    if (__argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        int64_t __result = THTensor_(getDevice)(LIBRARY_STATE arg_self);
        Py_BLOCK_THREADS;
        return PyInt_FromLong(__result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    }

    THPUtils_invalidArguments(args, kwargs, "get_device", 1, "no arguments");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if IS_CUDA
static PyObject * THPTensor_(pynew)(PyTypeObject *type, PyObject *args, PyObject *kwargs);
PyObject * THPTensor_(new)(THPTensor *self, PyObject *args, PyObject *kwargs)
{
  THCPAutoGPU gpu_guard(args, (PyObject*)self);
  return THPTensor_(pynew)(Py_TYPE(self), args, kwargs);
}
#endif

#if IS_CUDA
PyObject * THPTensor_(recordStream)(THPTensor *self, PyObject *arg)
{
  HANDLE_TH_ERRORS
  if (!THCPStream_Check(arg)) {
    return PyErr_Format(PyExc_TypeError, "expected Stream object");
  }
  void* data = THTensor_(data)(LIBRARY_STATE self->cdata);
  THCCachingAllocator_recordStream(data, ((THCPStream*)arg)->cdata);
  Py_RETURN_NONE;
  END_HANDLE_TH_ERRORS
}
#endif


#if !IS_DISTRIBUTED
#if IS_CUDA || !defined(TH_REAL_IS_HALF)
PyObject * THSPTensor_(size)(PyObject *self, PyObject *args, PyObject *kwargs)
{
  HANDLE_TH_ERRORS
  THSTensor* tensor = ((THSPTensor*)self)->cdata;
  if (PyTuple_Size(args) == 0 && (!kwargs || PyDict_Size(kwargs) == 0)) {
    return THPSize_New(tensor->nDimensionI + tensor->nDimensionV, tensor->size);
  }

  int tuplecount = args ? PyTuple_Size(args) : 0;
  int dictcount = kwargs ? PyDict_Size(kwargs) : 0;

  PyObject* pydim = NULL;
  if (tuplecount == 1 && dictcount == 0) {
    pydim = PyTuple_GET_ITEM(args, 0);
  } else if (dictcount == 1 && tuplecount == 0) {
    pydim = PyDict_GetItemString(kwargs, "dim");
  }

  if (pydim && THPUtils_checkLong(pydim)) {
    int dim = (int)THPUtils_unpackLong(pydim);
    if (dim < 0)
      dim += tensor->nDimensionI + tensor->nDimensionV;
    return PyInt_FromLong(THSTensor_(size)(LIBRARY_STATE tensor, dim));
  }

  THPUtils_invalidArguments(args, kwargs, "size", 2, "(int dim)", "no arguments");
  return NULL;
  END_HANDLE_TH_ERRORS
}
#endif

#if IS_CUDA
static PyObject * THSPTensor_(pynew)(PyTypeObject *type, PyObject *args, PyObject *kwargs);
PyObject * THSPTensor_(new)(THPTensor *self, PyObject *args, PyObject *kwargs)
{
  THCPAutoGPU gpu_guard(args, (PyObject*)self);
  return THSPTensor_(pynew)(Py_TYPE(self), args, kwargs);
}
#endif

#if !defined(TH_REAL_IS_HALF)
PyObject * THSPTensor_(nDimension)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    
    if (kwargs) {
      
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    #if !IS_DISTRIBUTED
          
    if (__argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THSTensor* arg_self = ((THSPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        int64_t __result = THSTensor_(nDimension)(LIBRARY_STATE arg_self);
        Py_BLOCK_THREADS;
        return PyInt_FromLong(__result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #else
    if (false) {
    #endif

    }

    THPUtils_invalidArguments(args, kwargs, "ndimension", 1, "no arguments");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THSPTensor_(nnz)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    
    if (kwargs) {
      
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    #if !IS_DISTRIBUTED
          
    if (__argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THSTensor* arg_self = ((THSPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        int64_t __result = THSTensor_(nnz)(LIBRARY_STATE arg_self);
        Py_BLOCK_THREADS;
        return PyInt_FromLong(__result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #else
    if (false) {
    #endif

    }

    THPUtils_invalidArguments(args, kwargs, "_nnz", 1, "no arguments");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THSPTensor_(nDimensionI)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    
    if (kwargs) {
      
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    #if !IS_DISTRIBUTED
          
    if (__argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THSTensor* arg_self = ((THSPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        long __result = THSTensor_(nDimensionI)(LIBRARY_STATE arg_self);
        Py_BLOCK_THREADS;
        return PyInt_FromLong(__result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #else
    if (false) {
    #endif

    }

    THPUtils_invalidArguments(args, kwargs, "_dimI", 1, "no arguments");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THSPTensor_(nDimensionV)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    
    if (kwargs) {
      
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    #if !IS_DISTRIBUTED
          
    if (__argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THSTensor* arg_self = ((THSPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        long __result = THSTensor_(nDimensionV)(LIBRARY_STATE arg_self);
        Py_BLOCK_THREADS;
        return PyInt_FromLong(__result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #else
    if (false) {
    #endif

    }

    THPUtils_invalidArguments(args, kwargs, "_dimV", 1, "no arguments");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THSPTensor_(isCoalesced)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    
    if (kwargs) {
      
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    #if !IS_DISTRIBUTED
          
    if (__argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THSTensor* arg_self = ((THSPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        bool __result = THSTensor_(isCoalesced)(LIBRARY_STATE arg_self);
        Py_BLOCK_THREADS;
        return PyBool_FromLong(__result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #else
    if (false) {
    #endif

    }

    THPUtils_invalidArguments(args, kwargs, "is_coalesced", 1, "no arguments");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THSPTensor_(indices)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    
    if (kwargs) {
      
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    #if !IS_DISTRIBUTED
          
    if (__argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THSTensor* arg_self = ((THSPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THIndexTensor* __result = THSTensor_(newIndices)(LIBRARY_STATE arg_self);
        Py_BLOCK_THREADS;
        return THPIndexTensor_(New)(__result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #else
    if (false) {
    #endif

    }

    THPUtils_invalidArguments(args, kwargs, "_indices", 1, "no arguments");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THSPTensor_(values)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    
    if (kwargs) {
      
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    #if !IS_DISTRIBUTED
          
    if (__argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THSTensor* arg_self = ((THSPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor* __result = THSTensor_(newValues)(LIBRARY_STATE arg_self);
        Py_BLOCK_THREADS;
        return THPTensor_(New)(__result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #else
    if (false) {
    #endif

    }

    THPUtils_invalidArguments(args, kwargs, "_values", 1, "no arguments");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THSPTensor_(coalesce)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    
    if (kwargs) {
      
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    #if !IS_DISTRIBUTED
          
    if (__argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THSTensor* arg_self = ((THSPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor* __result = THSTensor_(newCoalesce)(LIBRARY_STATE arg_self);
        Py_BLOCK_THREADS;
        return THSPTensor_(New)(__result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #else
    if (false) {
    #endif

    }

    THPUtils_invalidArguments(args, kwargs, "coalesce", 1, "no arguments");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THSPTensor_(clone)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    
    if (kwargs) {
      
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    #if !IS_DISTRIBUTED
          
    if (__argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THSTensor* arg_self = ((THSPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor* __result = THSTensor_(newClone)(LIBRARY_STATE arg_self);
        Py_BLOCK_THREADS;
        return THSPTensor_(New)(__result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #else
    if (false) {
    #endif

    }

    THPUtils_invalidArguments(args, kwargs, "clone", 1, "no arguments");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THSPTensor_(toDense)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    
    if (kwargs) {
      
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    #if !IS_DISTRIBUTED
          
    if (__argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THSTensor* arg_self = ((THSPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor* __result = THSTensor_(toDense)(LIBRARY_STATE arg_self);
        Py_BLOCK_THREADS;
        return THPTensor_(New)(__result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #else
    if (false) {
    #endif

    }

    THPUtils_invalidArguments(args, kwargs, "to_dense", 1, "no arguments");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THSPTensor_(resizeAs_)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_the_template = NULL;
    if (kwargs) {
      __kw_the_template = PyDict_GetItemString(kwargs, "the_template");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    #if !IS_DISTRIBUTED
          
    if (__argcount == 1 &&
          (__tuplecount > 0 || __kw_the_template) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_the_template)) == THSPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THSTensor* arg_self = ((THSPTensor*)self)->cdata;
      THSTensor* arg_the_template = ((THSPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_the_template))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(resizeAs)(LIBRARY_STATE arg_self, arg_the_template);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #else
    if (false) {
    #endif

    }

    THPUtils_invalidArguments(args, kwargs, "resize_as_", 1, "(" THSPTensorStr " the_template)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THSPTensor_(transpose)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_dim0 = NULL;
    PyObject *__kw_dim1 = NULL;
    if (kwargs) {
      __kw_dim0 = PyDict_GetItemString(kwargs, "dim0");
      __kw_dim1 = PyDict_GetItemString(kwargs, "dim1");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    #if !IS_DISTRIBUTED
          
    if (__argcount == 2 &&
          (__tuplecount > 0 || __kw_dim0) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim0)) &&
          (__tuplecount > 1 || __kw_dim1) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim1))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THSTensor* arg_self = ((THSPTensor*)self)->cdata;
      int64_t arg_dim0 = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim0));
      int64_t arg_dim1 = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim1));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor* __result = THSTensor_(newTranspose)(LIBRARY_STATE arg_self, arg_dim0, arg_dim1);
        Py_BLOCK_THREADS;
        return THSPTensor_(New)(__result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #else
    if (false) {
    #endif

    }

    THPUtils_invalidArguments(args, kwargs, "transpose", 1, "(int dim0, int dim1)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THSPTensor_(transpose_)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_dim0 = NULL;
    PyObject *__kw_dim1 = NULL;
    if (kwargs) {
      __kw_dim0 = PyDict_GetItemString(kwargs, "dim0");
      __kw_dim1 = PyDict_GetItemString(kwargs, "dim1");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    #if !IS_DISTRIBUTED
          
    if (__argcount == 2 &&
          (__tuplecount > 0 || __kw_dim0) && THPUtils_checkLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim0)) &&
          (__tuplecount > 1 || __kw_dim1) && THPUtils_checkLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim1))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THSTensor* arg_self = ((THSPTensor*)self)->cdata;
      int64_t arg_dim0 = THPUtils_unpackLong((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_dim0));
      int64_t arg_dim1 = THPUtils_unpackLong((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_dim1));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(transpose)(LIBRARY_STATE arg_self, arg_dim0, arg_dim1);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)(self);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #else
    if (false) {
    #endif

    }

    THPUtils_invalidArguments(args, kwargs, "transpose_", 1, "(int dim0, int dim1)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THSPTensor_(t)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    
    if (kwargs) {
      
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    #if !IS_DISTRIBUTED
          
    if (__argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THSTensor* arg_self = ((THSPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        int64_t nDimI = ((THSPTensor*)self)->cdata->nDimensionI;
        int64_t nDimV = ((THSPTensor*)self)->cdata->nDimensionV;
        THPUtils_assert(nDimI == 2 && nDimV == 0, "t() expects a 2D sparse tensor, but self is %ldD indices and %ldD values", nDimI, nDimV);
        
        Py_UNBLOCK_THREADS;
        THSTensor* __result = THSTensor_(newTranspose)(LIBRARY_STATE arg_self, 0, 1);
        Py_BLOCK_THREADS;
        return THSPTensor_(New)(__result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #else
    if (false) {
    #endif

    }

    THPUtils_invalidArguments(args, kwargs, "t", 1, "no arguments");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF)
PyObject * THSPTensor_stateless_(t)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    #if !IS_DISTRIBUTED
          
    if (__argcount == 1 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THSPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THSTensor* arg_self = ((THSPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        int64_t nDimI = ((THSPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata->nDimensionI;
        int64_t nDimV = ((THSPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata->nDimensionV;
        THPUtils_assert(nDimI == 2 && nDimV == 0, "t() expects a 2D sparse tensor, but self is %ldD indices and %ldD values", nDimI, nDimV);
        
        Py_UNBLOCK_THREADS;
        THSTensor* __result = THSTensor_(newTranspose)(LIBRARY_STATE arg_self, 0, 1);
        Py_BLOCK_THREADS;
        return THSPTensor_(New)(__result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #else
    if (false) {
    #endif

    }

    THPUtils_invalidArguments(args, kwargs, "torch.t", 1, "(" THSPTensorStr " source)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THSPTensor_(t_)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    
    if (kwargs) {
      
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    #if !IS_DISTRIBUTED
          
    if (__argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THSTensor* arg_self = ((THSPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        int64_t nDimI = ((THSPTensor*)self)->cdata->nDimensionI;
        int64_t nDimV = ((THSPTensor*)self)->cdata->nDimensionV;
        THPUtils_assert(nDimI == 2 && nDimV == 0, "t_() expects a 2D sparse tensor, but self is %ldD indices and %ldD values", nDimI, nDimV);
        
        Py_UNBLOCK_THREADS;
        THSTensor_(transpose)(LIBRARY_STATE arg_self, 0, 1);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #else
    if (false) {
    #endif

    }

    THPUtils_invalidArguments(args, kwargs, "t_", 1, "no arguments");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THSPTensor_stateless_(mm)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_mat1 = NULL;
    PyObject *__kw_mat2 = NULL;
    if (kwargs) {
      __kw_mat1 = PyDict_GetItemString(kwargs, "mat1");
      __kw_mat2 = PyDict_GetItemString(kwargs, "mat2");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    #if !IS_DISTRIBUTED
          
    if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_mat1) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mat1)) == THSPTensorClass &&
          (__tuplecount > 1 || __kw_mat2) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_0 = ((THPTensor*)___out)->cdata;
      THSTensor* arg_mat1 = ((THSPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mat1))->cdata;
      THTensor* arg_mat2 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat2))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(spaddmm)(LIBRARY_STATE arg_result, AS_REAL(0), arg_0, AS_REAL(1), arg_mat1, arg_mat2);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #else
    if (false) {
    #endif
#if !IS_DISTRIBUTED
          
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_mat1) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mat1)) == THSPTensorClass &&
          (__tuplecount > 1 || __kw_mat2) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_0 = ((THPTensor*)result)->cdata;
      THSTensor* arg_mat1 = ((THSPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mat1))->cdata;
      THTensor* arg_mat2 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat2))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(spaddmm)(LIBRARY_STATE arg_result, AS_REAL(0), arg_0, AS_REAL(1), arg_mat1, arg_mat2);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif

    }

    THPUtils_invalidArguments(args, kwargs, "torch.mm", 1, "(" THSPTensorStr " mat1, " THPTensorStr " mat2, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THSPTensor_stateless_(spmm)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_mat1 = NULL;
    PyObject *__kw_mat2 = NULL;
    if (kwargs) {
      __kw_mat1 = PyDict_GetItemString(kwargs, "mat1");
      __kw_mat2 = PyDict_GetItemString(kwargs, "mat2");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    #if !IS_DISTRIBUTED
          
    if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_mat1) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mat1)) == THSPTensorClass &&
          (__tuplecount > 1 || __kw_mat2) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_0 = ((THPTensor*)___out)->cdata;
      THSTensor* arg_mat1 = ((THSPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mat1))->cdata;
      THTensor* arg_mat2 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat2))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(spaddmm)(LIBRARY_STATE arg_result, AS_REAL(0), arg_0, AS_REAL(1), arg_mat1, arg_mat2);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #else
    if (false) {
    #endif
#if !IS_DISTRIBUTED
          
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_mat1) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mat1)) == THSPTensorClass &&
          (__tuplecount > 1 || __kw_mat2) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_0 = ((THPTensor*)result)->cdata;
      THSTensor* arg_mat1 = ((THSPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mat1))->cdata;
      THTensor* arg_mat2 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat2))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(spaddmm)(LIBRARY_STATE arg_result, AS_REAL(0), arg_0, AS_REAL(1), arg_mat1, arg_mat2);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif

    }

    THPUtils_invalidArguments(args, kwargs, "torch.spmm", 1, "(" THSPTensorStr " mat1, " THPTensorStr " mat2, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THSPTensor_stateless_(hspmm)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_mat1 = NULL;
    PyObject *__kw_mat2 = NULL;
    if (kwargs) {
      __kw_mat1 = PyDict_GetItemString(kwargs, "mat1");
      __kw_mat2 = PyDict_GetItemString(kwargs, "mat2");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    #if !IS_DISTRIBUTED
          
    if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THSPTensorClass &&
          (__tuplecount > 0 || __kw_mat1) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mat1)) == THSPTensorClass &&
          (__tuplecount > 1 || __kw_mat2) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THSTensor* arg_result = ((THSPTensor*)___out)->cdata;
      THSTensor* arg_mat1 = ((THSPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mat1))->cdata;
      THTensor* arg_mat2 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat2))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(hspmm)(LIBRARY_STATE arg_result, AS_REAL(1), arg_mat1, arg_mat2);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #else
    if (false) {
    #endif
#if !IS_DISTRIBUTED
          
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_mat1) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mat1)) == THSPTensorClass &&
          (__tuplecount > 1 || __kw_mat2) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THSPTensorPtr _result_guard((THSPTensor*) THSPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THSPTensor* result = _result_guard.get();
      
      
      THSTensor* arg_result = ((THSPTensor*)result)->cdata;
      THSTensor* arg_mat1 = ((THSPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mat1))->cdata;
      THTensor* arg_mat2 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat2))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(hspmm)(LIBRARY_STATE arg_result, AS_REAL(1), arg_mat1, arg_mat2);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif

    }

    THPUtils_invalidArguments(args, kwargs, "torch.hspmm", 1, "(" THSPTensorStr " mat1, " THPTensorStr " mat2, #" THSPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THSPTensor_stateless_(sspmm)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_mat1 = NULL;
    PyObject *__kw_mat2 = NULL;
    if (kwargs) {
      __kw_mat1 = PyDict_GetItemString(kwargs, "mat1");
      __kw_mat2 = PyDict_GetItemString(kwargs, "mat2");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    #if !IS_DISTRIBUTED
          
    if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THSPTensorClass &&
          (__tuplecount > 0 || __kw_mat1) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mat1)) == THSPTensorClass &&
          (__tuplecount > 1 || __kw_mat2) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THSTensor* arg_result = ((THSPTensor*)___out)->cdata;
      THSTensor* arg_0 = ((THSPTensor*)___out)->cdata;
      THSTensor* arg_mat1 = ((THSPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mat1))->cdata;
      THTensor* arg_mat2 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat2))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(sspaddmm)(LIBRARY_STATE arg_result, AS_REAL(0), arg_0, AS_REAL(1), arg_mat1, arg_mat2);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #else
    if (false) {
    #endif
#if !IS_DISTRIBUTED
          
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_mat1) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mat1)) == THSPTensorClass &&
          (__tuplecount > 1 || __kw_mat2) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THSPTensorPtr _result_guard((THSPTensor*) THSPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THSPTensor* result = _result_guard.get();
      
      
      THSTensor* arg_result = ((THSPTensor*)result)->cdata;
      THSTensor* arg_0 = ((THSPTensor*)result)->cdata;
      THSTensor* arg_mat1 = ((THSPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mat1))->cdata;
      THTensor* arg_mat2 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat2))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(sspaddmm)(LIBRARY_STATE arg_result, AS_REAL(0), arg_0, AS_REAL(1), arg_mat1, arg_mat2);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif

    }

    THPUtils_invalidArguments(args, kwargs, "torch.sspmm", 1, "(" THSPTensorStr " mat1, " THPTensorStr " mat2, #" THSPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THSPTensor_(sspaddmm)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_beta = NULL;
    PyObject *__kw_alpha = NULL;
    PyObject *__kw_mat1 = NULL;
    PyObject *__kw_mat2 = NULL;
    if (kwargs) {
      __kw_beta = PyDict_GetItemString(kwargs, "beta");
      __kw_alpha = PyDict_GetItemString(kwargs, "alpha");
      __kw_mat1 = PyDict_GetItemString(kwargs, "mat1");
      __kw_mat2 = PyDict_GetItemString(kwargs, "mat2");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    #if !IS_DISTRIBUTED
          
    if (___out != NULL &&
          __argcount == 5 &&
          (PyObject*)Py_TYPE(___out) == THSPTensorClass &&
          (__tuplecount > 0 || __kw_beta) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta)) &&
          (__tuplecount > 1 || __kw_alpha) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_alpha)) &&
          (__tuplecount > 2 || __kw_mat1) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat1)) == THSPTensorClass &&
          (__tuplecount > 3 || __kw_mat2) && (PyObject*)Py_TYPE((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_mat2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THSTensor* arg_result = ((THSPTensor*)___out)->cdata;
      real arg_beta = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta));
      THSTensor* arg_self = ((THSPTensor*)self)->cdata;
      real arg_alpha = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_alpha));
      THSTensor* arg_mat1 = ((THSPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat1))->cdata;
      THTensor* arg_mat2 = ((THPTensor*)(__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_mat2))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(sspaddmm)(LIBRARY_STATE arg_result, arg_beta, arg_self, arg_alpha, arg_mat1, arg_mat2);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #else
    if (false) {
    #endif
#if !IS_DISTRIBUTED
          
    } else if (___out == NULL &&
          __argcount == 4 &&
          (__tuplecount > 0 || __kw_beta) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta)) &&
          (__tuplecount > 1 || __kw_alpha) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_alpha)) &&
          (__tuplecount > 2 || __kw_mat1) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat1)) == THSPTensorClass &&
          (__tuplecount > 3 || __kw_mat2) && (PyObject*)Py_TYPE((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_mat2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THSPTensorPtr _result_guard((THSPTensor*) THSPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THSPTensor* result = _result_guard.get();
      
      
      THSTensor* arg_result = ((THSPTensor*)result)->cdata;
      real arg_beta = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta));
      THSTensor* arg_self = ((THSPTensor*)self)->cdata;
      real arg_alpha = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_alpha));
      THSTensor* arg_mat1 = ((THSPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat1))->cdata;
      THTensor* arg_mat2 = ((THPTensor*)(__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_mat2))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(sspaddmm)(LIBRARY_STATE arg_result, arg_beta, arg_self, arg_alpha, arg_mat1, arg_mat2);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif
#if !IS_DISTRIBUTED
          
    } else if (___out != NULL &&
          __argcount == 4 &&
          (PyObject*)Py_TYPE(___out) == THSPTensorClass &&
          (__tuplecount > 0 || __kw_beta) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta)) &&
          (__tuplecount > 1 || __kw_mat1) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat1)) == THSPTensorClass &&
          (__tuplecount > 2 || __kw_mat2) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THSTensor* arg_result = ((THSPTensor*)___out)->cdata;
      real arg_beta = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta));
      THSTensor* arg_self = ((THSPTensor*)self)->cdata;
      THSTensor* arg_mat1 = ((THSPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat1))->cdata;
      THTensor* arg_mat2 = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat2))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(sspaddmm)(LIBRARY_STATE arg_result, arg_beta, arg_self, AS_REAL(1), arg_mat1, arg_mat2);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif
#if !IS_DISTRIBUTED
          
    } else if (___out != NULL &&
          __argcount == 4 &&
          (PyObject*)Py_TYPE(___out) == THSPTensorClass &&
          (__tuplecount > 0 || __kw_alpha) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_alpha)) &&
          (__tuplecount > 1 || __kw_mat1) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat1)) == THSPTensorClass &&
          (__tuplecount > 2 || __kw_mat2) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THSTensor* arg_result = ((THSPTensor*)___out)->cdata;
      THSTensor* arg_self = ((THSPTensor*)self)->cdata;
      real arg_alpha = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_alpha));
      THSTensor* arg_mat1 = ((THSPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat1))->cdata;
      THTensor* arg_mat2 = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat2))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(sspaddmm)(LIBRARY_STATE arg_result, AS_REAL(1), arg_self, arg_alpha, arg_mat1, arg_mat2);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif
#if !IS_DISTRIBUTED
          
    } else if (___out == NULL &&
          __argcount == 3 &&
          (__tuplecount > 0 || __kw_beta) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta)) &&
          (__tuplecount > 1 || __kw_mat1) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat1)) == THSPTensorClass &&
          (__tuplecount > 2 || __kw_mat2) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THSPTensorPtr _result_guard((THSPTensor*) THSPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THSPTensor* result = _result_guard.get();
      
      
      THSTensor* arg_result = ((THSPTensor*)result)->cdata;
      real arg_beta = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta));
      THSTensor* arg_self = ((THSPTensor*)self)->cdata;
      THSTensor* arg_mat1 = ((THSPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat1))->cdata;
      THTensor* arg_mat2 = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat2))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(sspaddmm)(LIBRARY_STATE arg_result, arg_beta, arg_self, AS_REAL(1), arg_mat1, arg_mat2);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif
#if !IS_DISTRIBUTED
          
    } else if (___out == NULL &&
          __argcount == 3 &&
          (__tuplecount > 0 || __kw_alpha) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_alpha)) &&
          (__tuplecount > 1 || __kw_mat1) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat1)) == THSPTensorClass &&
          (__tuplecount > 2 || __kw_mat2) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THSPTensorPtr _result_guard((THSPTensor*) THSPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THSPTensor* result = _result_guard.get();
      
      
      THSTensor* arg_result = ((THSPTensor*)result)->cdata;
      THSTensor* arg_self = ((THSPTensor*)self)->cdata;
      real arg_alpha = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_alpha));
      THSTensor* arg_mat1 = ((THSPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat1))->cdata;
      THTensor* arg_mat2 = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat2))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(sspaddmm)(LIBRARY_STATE arg_result, AS_REAL(1), arg_self, arg_alpha, arg_mat1, arg_mat2);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif
#if !IS_DISTRIBUTED
          
    } else if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THSPTensorClass &&
          (__tuplecount > 0 || __kw_mat1) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mat1)) == THSPTensorClass &&
          (__tuplecount > 1 || __kw_mat2) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THSTensor* arg_result = ((THSPTensor*)___out)->cdata;
      THSTensor* arg_self = ((THSPTensor*)self)->cdata;
      THSTensor* arg_mat1 = ((THSPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mat1))->cdata;
      THTensor* arg_mat2 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat2))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(sspaddmm)(LIBRARY_STATE arg_result, AS_REAL(1), arg_self, AS_REAL(1), arg_mat1, arg_mat2);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif
#if !IS_DISTRIBUTED
          
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_mat1) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mat1)) == THSPTensorClass &&
          (__tuplecount > 1 || __kw_mat2) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THSPTensorPtr _result_guard((THSPTensor*) THSPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THSPTensor* result = _result_guard.get();
      
      
      THSTensor* arg_result = ((THSPTensor*)result)->cdata;
      THSTensor* arg_self = ((THSPTensor*)self)->cdata;
      THSTensor* arg_mat1 = ((THSPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mat1))->cdata;
      THTensor* arg_mat2 = ((THPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat2))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(sspaddmm)(LIBRARY_STATE arg_result, AS_REAL(1), arg_self, AS_REAL(1), arg_mat1, arg_mat2);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif

    }

    THPUtils_invalidArguments(args, kwargs, "sspaddmm", 4, "(" THSPTensorStr " mat1, " THPTensorStr " mat2, #" THSPTensorStr " out)", "(" RealStr " beta, " THSPTensorStr " mat1, " THPTensorStr " mat2, #" THSPTensorStr " out)", "(" RealStr " alpha, " THSPTensorStr " mat1, " THPTensorStr " mat2, #" THSPTensorStr " out)", "(" RealStr " beta, " RealStr " alpha, " THSPTensorStr " mat1, " THPTensorStr " mat2, #" THSPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF)
PyObject * THSPTensor_stateless_(sspaddmm)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_beta = NULL;
    PyObject *__kw_source = NULL;
    PyObject *__kw_alpha = NULL;
    PyObject *__kw_mat1 = NULL;
    PyObject *__kw_mat2 = NULL;
    if (kwargs) {
      __kw_beta = PyDict_GetItemString(kwargs, "beta");
      __kw_source = PyDict_GetItemString(kwargs, "source");
      __kw_alpha = PyDict_GetItemString(kwargs, "alpha");
      __kw_mat1 = PyDict_GetItemString(kwargs, "mat1");
      __kw_mat2 = PyDict_GetItemString(kwargs, "mat2");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    #if !IS_DISTRIBUTED
          
    if (___out != NULL &&
          __argcount == 6 &&
          (PyObject*)Py_TYPE(___out) == THSPTensorClass &&
          (__tuplecount > 0 || __kw_beta) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta)) &&
          (__tuplecount > 1 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_source)) == THSPTensorClass &&
          (__tuplecount > 2 || __kw_alpha) && THPUtils_(checkReal)((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_alpha)) &&
          (__tuplecount > 3 || __kw_mat1) && (PyObject*)Py_TYPE((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_mat1)) == THSPTensorClass &&
          (__tuplecount > 4 || __kw_mat2) && (PyObject*)Py_TYPE((__tuplecount > 4 ? PyTuple_GET_ITEM(args, 4) : __kw_mat2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THSTensor* arg_result = ((THSPTensor*)___out)->cdata;
      real arg_beta = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta));
      THSTensor* arg_self = ((THSPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_source))->cdata;
      real arg_alpha = THPUtils_(unpackReal)((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_alpha));
      THSTensor* arg_mat1 = ((THSPTensor*)(__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_mat1))->cdata;
      THTensor* arg_mat2 = ((THPTensor*)(__tuplecount > 4 ? PyTuple_GET_ITEM(args, 4) : __kw_mat2))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(sspaddmm)(LIBRARY_STATE arg_result, arg_beta, arg_self, arg_alpha, arg_mat1, arg_mat2);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #else
    if (false) {
    #endif
#if !IS_DISTRIBUTED
          
    } else if (___out == NULL &&
          __argcount == 5 &&
          (__tuplecount > 0 || __kw_beta) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta)) &&
          (__tuplecount > 1 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_source)) == THSPTensorClass &&
          (__tuplecount > 2 || __kw_alpha) && THPUtils_(checkReal)((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_alpha)) &&
          (__tuplecount > 3 || __kw_mat1) && (PyObject*)Py_TYPE((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_mat1)) == THSPTensorClass &&
          (__tuplecount > 4 || __kw_mat2) && (PyObject*)Py_TYPE((__tuplecount > 4 ? PyTuple_GET_ITEM(args, 4) : __kw_mat2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THSPTensorPtr _result_guard((THSPTensor*) THSPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THSPTensor* result = _result_guard.get();
      
      
      THSTensor* arg_result = ((THSPTensor*)result)->cdata;
      real arg_beta = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta));
      THSTensor* arg_self = ((THSPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_source))->cdata;
      real arg_alpha = THPUtils_(unpackReal)((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_alpha));
      THSTensor* arg_mat1 = ((THSPTensor*)(__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_mat1))->cdata;
      THTensor* arg_mat2 = ((THPTensor*)(__tuplecount > 4 ? PyTuple_GET_ITEM(args, 4) : __kw_mat2))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(sspaddmm)(LIBRARY_STATE arg_result, arg_beta, arg_self, arg_alpha, arg_mat1, arg_mat2);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif
#if !IS_DISTRIBUTED
          
    } else if (___out != NULL &&
          __argcount == 5 &&
          (PyObject*)Py_TYPE(___out) == THSPTensorClass &&
          (__tuplecount > 0 || __kw_beta) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta)) &&
          (__tuplecount > 1 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_source)) == THSPTensorClass &&
          (__tuplecount > 2 || __kw_mat1) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat1)) == THSPTensorClass &&
          (__tuplecount > 3 || __kw_mat2) && (PyObject*)Py_TYPE((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_mat2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THSTensor* arg_result = ((THSPTensor*)___out)->cdata;
      real arg_beta = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta));
      THSTensor* arg_self = ((THSPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_source))->cdata;
      THSTensor* arg_mat1 = ((THSPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat1))->cdata;
      THTensor* arg_mat2 = ((THPTensor*)(__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_mat2))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(sspaddmm)(LIBRARY_STATE arg_result, arg_beta, arg_self, AS_REAL(1), arg_mat1, arg_mat2);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif
#if !IS_DISTRIBUTED
          
    } else if (___out != NULL &&
          __argcount == 5 &&
          (PyObject*)Py_TYPE(___out) == THSPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THSPTensorClass &&
          (__tuplecount > 1 || __kw_alpha) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_alpha)) &&
          (__tuplecount > 2 || __kw_mat1) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat1)) == THSPTensorClass &&
          (__tuplecount > 3 || __kw_mat2) && (PyObject*)Py_TYPE((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_mat2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THSTensor* arg_result = ((THSPTensor*)___out)->cdata;
      THSTensor* arg_self = ((THSPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      real arg_alpha = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_alpha));
      THSTensor* arg_mat1 = ((THSPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat1))->cdata;
      THTensor* arg_mat2 = ((THPTensor*)(__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_mat2))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(sspaddmm)(LIBRARY_STATE arg_result, AS_REAL(1), arg_self, arg_alpha, arg_mat1, arg_mat2);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif
#if !IS_DISTRIBUTED
          
    } else if (___out == NULL &&
          __argcount == 4 &&
          (__tuplecount > 0 || __kw_beta) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta)) &&
          (__tuplecount > 1 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_source)) == THSPTensorClass &&
          (__tuplecount > 2 || __kw_mat1) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat1)) == THSPTensorClass &&
          (__tuplecount > 3 || __kw_mat2) && (PyObject*)Py_TYPE((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_mat2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THSPTensorPtr _result_guard((THSPTensor*) THSPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THSPTensor* result = _result_guard.get();
      
      
      THSTensor* arg_result = ((THSPTensor*)result)->cdata;
      real arg_beta = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_beta));
      THSTensor* arg_self = ((THSPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_source))->cdata;
      THSTensor* arg_mat1 = ((THSPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat1))->cdata;
      THTensor* arg_mat2 = ((THPTensor*)(__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_mat2))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(sspaddmm)(LIBRARY_STATE arg_result, arg_beta, arg_self, AS_REAL(1), arg_mat1, arg_mat2);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif
#if !IS_DISTRIBUTED
          
    } else if (___out == NULL &&
          __argcount == 4 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THSPTensorClass &&
          (__tuplecount > 1 || __kw_alpha) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_alpha)) &&
          (__tuplecount > 2 || __kw_mat1) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat1)) == THSPTensorClass &&
          (__tuplecount > 3 || __kw_mat2) && (PyObject*)Py_TYPE((__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_mat2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THSPTensorPtr _result_guard((THSPTensor*) THSPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THSPTensor* result = _result_guard.get();
      
      
      THSTensor* arg_result = ((THSPTensor*)result)->cdata;
      THSTensor* arg_self = ((THSPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      real arg_alpha = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_alpha));
      THSTensor* arg_mat1 = ((THSPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat1))->cdata;
      THTensor* arg_mat2 = ((THPTensor*)(__tuplecount > 3 ? PyTuple_GET_ITEM(args, 3) : __kw_mat2))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(sspaddmm)(LIBRARY_STATE arg_result, AS_REAL(1), arg_self, arg_alpha, arg_mat1, arg_mat2);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif
#if !IS_DISTRIBUTED
          
    } else if (___out != NULL &&
          __argcount == 4 &&
          (PyObject*)Py_TYPE(___out) == THSPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THSPTensorClass &&
          (__tuplecount > 1 || __kw_mat1) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat1)) == THSPTensorClass &&
          (__tuplecount > 2 || __kw_mat2) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THSTensor* arg_result = ((THSPTensor*)___out)->cdata;
      THSTensor* arg_self = ((THSPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THSTensor* arg_mat1 = ((THSPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat1))->cdata;
      THTensor* arg_mat2 = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat2))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(sspaddmm)(LIBRARY_STATE arg_result, AS_REAL(1), arg_self, AS_REAL(1), arg_mat1, arg_mat2);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif
#if !IS_DISTRIBUTED
          
    } else if (___out == NULL &&
          __argcount == 3 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THSPTensorClass &&
          (__tuplecount > 1 || __kw_mat1) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat1)) == THSPTensorClass &&
          (__tuplecount > 2 || __kw_mat2) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat2)) == THPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THSPTensorPtr _result_guard((THSPTensor*) THSPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THSPTensor* result = _result_guard.get();
      
      
      THSTensor* arg_result = ((THSPTensor*)result)->cdata;
      THSTensor* arg_self = ((THSPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THSTensor* arg_mat1 = ((THSPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat1))->cdata;
      THTensor* arg_mat2 = ((THPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat2))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(sspaddmm)(LIBRARY_STATE arg_result, AS_REAL(1), arg_self, AS_REAL(1), arg_mat1, arg_mat2);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif

    }

    THPUtils_invalidArguments(args, kwargs, "torch.sspaddmm", 4, "(" THSPTensorStr " source, " THSPTensorStr " mat1, " THPTensorStr " mat2, #" THSPTensorStr " out)", "(" RealStr " beta, " THSPTensorStr " source, " THSPTensorStr " mat1, " THPTensorStr " mat2, #" THSPTensorStr " out)", "(" THSPTensorStr " source, " RealStr " alpha, " THSPTensorStr " mat1, " THPTensorStr " mat2, #" THSPTensorStr " out)", "(" RealStr " beta, " THSPTensorStr " source, " RealStr " alpha, " THSPTensorStr " mat1, " THPTensorStr " mat2, #" THSPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THSPTensor_(spadd)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_mat1 = NULL;
    PyObject *__kw_value = NULL;
    PyObject *__kw_mat2 = NULL;
    if (kwargs) {
      __kw_mat1 = PyDict_GetItemString(kwargs, "mat1");
      __kw_value = PyDict_GetItemString(kwargs, "value");
      __kw_mat2 = PyDict_GetItemString(kwargs, "mat2");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    #if !IS_DISTRIBUTED
          
    if (___out != NULL &&
          __argcount == 4 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_mat1) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mat1)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value)) &&
          (__tuplecount > 2 || __kw_mat2) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat2)) == THSPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_mat1 = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mat1))->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value));
      THSTensor* arg_mat2 = ((THSPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat2))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(spcadd)(LIBRARY_STATE arg_result, arg_mat1, arg_value, arg_mat2);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #else
    if (false) {
    #endif
#if !IS_DISTRIBUTED
          
    } else if (___out == NULL &&
          __argcount == 3 &&
          (__tuplecount > 0 || __kw_mat1) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mat1)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value)) &&
          (__tuplecount > 2 || __kw_mat2) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat2)) == THSPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_mat1 = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mat1))->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value));
      THSTensor* arg_mat2 = ((THSPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat2))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(spcadd)(LIBRARY_STATE arg_result, arg_mat1, arg_value, arg_mat2);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif
#if !IS_DISTRIBUTED
          
    } else if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_mat1) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mat1)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_mat2) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat2)) == THSPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_mat1 = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mat1))->cdata;
      THSTensor* arg_mat2 = ((THSPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat2))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(spcadd)(LIBRARY_STATE arg_result, arg_mat1, AS_REAL(1), arg_mat2);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif
#if !IS_DISTRIBUTED
          
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_mat1) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mat1)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_mat2) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat2)) == THSPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_mat1 = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mat1))->cdata;
      THSTensor* arg_mat2 = ((THSPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat2))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(spcadd)(LIBRARY_STATE arg_result, arg_mat1, AS_REAL(1), arg_mat2);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif

    }

    THPUtils_invalidArguments(args, kwargs, "spadd", 2, "(" THPTensorStr " mat1, " THSPTensorStr " mat2, #" THPTensorStr " out)", "(" THPTensorStr " mat1, " RealStr " value, " THSPTensorStr " mat2, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF)
PyObject * THSPTensor_stateless_(spadd)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_mat1 = NULL;
    PyObject *__kw_value = NULL;
    PyObject *__kw_mat2 = NULL;
    if (kwargs) {
      __kw_mat1 = PyDict_GetItemString(kwargs, "mat1");
      __kw_value = PyDict_GetItemString(kwargs, "value");
      __kw_mat2 = PyDict_GetItemString(kwargs, "mat2");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    #if !IS_DISTRIBUTED
          
    if (___out != NULL &&
          __argcount == 4 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_mat1) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mat1)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value)) &&
          (__tuplecount > 2 || __kw_mat2) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat2)) == THSPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_mat1 = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mat1))->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value));
      THSTensor* arg_mat2 = ((THSPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat2))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(spcadd)(LIBRARY_STATE arg_result, arg_mat1, arg_value, arg_mat2);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #else
    if (false) {
    #endif
#if !IS_DISTRIBUTED
          
    } else if (___out == NULL &&
          __argcount == 3 &&
          (__tuplecount > 0 || __kw_mat1) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mat1)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value)) &&
          (__tuplecount > 2 || __kw_mat2) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat2)) == THSPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_mat1 = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mat1))->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value));
      THSTensor* arg_mat2 = ((THSPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_mat2))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(spcadd)(LIBRARY_STATE arg_result, arg_mat1, arg_value, arg_mat2);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif
#if !IS_DISTRIBUTED
          
    } else if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THPTensorClass &&
          (__tuplecount > 0 || __kw_mat1) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mat1)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_mat2) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat2)) == THSPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THTensor* arg_result = ((THPTensor*)___out)->cdata;
      THTensor* arg_mat1 = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mat1))->cdata;
      THSTensor* arg_mat2 = ((THSPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat2))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(spcadd)(LIBRARY_STATE arg_result, arg_mat1, AS_REAL(1), arg_mat2);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif
#if !IS_DISTRIBUTED
          
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_mat1) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mat1)) == THPTensorClass &&
          (__tuplecount > 1 || __kw_mat2) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat2)) == THSPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THPTensorPtr _result_guard((THPTensor*) THPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THPTensor* result = _result_guard.get();
      
      
      THTensor* arg_result = ((THPTensor*)result)->cdata;
      THTensor* arg_mat1 = ((THPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mat1))->cdata;
      THSTensor* arg_mat2 = ((THSPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_mat2))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(spcadd)(LIBRARY_STATE arg_result, arg_mat1, AS_REAL(1), arg_mat2);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif

    }

    THPUtils_invalidArguments(args, kwargs, "torch.spadd", 2, "(" THPTensorStr " mat1, " THSPTensorStr " mat2, #" THPTensorStr " out)", "(" THPTensorStr " mat1, " RealStr " value, " THSPTensorStr " mat2, #" THPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THSPTensor_(zero_)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    
    if (kwargs) {
      
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    #if !IS_DISTRIBUTED
          
    if (__argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THSTensor* arg_self = ((THSPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(zero)(LIBRARY_STATE arg_self);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)self;
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #else
    if (false) {
    #endif

    }

    THPUtils_invalidArguments(args, kwargs, "zero_", 1, "no arguments");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THSPTensor_stateless_(zeros)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    
    if (kwargs) {
      
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    THLongStoragePtr __size;
PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    #if !IS_DISTRIBUTED
          
    if (__dictcount == 1 &&
          ___out != NULL &&
          __argcount >= 2 &&
          (PyObject*)Py_TYPE(___out) == THSPTensorClass &&
          THPUtils_tryUnpackLongVarArgs(args, 0, __size)) {
      
      
      THSTensor* arg_result = ((THSPTensor*)___out)->cdata;
      THSize* arg_size = __size.get();
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(zeros)(LIBRARY_STATE arg_result, arg_size);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #else
    if (false) {
    #endif
#if !IS_DISTRIBUTED
          
    } else if (__dictcount == 0 &&
          ___out == NULL &&
          __argcount >= 1 &&
          THPUtils_tryUnpackLongVarArgs(args, 0, __size)) {
      
      THSPTensorPtr _result_guard((THSPTensor*) THSPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THSPTensor* result = _result_guard.get();
      
      
      THSTensor* arg_result = ((THSPTensor*)result)->cdata;
      THSize* arg_size = __size.get();
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(zeros)(LIBRARY_STATE arg_result, arg_size);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif

    }

    THPUtils_invalidArguments(args, kwargs, "torch.zeros", 2, "(int ... size, #" THSPTensorStr " out)", "(torch.Size size, #" THSPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THSPTensor_stateless_(zeros_like)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_input = NULL;
    if (kwargs) {
      __kw_input = PyDict_GetItemString(kwargs, "input");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    #if !IS_DISTRIBUTED
          
    if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THSPTensorClass &&
          (__tuplecount > 0 || __kw_input) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_input)) == THSPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THSTensor* arg_result = ((THSPTensor*)___out)->cdata;
      THSTensor* arg_input = ((THSPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_input))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(zerosLike)(LIBRARY_STATE arg_result, arg_input);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #else
    if (false) {
    #endif
#if !IS_DISTRIBUTED
          
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_input) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_input)) == THSPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THSPTensorPtr _result_guard((THSPTensor*) THSPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THSPTensor* result = _result_guard.get();
      
      
      THSTensor* arg_result = ((THSPTensor*)result)->cdata;
      THSTensor* arg_input = ((THSPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_input))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(zerosLike)(LIBRARY_STATE arg_result, arg_input);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif

    }

    THPUtils_invalidArguments(args, kwargs, "torch.zeros_like", 1, "(" THSPTensorStr " input, #" THSPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THSPTensor_(add)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_value = NULL;
    PyObject *__kw_other = NULL;
    if (kwargs) {
      __kw_value = PyDict_GetItemString(kwargs, "value");
      __kw_other = PyDict_GetItemString(kwargs, "other");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    #if !IS_DISTRIBUTED
          
    if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THSPTensorClass &&
          (__tuplecount > 0 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value)) &&
          (__tuplecount > 1 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other)) == THSPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THSTensor* arg_result = ((THSPTensor*)___out)->cdata;
      THSTensor* arg_self = ((THSPTensor*)self)->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value));
      THSTensor* arg_other = ((THSPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(cadd)(LIBRARY_STATE arg_result, arg_self, arg_value, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #else
    if (false) {
    #endif
#if !IS_DISTRIBUTED
          
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value)) &&
          (__tuplecount > 1 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other)) == THSPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THSPTensorPtr _result_guard((THSPTensor*) THSPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THSPTensor* result = _result_guard.get();
      
      
      THSTensor* arg_result = ((THSPTensor*)result)->cdata;
      THSTensor* arg_self = ((THSPTensor*)self)->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value));
      THSTensor* arg_other = ((THSPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(cadd)(LIBRARY_STATE arg_result, arg_self, arg_value, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif
#if !IS_DISTRIBUTED
          
    } else if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THSPTensorClass &&
          (__tuplecount > 0 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other)) == THSPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THSTensor* arg_result = ((THSPTensor*)___out)->cdata;
      THSTensor* arg_self = ((THSPTensor*)self)->cdata;
      THSTensor* arg_other = ((THSPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(cadd)(LIBRARY_STATE arg_result, arg_self, AS_REAL(1), arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif
#if !IS_DISTRIBUTED
          
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other)) == THSPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THSPTensorPtr _result_guard((THSPTensor*) THSPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THSPTensor* result = _result_guard.get();
      
      
      THSTensor* arg_result = ((THSPTensor*)result)->cdata;
      THSTensor* arg_self = ((THSPTensor*)self)->cdata;
      THSTensor* arg_other = ((THSPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(cadd)(LIBRARY_STATE arg_result, arg_self, AS_REAL(1), arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif

    }

    THPUtils_invalidArguments(args, kwargs, "add", 2, "(" THSPTensorStr " other, #" THSPTensorStr " out)", "(" RealStr " value, " THSPTensorStr " other, #" THSPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF)
PyObject * THSPTensor_stateless_(add)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    PyObject *__kw_value = NULL;
    PyObject *__kw_other = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
      __kw_value = PyDict_GetItemString(kwargs, "value");
      __kw_other = PyDict_GetItemString(kwargs, "other");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    #if !IS_DISTRIBUTED
          
    if (___out != NULL &&
          __argcount == 4 &&
          (PyObject*)Py_TYPE(___out) == THSPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THSPTensorClass &&
          (__tuplecount > 1 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value)) &&
          (__tuplecount > 2 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_other)) == THSPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THSTensor* arg_result = ((THSPTensor*)___out)->cdata;
      THSTensor* arg_self = ((THSPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value));
      THSTensor* arg_other = ((THSPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_other))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(cadd)(LIBRARY_STATE arg_result, arg_self, arg_value, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #else
    if (false) {
    #endif
#if !IS_DISTRIBUTED
          
    } else if (___out == NULL &&
          __argcount == 3 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THSPTensorClass &&
          (__tuplecount > 1 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value)) &&
          (__tuplecount > 2 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_other)) == THSPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THSPTensorPtr _result_guard((THSPTensor*) THSPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THSPTensor* result = _result_guard.get();
      
      
      THSTensor* arg_result = ((THSPTensor*)result)->cdata;
      THSTensor* arg_self = ((THSPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value));
      THSTensor* arg_other = ((THSPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_other))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(cadd)(LIBRARY_STATE arg_result, arg_self, arg_value, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif
#if !IS_DISTRIBUTED
          
    } else if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THSPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THSPTensorClass &&
          (__tuplecount > 1 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other)) == THSPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THSTensor* arg_result = ((THSPTensor*)___out)->cdata;
      THSTensor* arg_self = ((THSPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THSTensor* arg_other = ((THSPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(cadd)(LIBRARY_STATE arg_result, arg_self, AS_REAL(1), arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif
#if !IS_DISTRIBUTED
          
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THSPTensorClass &&
          (__tuplecount > 1 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other)) == THSPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THSPTensorPtr _result_guard((THSPTensor*) THSPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THSPTensor* result = _result_guard.get();
      
      
      THSTensor* arg_result = ((THSPTensor*)result)->cdata;
      THSTensor* arg_self = ((THSPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THSTensor* arg_other = ((THSPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(cadd)(LIBRARY_STATE arg_result, arg_self, AS_REAL(1), arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif

    }

    THPUtils_invalidArguments(args, kwargs, "torch.add", 2, "(" THSPTensorStr " source, " THSPTensorStr " other, #" THSPTensorStr " out)", "(" THSPTensorStr " source, " RealStr " value, " THSPTensorStr " other, #" THSPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THSPTensor_(add_)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_value = NULL;
    PyObject *__kw_other = NULL;
    if (kwargs) {
      __kw_value = PyDict_GetItemString(kwargs, "value");
      __kw_other = PyDict_GetItemString(kwargs, "other");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    #if !IS_DISTRIBUTED
          
    if (__argcount == 2 &&
          (__tuplecount > 0 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value)) &&
          (__tuplecount > 1 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other)) == THSPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THSTensor* arg_self = ((THSPTensor*)self)->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value));
      THSTensor* arg_other = ((THSPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(cadd)(LIBRARY_STATE arg_self, arg_self, arg_value, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)(self);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #else
    if (false) {
    #endif
#if !IS_DISTRIBUTED
          
    } else if (__argcount == 1 &&
          (__tuplecount > 0 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other)) == THSPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THSTensor* arg_self = ((THSPTensor*)self)->cdata;
      THSTensor* arg_other = ((THSPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(cadd)(LIBRARY_STATE arg_self, arg_self, AS_REAL(1), arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)(self);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif

    }

    THPUtils_invalidArguments(args, kwargs, "add_", 2, "(" THSPTensorStr " other)", "(" RealStr " value, " THSPTensorStr " other)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THSPTensor_(sub)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_value = NULL;
    PyObject *__kw_other = NULL;
    if (kwargs) {
      __kw_value = PyDict_GetItemString(kwargs, "value");
      __kw_other = PyDict_GetItemString(kwargs, "other");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    #if !IS_DISTRIBUTED
          
    if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THSPTensorClass &&
          (__tuplecount > 0 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value)) &&
          (__tuplecount > 1 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other)) == THSPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THSTensor* arg_result = ((THSPTensor*)___out)->cdata;
      THSTensor* arg_self = ((THSPTensor*)self)->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value));
      THSTensor* arg_other = ((THSPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(csub)(LIBRARY_STATE arg_result, arg_self, arg_value, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #else
    if (false) {
    #endif
#if !IS_DISTRIBUTED
          
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value)) &&
          (__tuplecount > 1 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other)) == THSPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THSPTensorPtr _result_guard((THSPTensor*) THSPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THSPTensor* result = _result_guard.get();
      
      
      THSTensor* arg_result = ((THSPTensor*)result)->cdata;
      THSTensor* arg_self = ((THSPTensor*)self)->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value));
      THSTensor* arg_other = ((THSPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(csub)(LIBRARY_STATE arg_result, arg_self, arg_value, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif
#if !IS_DISTRIBUTED
          
    } else if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THSPTensorClass &&
          (__tuplecount > 0 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other)) == THSPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THSTensor* arg_result = ((THSPTensor*)___out)->cdata;
      THSTensor* arg_self = ((THSPTensor*)self)->cdata;
      THSTensor* arg_other = ((THSPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(csub)(LIBRARY_STATE arg_result, arg_self, AS_REAL(1), arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif
#if !IS_DISTRIBUTED
          
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other)) == THSPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THSPTensorPtr _result_guard((THSPTensor*) THSPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THSPTensor* result = _result_guard.get();
      
      
      THSTensor* arg_result = ((THSPTensor*)result)->cdata;
      THSTensor* arg_self = ((THSPTensor*)self)->cdata;
      THSTensor* arg_other = ((THSPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(csub)(LIBRARY_STATE arg_result, arg_self, AS_REAL(1), arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif

    }

    THPUtils_invalidArguments(args, kwargs, "sub", 2, "(" THSPTensorStr " other, #" THSPTensorStr " out)", "(" RealStr " value, " THSPTensorStr " other, #" THSPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF)
PyObject * THSPTensor_stateless_(sub)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    PyObject *__kw_value = NULL;
    PyObject *__kw_other = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
      __kw_value = PyDict_GetItemString(kwargs, "value");
      __kw_other = PyDict_GetItemString(kwargs, "other");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    #if !IS_DISTRIBUTED
          
    if (___out != NULL &&
          __argcount == 4 &&
          (PyObject*)Py_TYPE(___out) == THSPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THSPTensorClass &&
          (__tuplecount > 1 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value)) &&
          (__tuplecount > 2 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_other)) == THSPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THSTensor* arg_result = ((THSPTensor*)___out)->cdata;
      THSTensor* arg_self = ((THSPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value));
      THSTensor* arg_other = ((THSPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_other))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(csub)(LIBRARY_STATE arg_result, arg_self, arg_value, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #else
    if (false) {
    #endif
#if !IS_DISTRIBUTED
          
    } else if (___out == NULL &&
          __argcount == 3 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THSPTensorClass &&
          (__tuplecount > 1 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value)) &&
          (__tuplecount > 2 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_other)) == THSPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THSPTensorPtr _result_guard((THSPTensor*) THSPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THSPTensor* result = _result_guard.get();
      
      
      THSTensor* arg_result = ((THSPTensor*)result)->cdata;
      THSTensor* arg_self = ((THSPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value));
      THSTensor* arg_other = ((THSPTensor*)(__tuplecount > 2 ? PyTuple_GET_ITEM(args, 2) : __kw_other))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(csub)(LIBRARY_STATE arg_result, arg_self, arg_value, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif
#if !IS_DISTRIBUTED
          
    } else if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THSPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THSPTensorClass &&
          (__tuplecount > 1 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other)) == THSPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THSTensor* arg_result = ((THSPTensor*)___out)->cdata;
      THSTensor* arg_self = ((THSPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THSTensor* arg_other = ((THSPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(csub)(LIBRARY_STATE arg_result, arg_self, AS_REAL(1), arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif
#if !IS_DISTRIBUTED
          
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THSPTensorClass &&
          (__tuplecount > 1 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other)) == THSPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THSPTensorPtr _result_guard((THSPTensor*) THSPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THSPTensor* result = _result_guard.get();
      
      
      THSTensor* arg_result = ((THSPTensor*)result)->cdata;
      THSTensor* arg_self = ((THSPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THSTensor* arg_other = ((THSPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(csub)(LIBRARY_STATE arg_result, arg_self, AS_REAL(1), arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif

    }

    THPUtils_invalidArguments(args, kwargs, "torch.sub", 2, "(" THSPTensorStr " source, " THSPTensorStr " other, #" THSPTensorStr " out)", "(" THSPTensorStr " source, " RealStr " value, " THSPTensorStr " other, #" THSPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THSPTensor_(sub_)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_value = NULL;
    PyObject *__kw_other = NULL;
    if (kwargs) {
      __kw_value = PyDict_GetItemString(kwargs, "value");
      __kw_other = PyDict_GetItemString(kwargs, "other");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    #if !IS_DISTRIBUTED
          
    if (__argcount == 2 &&
          (__tuplecount > 0 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value)) &&
          (__tuplecount > 1 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other)) == THSPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THSTensor* arg_self = ((THSPTensor*)self)->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value));
      THSTensor* arg_other = ((THSPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(csub)(LIBRARY_STATE arg_self, arg_self, arg_value, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)(self);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #else
    if (false) {
    #endif
#if !IS_DISTRIBUTED
          
    } else if (__argcount == 1 &&
          (__tuplecount > 0 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other)) == THSPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THSTensor* arg_self = ((THSPTensor*)self)->cdata;
      THSTensor* arg_other = ((THSPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(csub)(LIBRARY_STATE arg_self, arg_self, AS_REAL(1), arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)(self);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif

    }

    THPUtils_invalidArguments(args, kwargs, "sub_", 2, "(" THSPTensorStr " other)", "(" RealStr " value, " THSPTensorStr " other)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THSPTensor_(mul)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_value = NULL;
    PyObject *__kw_other = NULL;
    if (kwargs) {
      __kw_value = PyDict_GetItemString(kwargs, "value");
      __kw_other = PyDict_GetItemString(kwargs, "other");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    #if !IS_DISTRIBUTED
          
    if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THSPTensorClass &&
          (__tuplecount > 0 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THSTensor* arg_result = ((THSPTensor*)___out)->cdata;
      THSTensor* arg_self = ((THSPTensor*)self)->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(mul)(LIBRARY_STATE arg_result, arg_self, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #else
    if (false) {
    #endif
#if !IS_DISTRIBUTED
          
    } else if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THSPTensorClass &&
          (__tuplecount > 0 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other)) == THSPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THSTensor* arg_result = ((THSPTensor*)___out)->cdata;
      THSTensor* arg_self = ((THSPTensor*)self)->cdata;
      THSTensor* arg_other = ((THSPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(cmul)(LIBRARY_STATE arg_result, arg_self, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif
#if !IS_DISTRIBUTED
          
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THSPTensorPtr _result_guard((THSPTensor*) THSPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THSPTensor* result = _result_guard.get();
      
      
      THSTensor* arg_result = ((THSPTensor*)result)->cdata;
      THSTensor* arg_self = ((THSPTensor*)self)->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(mul)(LIBRARY_STATE arg_result, arg_self, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif
#if !IS_DISTRIBUTED
          
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other)) == THSPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THSPTensorPtr _result_guard((THSPTensor*) THSPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THSPTensor* result = _result_guard.get();
      
      
      THSTensor* arg_result = ((THSPTensor*)result)->cdata;
      THSTensor* arg_self = ((THSPTensor*)self)->cdata;
      THSTensor* arg_other = ((THSPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(cmul)(LIBRARY_STATE arg_result, arg_self, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif

    }

    THPUtils_invalidArguments(args, kwargs, "mul", 2, "(" RealStr " value, #" THSPTensorStr " out)", "(" THSPTensorStr " other, #" THSPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF)
PyObject * THSPTensor_stateless_(mul)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    PyObject *__kw_value = NULL;
    PyObject *__kw_other = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
      __kw_value = PyDict_GetItemString(kwargs, "value");
      __kw_other = PyDict_GetItemString(kwargs, "other");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    #if !IS_DISTRIBUTED
          
    if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THSPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THSPTensorClass &&
          (__tuplecount > 1 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THSTensor* arg_result = ((THSPTensor*)___out)->cdata;
      THSTensor* arg_self = ((THSPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(mul)(LIBRARY_STATE arg_result, arg_self, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #else
    if (false) {
    #endif
#if !IS_DISTRIBUTED
          
    } else if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THSPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THSPTensorClass &&
          (__tuplecount > 1 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other)) == THSPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THSTensor* arg_result = ((THSPTensor*)___out)->cdata;
      THSTensor* arg_self = ((THSPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THSTensor* arg_other = ((THSPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(cmul)(LIBRARY_STATE arg_result, arg_self, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif
#if !IS_DISTRIBUTED
          
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THSPTensorClass &&
          (__tuplecount > 1 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THSPTensorPtr _result_guard((THSPTensor*) THSPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THSPTensor* result = _result_guard.get();
      
      
      THSTensor* arg_result = ((THSPTensor*)result)->cdata;
      THSTensor* arg_self = ((THSPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(mul)(LIBRARY_STATE arg_result, arg_self, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif
#if !IS_DISTRIBUTED
          
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THSPTensorClass &&
          (__tuplecount > 1 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other)) == THSPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THSPTensorPtr _result_guard((THSPTensor*) THSPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THSPTensor* result = _result_guard.get();
      
      
      THSTensor* arg_result = ((THSPTensor*)result)->cdata;
      THSTensor* arg_self = ((THSPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      THSTensor* arg_other = ((THSPTensor*)(__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_other))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(cmul)(LIBRARY_STATE arg_result, arg_self, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif

    }

    THPUtils_invalidArguments(args, kwargs, "torch.mul", 2, "(" THSPTensorStr " source, " RealStr " value, #" THSPTensorStr " out)", "(" THSPTensorStr " source, " THSPTensorStr " other, #" THSPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THSPTensor_(mul_)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_value = NULL;
    PyObject *__kw_other = NULL;
    if (kwargs) {
      __kw_value = PyDict_GetItemString(kwargs, "value");
      __kw_other = PyDict_GetItemString(kwargs, "other");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    #if !IS_DISTRIBUTED
          
    if (__argcount == 1 &&
          (__tuplecount > 0 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THSTensor* arg_self = ((THSPTensor*)self)->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(mul)(LIBRARY_STATE arg_self, arg_self, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)(self);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #else
    if (false) {
    #endif
#if !IS_DISTRIBUTED
          
    } else if (__argcount == 1 &&
          (__tuplecount > 0 || __kw_other) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other)) == THSPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THSTensor* arg_self = ((THSPTensor*)self)->cdata;
      THSTensor* arg_other = ((THSPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_other))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(cmul)(LIBRARY_STATE arg_self, arg_self, arg_other);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)(self);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif

    }

    THPUtils_invalidArguments(args, kwargs, "mul_", 2, "(" RealStr " value)", "(" THSPTensorStr " other)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THSPTensor_(div)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_value = NULL;
    if (kwargs) {
      __kw_value = PyDict_GetItemString(kwargs, "value");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    #if !IS_DISTRIBUTED
          
    if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THSPTensorClass &&
          (__tuplecount > 0 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THSTensor* arg_result = ((THSPTensor*)___out)->cdata;
      THSTensor* arg_self = ((THSPTensor*)self)->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(div)(LIBRARY_STATE arg_result, arg_self, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #else
    if (false) {
    #endif
#if !IS_DISTRIBUTED
          
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THSPTensorPtr _result_guard((THSPTensor*) THSPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THSPTensor* result = _result_guard.get();
      
      
      THSTensor* arg_result = ((THSPTensor*)result)->cdata;
      THSTensor* arg_self = ((THSPTensor*)self)->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(div)(LIBRARY_STATE arg_result, arg_self, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif

    }

    THPUtils_invalidArguments(args, kwargs, "div", 1, "(" RealStr " value, #" THSPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF)
PyObject * THSPTensor_stateless_(div)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    PyObject *__kw_value = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
      __kw_value = PyDict_GetItemString(kwargs, "value");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    #if !IS_DISTRIBUTED
          
    if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THSPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THSPTensorClass &&
          (__tuplecount > 1 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THSTensor* arg_result = ((THSPTensor*)___out)->cdata;
      THSTensor* arg_self = ((THSPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(div)(LIBRARY_STATE arg_result, arg_self, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #else
    if (false) {
    #endif
#if !IS_DISTRIBUTED
          
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THSPTensorClass &&
          (__tuplecount > 1 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THSPTensorPtr _result_guard((THSPTensor*) THSPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THSPTensor* result = _result_guard.get();
      
      
      THSTensor* arg_result = ((THSPTensor*)result)->cdata;
      THSTensor* arg_self = ((THSPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(div)(LIBRARY_STATE arg_result, arg_self, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif

    }

    THPUtils_invalidArguments(args, kwargs, "torch.div", 1, "(" THSPTensorStr " source, " RealStr " value, #" THSPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF)
PyObject * THSPTensor_(div_)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_value = NULL;
    if (kwargs) {
      __kw_value = PyDict_GetItemString(kwargs, "value");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    #if !IS_DISTRIBUTED
          
    if (__argcount == 1 &&
          (__tuplecount > 0 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THSTensor* arg_self = ((THSPTensor*)self)->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(div)(LIBRARY_STATE arg_self, arg_self, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(self);
        return (PyObject*)(self);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #else
    if (false) {
    #endif

    }

    THPUtils_invalidArguments(args, kwargs, "div_", 1, "(" RealStr " value)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_FLOAT) || defined(TH_REAL_IS_DOUBLE) || CUDA_FLOAT || CUDA_DOUBLE)
PyObject * THSPTensor_(norm)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_value = NULL;
    if (kwargs) {
      __kw_value = PyDict_GetItemString(kwargs, "value");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    #if !IS_DISTRIBUTED
          
    if (__argcount == 1 &&
          (__tuplecount > 0 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THSTensor* arg_self = ((THSPTensor*)self)->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        real __result = THSTensor_(normall)(LIBRARY_STATE arg_self, arg_value);
        Py_BLOCK_THREADS;
        return THPUtils_(newReal)(__result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #else
    if (false) {
    #endif
#if !IS_DISTRIBUTED
          
    } else if (__argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THSTensor* arg_self = ((THSPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        real __result = THSTensor_(normall)(LIBRARY_STATE arg_self, AS_REAL(2));
        Py_BLOCK_THREADS;
        return THPUtils_(newReal)(__result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif

    }

    THPUtils_invalidArguments(args, kwargs, "norm", 2, "no arguments", "(" RealStr " value)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_FLOAT) || defined(TH_REAL_IS_DOUBLE) || CUDA_FLOAT || CUDA_DOUBLE)
PyObject * THSPTensor_stateless_(norm)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    PyObject *__kw_value = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
      __kw_value = PyDict_GetItemString(kwargs, "value");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    #if !IS_DISTRIBUTED
          
    if (__argcount == 2 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THSPTensorClass &&
          (__tuplecount > 1 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THSTensor* arg_self = ((THSPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        real __result = THSTensor_(normall)(LIBRARY_STATE arg_self, arg_value);
        Py_BLOCK_THREADS;
        return THPUtils_(newReal)(__result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #else
    if (false) {
    #endif
#if !IS_DISTRIBUTED
          
    } else if (__argcount == 1 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THSPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THSTensor* arg_self = ((THSPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        real __result = THSTensor_(normall)(LIBRARY_STATE arg_self, AS_REAL(2));
        Py_BLOCK_THREADS;
        return THPUtils_(newReal)(__result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif

    }

    THPUtils_invalidArguments(args, kwargs, "torch.norm", 2, "(" THSPTensorStr " source)", "(" THSPTensorStr " source, " RealStr " value)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_FLOAT) || defined(TH_REAL_IS_DOUBLE) || CUDA_FLOAT || CUDA_DOUBLE)
PyObject * THSPTensor_(pow)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_value = NULL;
    if (kwargs) {
      __kw_value = PyDict_GetItemString(kwargs, "value");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    #if !IS_DISTRIBUTED
          
    if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THSPTensorClass &&
          (__tuplecount > 0 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THSTensor* arg_result = ((THSPTensor*)___out)->cdata;
      THSTensor* arg_self = ((THSPTensor*)self)->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(pow)(LIBRARY_STATE arg_result, arg_self, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #else
    if (false) {
    #endif
#if !IS_DISTRIBUTED
          
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THSPTensorPtr _result_guard((THSPTensor*) THSPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THSPTensor* result = _result_guard.get();
      
      
      THSTensor* arg_result = ((THSPTensor*)result)->cdata;
      THSTensor* arg_self = ((THSPTensor*)self)->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(pow)(LIBRARY_STATE arg_result, arg_self, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif

    }

    THPUtils_invalidArguments(args, kwargs, "pow", 1, "(" RealStr " value, #" THSPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif

#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_FLOAT) || defined(TH_REAL_IS_DOUBLE) || CUDA_FLOAT || CUDA_DOUBLE)
PyObject * THSPTensor_stateless_(pow)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_source = NULL;
    PyObject *__kw_value = NULL;
    if (kwargs) {
      __kw_source = PyDict_GetItemString(kwargs, "source");
      __kw_value = PyDict_GetItemString(kwargs, "value");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    #if !IS_DISTRIBUTED
          
    if (___out != NULL &&
          __argcount == 3 &&
          (PyObject*)Py_TYPE(___out) == THSPTensorClass &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THSPTensorClass &&
          (__tuplecount > 1 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THSTensor* arg_result = ((THSPTensor*)___out)->cdata;
      THSTensor* arg_self = ((THSPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(pow)(LIBRARY_STATE arg_result, arg_self, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #else
    if (false) {
    #endif
#if !IS_DISTRIBUTED
          
    } else if (___out == NULL &&
          __argcount == 2 &&
          (__tuplecount > 0 || __kw_source) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source)) == THSPTensorClass &&
          (__tuplecount > 1 || __kw_value) && THPUtils_(checkReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value))) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THSPTensorPtr _result_guard((THSPTensor*) THSPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THSPTensor* result = _result_guard.get();
      
      
      THSTensor* arg_result = ((THSPTensor*)result)->cdata;
      THSTensor* arg_self = ((THSPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_source))->cdata;
      real arg_value = THPUtils_(unpackReal)((__tuplecount > 1 ? PyTuple_GET_ITEM(args, 1) : __kw_value));
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THSTensor_(pow)(LIBRARY_STATE arg_result, arg_self, arg_value);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif

    }

    THPUtils_invalidArguments(args, kwargs, "torch.pow", 1, "(" THSPTensorStr " source, " RealStr " value, #" THSPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF) && (!IS_DISTRIBUTED)
PyObject * THPTensor_(_sparse_mask)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    PyObject *__kw_mask = NULL;
    if (kwargs) {
      __kw_mask = PyDict_GetItemString(kwargs, "mask");
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    PyObject *___out;
    
    ___out = kwargs ? PyDict_GetItemString(kwargs, "out") : NULL;
    if (___out == Py_None) { ___out = NULL; __dictcount--; __argcount--; }
    

    #if !IS_DISTRIBUTED
          
    if (___out != NULL &&
          __argcount == 2 &&
          (PyObject*)Py_TYPE(___out) == THSPTensorClass &&
          (__tuplecount > 0 || __kw_mask) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mask)) == THSPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THSTensor* arg_result = ((THSPTensor*)___out)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THSTensor* arg_mask = ((THSPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mask))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(sparseMask)(LIBRARY_STATE arg_result, arg_self, arg_mask);
        Py_BLOCK_THREADS;
        Py_INCREF(___out);
        return (PyObject*)(___out);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #else
    if (false) {
    #endif
#if !IS_DISTRIBUTED
          
    } else if (___out == NULL &&
          __argcount == 1 &&
          (__tuplecount > 0 || __kw_mask) && (PyObject*)Py_TYPE((__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mask)) == THSPTensorClass) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      THSPTensorPtr _result_guard((THSPTensor*) THSPTensor_(NewEmpty)());
      if (!_result_guard.get()) return NULL;
      THSPTensor* result = _result_guard.get();
      
      
      THSTensor* arg_result = ((THSPTensor*)result)->cdata;
      THTensor* arg_self = ((THPTensor*)self)->cdata;
      THSTensor* arg_mask = ((THSPTensor*)(__tuplecount > 0 ? PyTuple_GET_ITEM(args, 0) : __kw_mask))->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        THTensor_(sparseMask)(LIBRARY_STATE arg_result, arg_self, arg_mask);
        Py_BLOCK_THREADS;
        Py_INCREF(result);
        return (PyObject*)(result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #endif

    }

    THPUtils_invalidArguments(args, kwargs, "_sparse_mask", 1, "(" THSPTensorStr " mask, #" THSPTensorStr " out)");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#if !defined(TH_REAL_IS_HALF) && (IS_CUDA)
PyObject * THSPTensor_(getDevice)(PyObject *self, PyObject *args, PyObject *kwargs)
{
    
    if (kwargs) {
      
    }
    
    HANDLE_TH_ERRORS
    int __tuplecount = args ? (int) PyTuple_Size(args) : 0;
    int __dictcount = kwargs ? (int) PyDict_Size(kwargs) : 0;
    int __argcount = __tuplecount + __dictcount;
    
    

    #if !IS_DISTRIBUTED
          
    if (__argcount == 0) {
      
      #if IS_CUDA
      THCPAutoGPU __autogpu_guard = THCPAutoGPU(args, (PyObject*)self);
      #endif
      
      
      THSTensor* arg_self = ((THSPTensor*)self)->cdata;
      
      PyThreadState *_save = NULL;
      try {
        Py_UNBLOCK_THREADS;
        int64_t __result = THSTensor_(getDevice)(LIBRARY_STATE arg_self);
        Py_BLOCK_THREADS;
        return PyInt_FromLong(__result);
      } catch (...) {
        if (_save) {
          Py_BLOCK_THREADS;
        }
        throw;
      }
    
    #else
    if (false) {
    #endif

    }

    THPUtils_invalidArguments(args, kwargs, "get_device", 1, "no arguments");
    return NULL;
    END_HANDLE_TH_ERRORS
}
    #endif


#endif

// cwrap should put definitions before undefs, so let's mark this place

static PyMethodDef THPTensor_(methods)[] = {
      {"element_size", (PyCFunction)THPTensor_(elementSize), METH_VARARGS, NULL},
  {"storage", (PyCFunction)THPTensor_(storage), METH_VARARGS, NULL},
  {"storage_offset", (PyCFunction)THPTensor_(storageOffset), METH_VARARGS | METH_KEYWORDS, NULL},
  {"ndimension", (PyCFunction)THPTensor_(nDimension), METH_VARARGS | METH_KEYWORDS, NULL},
  {"dim", (PyCFunction)THPTensor_(nDimension), METH_VARARGS | METH_KEYWORDS, NULL},
#if !defined(TH_REAL_IS_HALF)
  {"index", (PyCFunction)THPTensor_(getValue)<true>, METH_O, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"_set_index", (PyCFunction)THPTensor_(setIndex), METH_VARARGS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"_check_advanced_indexing", (PyCFunction)THPTensor_(checkAdvancedIndexing), METH_O, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"_advanced_index_add", (PyCFunction)THPTensor_(advancedIndexAdd), METH_VARARGS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"_advanced_index_select", (PyCFunction)THPTensor_(advancedIndexSelect), METH_VARARGS, NULL},
#endif
  {"resize_", (PyCFunction)THPTensor_(resize_), METH_VARARGS | METH_KEYWORDS, NULL},
  {"numel", (PyCFunction)THPTensor_(numel), METH_VARARGS | METH_KEYWORDS, NULL},
  {"nelement", (PyCFunction)THPTensor_(numel), METH_VARARGS | METH_KEYWORDS, NULL},
  {"set_", (PyCFunction)THPTensor_(set_), METH_VARARGS | METH_KEYWORDS, NULL},
  {"select", (PyCFunction)THPTensor_(select), METH_VARARGS, NULL},
  {"size", (PyCFunction)THPTensor_(size), METH_VARARGS | METH_KEYWORDS, NULL},
  {"stride", (PyCFunction)THPTensor_(stride), METH_VARARGS | METH_KEYWORDS, NULL},
#if !defined(TH_REAL_IS_HALF)
  {"fill_", (PyCFunction)THPTensor_(fill_), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
  {"is_same_size", (PyCFunction)THPTensor_(isSameSizeAs), METH_VARARGS | METH_KEYWORDS, NULL},
  {"is_contiguous", (PyCFunction)THPTensor_(isContiguous), METH_VARARGS | METH_KEYWORDS, NULL},
  {"is_set_to", (PyCFunction)THPTensor_(isSetTo), METH_VARARGS | METH_KEYWORDS, NULL},
#if !defined(TH_REAL_IS_HALF)
  {"masked_fill_", (PyCFunction)THPTensor_(maskedFill_), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"masked_scatter_", (PyCFunction)THPTensor_(maskedCopy_), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"masked_select", (PyCFunction)THPTensor_(maskedSelect), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
  {"transpose", (PyCFunction)THPTensor_(transpose), METH_VARARGS | METH_KEYWORDS, NULL},
  {"transpose_", (PyCFunction)THPTensor_(transpose_), METH_VARARGS | METH_KEYWORDS, NULL},
#if !defined(TH_REAL_IS_HALF)
  {"t", (PyCFunction)THPTensor_(t), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"t_", (PyCFunction)THPTensor_(t_), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
  {"squeeze", (PyCFunction)THPTensor_(squeeze), METH_VARARGS | METH_KEYWORDS, NULL},
  {"squeeze_", (PyCFunction)THPTensor_(squeeze_), METH_VARARGS | METH_KEYWORDS, NULL},
  {"unsqueeze", (PyCFunction)THPTensor_(unsqueeze), METH_VARARGS | METH_KEYWORDS, NULL},
  {"unsqueeze_", (PyCFunction)THPTensor_(unsqueeze_), METH_VARARGS | METH_KEYWORDS, NULL},
#if !defined(TH_REAL_IS_HALF)
  {"nonzero", (PyCFunction)THPTensor_(nonzero), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"contiguous", (PyCFunction)THPTensor_(contiguous), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"clone", (PyCFunction)THPTensor_(clone), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"view", (PyCFunction)THPTensor_(view), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"expand", (PyCFunction)THPTensor_(expand), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"resize_as_", (PyCFunction)THPTensor_(resizeAs_), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"index_select", (PyCFunction)THPTensor_(indexSelect), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"index_copy_", (PyCFunction)THPTensor_(indexCopy_), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"take", (PyCFunction)THPTensor_(take), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"put_", (PyCFunction)THPTensor_(put_), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"index_add_", (PyCFunction)THPTensor_(indexAdd_), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"index_fill_", (PyCFunction)THPTensor_(indexFill_), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
  {"narrow", (PyCFunction)THPTensor_(narrow), METH_VARARGS | METH_KEYWORDS, NULL},
  {"unfold", (PyCFunction)THPTensor_(unfold), METH_VARARGS | METH_KEYWORDS, NULL},
#if !defined(TH_REAL_IS_HALF)
  {"scatter_", (PyCFunction)THPTensor_(scatter_), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"scatter_add_", (PyCFunction)THPTensor_(scatter_add_), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"gather", (PyCFunction)THPTensor_(gather), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !IS_DISTRIBUTED
  {"data_ptr", (PyCFunction)THPTensor_(data_ptr), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"equal", (PyCFunction)THPTensor_(equal), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
  {"copy_", (PyCFunction)THPTensor_(copy_), METH_VARARGS | METH_KEYWORDS, NULL},
#if !defined(TH_REAL_IS_HALF)
  {"__and__", (PyCFunction)THPTensor_(__and__), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"__iand__", (PyCFunction)THPTensor_(__iand__), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"__or__", (PyCFunction)THPTensor_(__or__), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"__ior__", (PyCFunction)THPTensor_(__ior__), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"__xor__", (PyCFunction)THPTensor_(__xor__), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"__ixor__", (PyCFunction)THPTensor_(__ixor__), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"__lshift__", (PyCFunction)THPTensor_(__lshift__), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"__ilshift__", (PyCFunction)THPTensor_(__ilshift__), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"__rshift__", (PyCFunction)THPTensor_(__rshift__), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"__irshift__", (PyCFunction)THPTensor_(__irshift__), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !IS_DISTRIBUTED && (!IS_CUDA)
  {"apply_", (PyCFunction)THPTensor_(apply), METH_O, NULL},
#endif
#if !IS_DISTRIBUTED && (!IS_CUDA)
  {"map_", (PyCFunction)THPTensor_(map), METH_VARARGS, NULL},
#endif
#if !IS_DISTRIBUTED && (!IS_CUDA)
  {"map2_", (PyCFunction)THPTensor_(map2), METH_VARARGS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || defined(TH_REAL_IS_LONG) || defined(TH_REAL_IS_INT) || defined(TH_REAL_IS_SHORT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF || CUDA_LONG || CUDA_INT || CUDA_SHORT)
  {"abs", (PyCFunction)THPTensor_(abs), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || defined(TH_REAL_IS_LONG) || defined(TH_REAL_IS_INT) || defined(TH_REAL_IS_SHORT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF || CUDA_LONG || CUDA_INT || CUDA_SHORT)
  {"abs_", (PyCFunction)THPTensor_(abs_), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"sigmoid_", (PyCFunction)THPTensor_(sigmoid_), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"sigmoid", (PyCFunction)THPTensor_(sigmoid), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"log_", (PyCFunction)THPTensor_(log_), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"log", (PyCFunction)THPTensor_(log), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"log1p_", (PyCFunction)THPTensor_(log1p_), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"log1p", (PyCFunction)THPTensor_(log1p), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"lgamma", (PyCFunction)THPTensor_(lgamma), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"lgamma_", (PyCFunction)THPTensor_(lgamma_), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"digamma", (PyCFunction)THPTensor_(digamma), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"digamma_", (PyCFunction)THPTensor_(digamma_), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"polygamma", (PyCFunction)THPTensor_(polygamma), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"polygamma_", (PyCFunction)THPTensor_(polygamma_), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"exp_", (PyCFunction)THPTensor_(exp_), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"exp", (PyCFunction)THPTensor_(exp), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"expm1_", (PyCFunction)THPTensor_(expm1_), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"expm1", (PyCFunction)THPTensor_(expm1), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"cos_", (PyCFunction)THPTensor_(cos_), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"cos", (PyCFunction)THPTensor_(cos), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"acos_", (PyCFunction)THPTensor_(acos_), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"acos", (PyCFunction)THPTensor_(acos), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"cosh_", (PyCFunction)THPTensor_(cosh_), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"cosh", (PyCFunction)THPTensor_(cosh), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"sin_", (PyCFunction)THPTensor_(sin_), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"sin", (PyCFunction)THPTensor_(sin), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"asin_", (PyCFunction)THPTensor_(asin_), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"asin", (PyCFunction)THPTensor_(asin), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"sinh_", (PyCFunction)THPTensor_(sinh_), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"sinh", (PyCFunction)THPTensor_(sinh), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"tan_", (PyCFunction)THPTensor_(tan_), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"tan", (PyCFunction)THPTensor_(tan), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"atan_", (PyCFunction)THPTensor_(atan_), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"atan", (PyCFunction)THPTensor_(atan), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"tanh_", (PyCFunction)THPTensor_(tanh_), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"tanh", (PyCFunction)THPTensor_(tanh), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"erf_", (PyCFunction)THPTensor_(erf_), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"erf", (PyCFunction)THPTensor_(erf), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"erfinv_", (PyCFunction)THPTensor_(erfinv_), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"erfinv", (PyCFunction)THPTensor_(erfinv), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"sqrt_", (PyCFunction)THPTensor_(sqrt_), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"sqrt", (PyCFunction)THPTensor_(sqrt), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"rsqrt_", (PyCFunction)THPTensor_(rsqrt_), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"rsqrt", (PyCFunction)THPTensor_(rsqrt), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"ceil_", (PyCFunction)THPTensor_(ceil_), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"ceil", (PyCFunction)THPTensor_(ceil), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"floor_", (PyCFunction)THPTensor_(floor_), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"floor", (PyCFunction)THPTensor_(floor), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"round_", (PyCFunction)THPTensor_(round_), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"round", (PyCFunction)THPTensor_(round), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"trunc_", (PyCFunction)THPTensor_(trunc_), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"trunc", (PyCFunction)THPTensor_(trunc), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"frac_", (PyCFunction)THPTensor_(frac_), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"frac", (PyCFunction)THPTensor_(frac), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"mean", (PyCFunction)THPTensor_(mean), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"var", (PyCFunction)THPTensor_(var), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"std", (PyCFunction)THPTensor_(std), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"norm", (PyCFunction)THPTensor_(norm), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"renorm", (PyCFunction)THPTensor_(renorm), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"renorm_", (PyCFunction)THPTensor_(renorm_), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"dist", (PyCFunction)THPTensor_(dist), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"reciprocal", (PyCFunction)THPTensor_(reciprocal), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"reciprocal_", (PyCFunction)THPTensor_(reciprocal_), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"neg", (PyCFunction)THPTensor_(neg), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"neg_", (PyCFunction)THPTensor_(neg_), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"atan2", (PyCFunction)THPTensor_(atan2), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"atan2_", (PyCFunction)THPTensor_(atan2_), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"pow", (PyCFunction)THPTensor_(pow), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"pow_", (PyCFunction)THPTensor_(pow_), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"lerp", (PyCFunction)THPTensor_(lerp), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"lerp_", (PyCFunction)THPTensor_(lerp_), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_FLOAT) || defined(TH_REAL_IS_DOUBLE))
  {"histc", (PyCFunction)THPTensor_(histc), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"zero_", (PyCFunction)THPTensor_(zero_), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"sum", (PyCFunction)THPTensor_(sum), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"prod", (PyCFunction)THPTensor_(prod), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"cumsum", (PyCFunction)THPTensor_(cumsum), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"cumprod", (PyCFunction)THPTensor_(cumprod), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"sign", (PyCFunction)THPTensor_(sign), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"sign_", (PyCFunction)THPTensor_(sign_), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"trace", (PyCFunction)THPTensor_(trace), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"add", (PyCFunction)THPTensor_(add), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"add_", (PyCFunction)THPTensor_(add_), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"sub", (PyCFunction)THPTensor_(sub), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"sub_", (PyCFunction)THPTensor_(sub_), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"mul", (PyCFunction)THPTensor_(mul), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"mul_", (PyCFunction)THPTensor_(mul_), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"div", (PyCFunction)THPTensor_(div), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"div_", (PyCFunction)THPTensor_(div_), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"fmod", (PyCFunction)THPTensor_(fmod), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"fmod_", (PyCFunction)THPTensor_(fmod_), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"remainder", (PyCFunction)THPTensor_(remainder), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"remainder_", (PyCFunction)THPTensor_(remainder_), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"clamp", (PyCFunction)THPTensor_(clamp), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"clamp_", (PyCFunction)THPTensor_(clamp_), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF || !IS_CUDA)
  {"dot", (PyCFunction)THPTensor_(dot), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"tril", (PyCFunction)THPTensor_(tril), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"tril_", (PyCFunction)THPTensor_(tril_), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"triu", (PyCFunction)THPTensor_(triu), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"triu_", (PyCFunction)THPTensor_(triu_), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"cross", (PyCFunction)THPTensor_(cross), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"diag", (PyCFunction)THPTensor_(diag), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"addmm", (PyCFunction)THPTensor_(addmm), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"addmm_", (PyCFunction)THPTensor_(addmm_), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"addmv", (PyCFunction)THPTensor_(addmv), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"addmv_", (PyCFunction)THPTensor_(addmv_), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"addr", (PyCFunction)THPTensor_(addr), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"addr_", (PyCFunction)THPTensor_(addr_), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"ger", (PyCFunction)THPTensor_(ger), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"mv", (PyCFunction)THPTensor_(mv), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"mm", (PyCFunction)THPTensor_(mm), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"bmm", (PyCFunction)THPTensor_(bmm), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"addbmm", (PyCFunction)THPTensor_(addbmm), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"addbmm_", (PyCFunction)THPTensor_(addbmm_), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"baddbmm", (PyCFunction)THPTensor_(baddbmm), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"baddbmm_", (PyCFunction)THPTensor_(baddbmm_), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"addcmul", (PyCFunction)THPTensor_(addcmul), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"addcmul_", (PyCFunction)THPTensor_(addcmul_), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"addcdiv", (PyCFunction)THPTensor_(addcdiv), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"addcdiv_", (PyCFunction)THPTensor_(addcdiv_), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_FLOAT) || defined(TH_REAL_IS_DOUBLE) || CUDA_FLOAT || CUDA_DOUBLE)
  {"gesv", (PyCFunction)THPTensor_(gesv), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_FLOAT) || defined(TH_REAL_IS_DOUBLE) || CUDA_FLOAT || CUDA_DOUBLE)
  {"gels", (PyCFunction)THPTensor_(gels), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_FLOAT) || defined(TH_REAL_IS_DOUBLE))
  {"trtrs", (PyCFunction)THPTensor_(trtrs), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_FLOAT) || defined(TH_REAL_IS_DOUBLE) || CUDA_FLOAT || CUDA_DOUBLE)
  {"symeig", (PyCFunction)THPTensor_(symeig), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_FLOAT) || defined(TH_REAL_IS_DOUBLE) || CUDA_FLOAT || CUDA_DOUBLE)
  {"eig", (PyCFunction)THPTensor_(eig), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_FLOAT) || defined(TH_REAL_IS_DOUBLE) || CUDA_FLOAT || CUDA_DOUBLE)
  {"svd", (PyCFunction)THPTensor_(svd), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_FLOAT) || defined(TH_REAL_IS_DOUBLE) || CUDA_FLOAT || CUDA_DOUBLE)
  {"inverse", (PyCFunction)THPTensor_(inverse), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_FLOAT) || defined(TH_REAL_IS_DOUBLE) || CUDA_FLOAT || CUDA_DOUBLE)
  {"potrf", (PyCFunction)THPTensor_(potrf), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_FLOAT) || defined(TH_REAL_IS_DOUBLE) || CUDA_FLOAT || CUDA_DOUBLE)
  {"potrs", (PyCFunction)THPTensor_(potrs), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_FLOAT) || defined(TH_REAL_IS_DOUBLE) || CUDA_FLOAT || CUDA_DOUBLE)
  {"potri", (PyCFunction)THPTensor_(potri), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_FLOAT) || defined(TH_REAL_IS_DOUBLE))
  {"pstrf", (PyCFunction)THPTensor_(pstrf), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_FLOAT) || defined(TH_REAL_IS_DOUBLE) || CUDA_FLOAT || CUDA_DOUBLE)
  {"qr", (PyCFunction)THPTensor_(qr), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_FLOAT) || defined(TH_REAL_IS_DOUBLE) || CUDA_FLOAT || CUDA_DOUBLE)
  {"geqrf", (PyCFunction)THPTensor_(geqrf), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_FLOAT) || defined(TH_REAL_IS_DOUBLE))
  {"orgqr", (PyCFunction)THPTensor_(orgqr), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_FLOAT) || defined(TH_REAL_IS_DOUBLE))
  {"ormqr", (PyCFunction)THPTensor_(ormqr), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"btrifact", (PyCFunction)THPTensor_(btrifact), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"btrifact_with_info", (PyCFunction)THPTensor_(btrifact_with_info), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"btrisolve", (PyCFunction)THPTensor_(btrisolve), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"lt", (PyCFunction)THPTensor_(lt), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"lt_", (PyCFunction)THPTensor_(lt_), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"gt", (PyCFunction)THPTensor_(gt), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"gt_", (PyCFunction)THPTensor_(gt_), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"le", (PyCFunction)THPTensor_(le), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"le_", (PyCFunction)THPTensor_(le_), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"ge", (PyCFunction)THPTensor_(ge), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"ge_", (PyCFunction)THPTensor_(ge_), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"eq", (PyCFunction)THPTensor_(eq), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"eq_", (PyCFunction)THPTensor_(eq_), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"ne", (PyCFunction)THPTensor_(ne), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"ne_", (PyCFunction)THPTensor_(ne_), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"min", (PyCFunction)THPTensor_(min), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"max", (PyCFunction)THPTensor_(max), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (!IS_CUDA)
  {"kthvalue", (PyCFunction)THPTensor_(kthvalue), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"mode", (PyCFunction)THPTensor_(mode), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"median", (PyCFunction)THPTensor_(median), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"sort", (PyCFunction)THPTensor_(sort), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"topk", (PyCFunction)THPTensor_(topk), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_BYTE) || CUDA_BYTE)
  {"all", (PyCFunction)THPTensor_(all), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_BYTE) || CUDA_BYTE)
  {"any", (PyCFunction)THPTensor_(any), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (!IS_DISTRIBUTED && (!IS_CUDA))
  {"random_", (PyCFunction)THPTensor_(random_), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (!IS_DISTRIBUTED && (IS_CUDA))
  {"random_", (PyCFunction)THPTensor_(random_), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT))
  {"multinomial", (PyCFunction)THPTensor_(multinomial), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"multinomial", (PyCFunction)THPTensor_(multinomial), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT))
  {"uniform_", (PyCFunction)THPTensor_(uniform_), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"uniform_", (PyCFunction)THPTensor_(uniform_), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT))
  {"normal_", (PyCFunction)THPTensor_(normal_), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"normal_", (PyCFunction)THPTensor_(normal_), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT))
  {"cauchy_", (PyCFunction)THPTensor_(cauchy_), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"cauchy_", (PyCFunction)THPTensor_(cauchy_), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT))
  {"log_normal_", (PyCFunction)THPTensor_(logNormal_), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"log_normal_", (PyCFunction)THPTensor_(logNormal_), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT))
  {"exponential_", (PyCFunction)THPTensor_(exponential_), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"exponential_", (PyCFunction)THPTensor_(exponential_), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (!IS_DISTRIBUTED && (!IS_CUDA))
  {"geometric_", (PyCFunction)THPTensor_(geometric_), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (!IS_DISTRIBUTED && (IS_CUDA))
  {"geometric_", (PyCFunction)THPTensor_(geometric_), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_FLOAT) || defined(TH_REAL_IS_DOUBLE))
  {"bernoulli", (PyCFunction)THPTensor_(bernoulli), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (CUDA_FLOAT || CUDA_DOUBLE)
  {"bernoulli", (PyCFunction)THPTensor_(bernoulli), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (!IS_DISTRIBUTED && (!IS_CUDA))
  {"bernoulli_", (PyCFunction)THPTensor_(bernoulli_), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (!IS_DISTRIBUTED && (IS_CUDA))
  {"bernoulli_", (PyCFunction)THPTensor_(bernoulli_), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (IS_CUDA)
  {"get_device", (PyCFunction)THPTensor_(getDevice), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (IS_CUDA)
  {"new", (PyCFunction)THPTensor_(new), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (IS_CUDA)
  {"record_stream", (PyCFunction)THPTensor_(recordStream), METH_O, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (!IS_DISTRIBUTED)
  {"_sparse_mask", (PyCFunction)THPTensor_(_sparse_mask), METH_VARARGS | METH_KEYWORDS, NULL},
#endif

    {NULL}
};

static PyMethodDef THPTensor_stateless_(methods)[] = {
    #if !defined(TH_REAL_IS_HALF)
  {"zeros", (PyCFunction)THPTensor_stateless_(zeros), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"zeros_like", (PyCFunction)THPTensor_stateless_(zeros_like), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"ones", (PyCFunction)THPTensor_stateless_(ones), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"ones_like", (PyCFunction)THPTensor_stateless_(ones_like), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
  {"numel", (PyCFunction)THPTensor_stateless_(numel), METH_VARARGS | METH_KEYWORDS, NULL},
#if !defined(TH_REAL_IS_HALF)
  {"masked_select", (PyCFunction)THPTensor_stateless_(maskedSelect), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
  {"transpose", (PyCFunction)THPTensor_stateless_(transpose), METH_VARARGS | METH_KEYWORDS, NULL},
#if !defined(TH_REAL_IS_HALF)
  {"t", (PyCFunction)THPTensor_stateless_(t), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
  {"squeeze", (PyCFunction)THPTensor_stateless_(squeeze), METH_VARARGS | METH_KEYWORDS, NULL},
  {"unsqueeze", (PyCFunction)THPTensor_stateless_(unsqueeze), METH_VARARGS | METH_KEYWORDS, NULL},
#if !defined(TH_REAL_IS_HALF)
  {"nonzero", (PyCFunction)THPTensor_stateless_(nonzero), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"index_select", (PyCFunction)THPTensor_stateless_(indexSelect), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"take", (PyCFunction)THPTensor_stateless_(take), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"range", (PyCFunction)THPTensor_stateless_(range), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"arange", (PyCFunction)THPTensor_stateless_(arange), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"gather", (PyCFunction)THPTensor_stateless_(gather), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"cat", (PyCFunction)THPTensor_stateless_(cat), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"equal", (PyCFunction)THPTensor_stateless_(equal), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"__and__", (PyCFunction)THPTensor_stateless_(__and__), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"__iand__", (PyCFunction)THPTensor_stateless_(__iand__), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"__or__", (PyCFunction)THPTensor_stateless_(__or__), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"__ior__", (PyCFunction)THPTensor_stateless_(__ior__), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"__xor__", (PyCFunction)THPTensor_stateless_(__xor__), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"__ixor__", (PyCFunction)THPTensor_stateless_(__ixor__), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"__lshift__", (PyCFunction)THPTensor_stateless_(__lshift__), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"__ilshift__", (PyCFunction)THPTensor_stateless_(__ilshift__), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"__rshift__", (PyCFunction)THPTensor_stateless_(__rshift__), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"__irshift__", (PyCFunction)THPTensor_stateless_(__irshift__), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || defined(TH_REAL_IS_LONG) || defined(TH_REAL_IS_INT) || defined(TH_REAL_IS_SHORT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF || CUDA_LONG || CUDA_INT || CUDA_SHORT)
  {"abs", (PyCFunction)THPTensor_stateless_(abs), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"sigmoid", (PyCFunction)THPTensor_stateless_(sigmoid), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"log", (PyCFunction)THPTensor_stateless_(log), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"log1p", (PyCFunction)THPTensor_stateless_(log1p), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"lgamma", (PyCFunction)THPTensor_stateless_(lgamma), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"digamma", (PyCFunction)THPTensor_stateless_(digamma), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"polygamma", (PyCFunction)THPTensor_stateless_(polygamma), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"exp", (PyCFunction)THPTensor_stateless_(exp), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"expm1", (PyCFunction)THPTensor_stateless_(expm1), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"cos", (PyCFunction)THPTensor_stateless_(cos), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"acos", (PyCFunction)THPTensor_stateless_(acos), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"cosh", (PyCFunction)THPTensor_stateless_(cosh), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"sin", (PyCFunction)THPTensor_stateless_(sin), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"asin", (PyCFunction)THPTensor_stateless_(asin), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"sinh", (PyCFunction)THPTensor_stateless_(sinh), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"tan", (PyCFunction)THPTensor_stateless_(tan), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"atan", (PyCFunction)THPTensor_stateless_(atan), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"tanh", (PyCFunction)THPTensor_stateless_(tanh), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"erf", (PyCFunction)THPTensor_stateless_(erf), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"erfinv", (PyCFunction)THPTensor_stateless_(erfinv), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"sqrt", (PyCFunction)THPTensor_stateless_(sqrt), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"rsqrt", (PyCFunction)THPTensor_stateless_(rsqrt), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"ceil", (PyCFunction)THPTensor_stateless_(ceil), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"floor", (PyCFunction)THPTensor_stateless_(floor), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"round", (PyCFunction)THPTensor_stateless_(round), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"trunc", (PyCFunction)THPTensor_stateless_(trunc), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"frac", (PyCFunction)THPTensor_stateless_(frac), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"mean", (PyCFunction)THPTensor_stateless_(mean), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"var", (PyCFunction)THPTensor_stateless_(var), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"std", (PyCFunction)THPTensor_stateless_(std), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"norm", (PyCFunction)THPTensor_stateless_(norm), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"renorm", (PyCFunction)THPTensor_stateless_(renorm), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"dist", (PyCFunction)THPTensor_stateless_(dist), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"reciprocal", (PyCFunction)THPTensor_stateless_(reciprocal), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"neg", (PyCFunction)THPTensor_stateless_(neg), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"atan2", (PyCFunction)THPTensor_stateless_(atan2), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"pow", (PyCFunction)THPTensor_stateless_(pow), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"lerp", (PyCFunction)THPTensor_stateless_(lerp), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_FLOAT) || defined(TH_REAL_IS_DOUBLE))
  {"linspace", (PyCFunction)THPTensor_stateless_(linspace), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_FLOAT) || defined(TH_REAL_IS_DOUBLE))
  {"logspace", (PyCFunction)THPTensor_stateless_(logspace), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_FLOAT) || defined(TH_REAL_IS_DOUBLE))
  {"histc", (PyCFunction)THPTensor_stateless_(histc), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"sum", (PyCFunction)THPTensor_stateless_(sum), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"prod", (PyCFunction)THPTensor_stateless_(prod), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"cumsum", (PyCFunction)THPTensor_stateless_(cumsum), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"cumprod", (PyCFunction)THPTensor_stateless_(cumprod), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"sign", (PyCFunction)THPTensor_stateless_(sign), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"trace", (PyCFunction)THPTensor_stateless_(trace), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"add", (PyCFunction)THPTensor_stateless_(add), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"sub", (PyCFunction)THPTensor_stateless_(sub), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"mul", (PyCFunction)THPTensor_stateless_(mul), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"div", (PyCFunction)THPTensor_stateless_(div), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"fmod", (PyCFunction)THPTensor_stateless_(fmod), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"remainder", (PyCFunction)THPTensor_stateless_(remainder), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"clamp", (PyCFunction)THPTensor_stateless_(clamp), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF || !IS_CUDA)
  {"dot", (PyCFunction)THPTensor_stateless_(dot), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"tril", (PyCFunction)THPTensor_stateless_(tril), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"triu", (PyCFunction)THPTensor_stateless_(triu), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"cross", (PyCFunction)THPTensor_stateless_(cross), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"eye", (PyCFunction)THPTensor_stateless_(eye), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"diag", (PyCFunction)THPTensor_stateless_(diag), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"addmm", (PyCFunction)THPTensor_stateless_(addmm), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"addmv", (PyCFunction)THPTensor_stateless_(addmv), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"addr", (PyCFunction)THPTensor_stateless_(addr), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"ger", (PyCFunction)THPTensor_stateless_(ger), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"mv", (PyCFunction)THPTensor_stateless_(mv), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"mm", (PyCFunction)THPTensor_stateless_(mm), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"bmm", (PyCFunction)THPTensor_stateless_(bmm), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"addbmm", (PyCFunction)THPTensor_stateless_(addbmm), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"baddbmm", (PyCFunction)THPTensor_stateless_(baddbmm), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"addcmul", (PyCFunction)THPTensor_stateless_(addcmul), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"addcdiv", (PyCFunction)THPTensor_stateless_(addcdiv), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_FLOAT) || defined(TH_REAL_IS_DOUBLE) || CUDA_FLOAT || CUDA_DOUBLE)
  {"gesv", (PyCFunction)THPTensor_stateless_(gesv), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_FLOAT) || defined(TH_REAL_IS_DOUBLE) || CUDA_FLOAT || CUDA_DOUBLE)
  {"gels", (PyCFunction)THPTensor_stateless_(gels), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_FLOAT) || defined(TH_REAL_IS_DOUBLE))
  {"trtrs", (PyCFunction)THPTensor_stateless_(trtrs), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_FLOAT) || defined(TH_REAL_IS_DOUBLE) || CUDA_FLOAT || CUDA_DOUBLE)
  {"symeig", (PyCFunction)THPTensor_stateless_(symeig), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_FLOAT) || defined(TH_REAL_IS_DOUBLE) || CUDA_FLOAT || CUDA_DOUBLE)
  {"eig", (PyCFunction)THPTensor_stateless_(eig), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_FLOAT) || defined(TH_REAL_IS_DOUBLE) || CUDA_FLOAT || CUDA_DOUBLE)
  {"svd", (PyCFunction)THPTensor_stateless_(svd), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_FLOAT) || defined(TH_REAL_IS_DOUBLE) || CUDA_FLOAT || CUDA_DOUBLE)
  {"inverse", (PyCFunction)THPTensor_stateless_(inverse), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_FLOAT) || defined(TH_REAL_IS_DOUBLE) || CUDA_FLOAT || CUDA_DOUBLE)
  {"potrf", (PyCFunction)THPTensor_stateless_(potrf), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_FLOAT) || defined(TH_REAL_IS_DOUBLE) || CUDA_FLOAT || CUDA_DOUBLE)
  {"potrs", (PyCFunction)THPTensor_stateless_(potrs), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_FLOAT) || defined(TH_REAL_IS_DOUBLE) || CUDA_FLOAT || CUDA_DOUBLE)
  {"potri", (PyCFunction)THPTensor_stateless_(potri), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_FLOAT) || defined(TH_REAL_IS_DOUBLE))
  {"pstrf", (PyCFunction)THPTensor_stateless_(pstrf), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_FLOAT) || defined(TH_REAL_IS_DOUBLE) || CUDA_FLOAT || CUDA_DOUBLE)
  {"qr", (PyCFunction)THPTensor_stateless_(qr), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_FLOAT) || defined(TH_REAL_IS_DOUBLE) || CUDA_FLOAT || CUDA_DOUBLE)
  {"geqrf", (PyCFunction)THPTensor_stateless_(geqrf), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_FLOAT) || defined(TH_REAL_IS_DOUBLE))
  {"orgqr", (PyCFunction)THPTensor_stateless_(orgqr), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_FLOAT) || defined(TH_REAL_IS_DOUBLE))
  {"ormqr", (PyCFunction)THPTensor_stateless_(ormqr), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"btrifact", (PyCFunction)THPTensor_stateless_(btrifact), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"btrifact_with_info", (PyCFunction)THPTensor_stateless_(btrifact_with_info), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT) || CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"btrisolve", (PyCFunction)THPTensor_stateless_(btrisolve), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"lt", (PyCFunction)THPTensor_stateless_(lt), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"gt", (PyCFunction)THPTensor_stateless_(gt), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"le", (PyCFunction)THPTensor_stateless_(le), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"ge", (PyCFunction)THPTensor_stateless_(ge), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"eq", (PyCFunction)THPTensor_stateless_(eq), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"ne", (PyCFunction)THPTensor_stateless_(ne), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"min", (PyCFunction)THPTensor_stateless_(min), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"max", (PyCFunction)THPTensor_stateless_(max), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (!IS_CUDA)
  {"kthvalue", (PyCFunction)THPTensor_stateless_(kthvalue), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"mode", (PyCFunction)THPTensor_stateless_(mode), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"median", (PyCFunction)THPTensor_stateless_(median), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"sort", (PyCFunction)THPTensor_stateless_(sort), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"topk", (PyCFunction)THPTensor_stateless_(topk), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (!IS_DISTRIBUTED && (!IS_CUDA))
  {"randperm", (PyCFunction)THPTensor_stateless_(randperm), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT))
  {"multinomial", (PyCFunction)THPTensor_stateless_(multinomial), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"multinomial", (PyCFunction)THPTensor_stateless_(multinomial), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT))
  {"normal", (PyCFunction)THPTensor_stateless_(normal), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"normal", (PyCFunction)THPTensor_stateless_(normal), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT))
  {"_standard_gamma", (PyCFunction)THPTensor_stateless_(_standard_gamma), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT))
  {"_dirichlet_grad", (PyCFunction)THPTensor_stateless_(_dirichlet_grad), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT))
  {"rand", (PyCFunction)THPTensor_stateless_(rand), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"rand", (PyCFunction)THPTensor_stateless_(rand), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT))
  {"randn", (PyCFunction)THPTensor_stateless_(randn), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (CUDA_DOUBLE || CUDA_FLOAT || CUDA_HALF)
  {"randn", (PyCFunction)THPTensor_stateless_(randn), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_FLOAT) || defined(TH_REAL_IS_DOUBLE))
  {"bernoulli", (PyCFunction)THPTensor_stateless_(bernoulli), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (CUDA_FLOAT || CUDA_DOUBLE)
  {"bernoulli", (PyCFunction)THPTensor_stateless_(bernoulli), METH_VARARGS | METH_KEYWORDS, NULL},
#endif

    {NULL}
};
#if !defined(TH_REAL_IS_HALF) && !IS_DISTRIBUTED

static PyMethodDef THSPTensor_(methods)[] = {
    #if !defined(TH_REAL_IS_HALF)
  {"size", (PyCFunction)THSPTensor_(size), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (IS_CUDA)
  {"new", (PyCFunction)THSPTensor_(new), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"ndimension", (PyCFunction)THSPTensor_(nDimension), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"dim", (PyCFunction)THPTensor_(nDimension), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"_nnz", (PyCFunction)THSPTensor_(nnz), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"_dimI", (PyCFunction)THSPTensor_(nDimensionI), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"_dimV", (PyCFunction)THSPTensor_(nDimensionV), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"is_coalesced", (PyCFunction)THSPTensor_(isCoalesced), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"_indices", (PyCFunction)THSPTensor_(indices), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"_values", (PyCFunction)THSPTensor_(values), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"coalesce", (PyCFunction)THSPTensor_(coalesce), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"clone", (PyCFunction)THSPTensor_(clone), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"to_dense", (PyCFunction)THSPTensor_(toDense), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"resize_as_", (PyCFunction)THSPTensor_(resizeAs_), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"transpose", (PyCFunction)THSPTensor_(transpose), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"transpose_", (PyCFunction)THSPTensor_(transpose_), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"t", (PyCFunction)THSPTensor_(t), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"t_", (PyCFunction)THSPTensor_(t_), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"sspaddmm", (PyCFunction)THSPTensor_(sspaddmm), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"spadd", (PyCFunction)THSPTensor_(spadd), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"zero_", (PyCFunction)THSPTensor_(zero_), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"add", (PyCFunction)THSPTensor_(add), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"add_", (PyCFunction)THSPTensor_(add_), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"sub", (PyCFunction)THSPTensor_(sub), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"sub_", (PyCFunction)THSPTensor_(sub_), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"mul", (PyCFunction)THSPTensor_(mul), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"mul_", (PyCFunction)THSPTensor_(mul_), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"div", (PyCFunction)THSPTensor_(div), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"div_", (PyCFunction)THSPTensor_(div_), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_FLOAT) || defined(TH_REAL_IS_DOUBLE) || CUDA_FLOAT || CUDA_DOUBLE)
  {"norm", (PyCFunction)THSPTensor_(norm), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_FLOAT) || defined(TH_REAL_IS_DOUBLE) || CUDA_FLOAT || CUDA_DOUBLE)
  {"pow", (PyCFunction)THSPTensor_(pow), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (IS_CUDA)
  {"get_device", (PyCFunction)THSPTensor_(getDevice), METH_VARARGS | METH_KEYWORDS, NULL},
#endif

    {NULL}
};

#endif

#if !defined(TH_REAL_IS_HALF) && !IS_DISTRIBUTED

static PyMethodDef THSPTensor_stateless_(methods)[] = {
    #if !defined(TH_REAL_IS_HALF)
  {"t", (PyCFunction)THSPTensor_stateless_(t), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"mm", (PyCFunction)THSPTensor_stateless_(mm), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"spmm", (PyCFunction)THSPTensor_stateless_(spmm), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"hspmm", (PyCFunction)THSPTensor_stateless_(hspmm), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"sspmm", (PyCFunction)THSPTensor_stateless_(sspmm), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"sspaddmm", (PyCFunction)THSPTensor_stateless_(sspaddmm), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"spadd", (PyCFunction)THSPTensor_stateless_(spadd), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"zeros", (PyCFunction)THSPTensor_stateless_(zeros), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"zeros_like", (PyCFunction)THSPTensor_stateless_(zeros_like), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"add", (PyCFunction)THSPTensor_stateless_(add), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"sub", (PyCFunction)THSPTensor_stateless_(sub), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"mul", (PyCFunction)THSPTensor_stateless_(mul), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF)
  {"div", (PyCFunction)THSPTensor_stateless_(div), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_FLOAT) || defined(TH_REAL_IS_DOUBLE) || CUDA_FLOAT || CUDA_DOUBLE)
  {"norm", (PyCFunction)THSPTensor_stateless_(norm), METH_VARARGS | METH_KEYWORDS, NULL},
#endif
#if !defined(TH_REAL_IS_HALF) && (defined(TH_REAL_IS_FLOAT) || defined(TH_REAL_IS_DOUBLE) || CUDA_FLOAT || CUDA_DOUBLE)
  {"pow", (PyCFunction)THSPTensor_stateless_(pow), METH_VARARGS | METH_KEYWORDS, NULL},
#endif

    {NULL}
};

#endif

// PUT DEFINITIONS IN HERE PLEASE

#undef IS_CUDA
#undef CUDA_BYTE
#undef CUDA_CHAR
#undef CUDA_SHORT
#undef CUDA_INT
#undef CUDA_LONG
#undef CUDA_FLOAT
#undef CUDA_DOUBLE
#undef CUDA_HALF
#undef THIndexTensor
#undef THIndexTensor_
#undef THPIndexTensor
#undef THPIndexTensorClass
#undef THBoolTensor
#undef THPBoolTensor
#undef THPBoolTensorClass
#undef RealStr
#undef AS_REAL
